{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6fd1391-befe-46e2-b914-fa0fd1c2fe58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import dearpygui.dearpygui as dpg\n",
    "import threading\n",
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tkinter import Tk, filedialog, simpledialog\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import shutil \n",
    "import zipfile\n",
    "import gc\n",
    "from scipy.signal import butter, filtfilt, welch, iirnotch, hilbert, get_window\n",
    "import tracemalloc\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import pyperclip\n",
    "import mne\n",
    "from autoreject import AutoReject\n",
    "from pynwb import NWBHDF5IO, NWBFile, TimeSeries\n",
    "from pynwb.behavior import Position, SpatialSeries\n",
    "from pynwb.file import Subject\n",
    "from pynwb.misc import AnnotationSeries\n",
    "from typing import Optional\n",
    "from tensorpac import Pac\n",
    "from tensorpac.signals import pac_signals_wavelet\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.font_manager as fm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import io\n",
    "import contextlib\n",
    "from mne.time_frequency import psd_array_welch\n",
    "from scipy.stats import linregress\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from matplotlib.lines import Line2D\n",
    "from __future__ import annotations\n",
    "\n",
    "# Create the Logs and Save directory if it doesn't exist\n",
    "log_dir = \"Logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "save_dir = \"Saved Info\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "config_file = os.path.join(save_dir, \"config.json\")\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Create the log filename with the timestamp\n",
    "log_filename = os.path.join(log_dir, f'gui_V2_debug_{timestamp}.log')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, filename=log_filename, filemode='w', \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "global window_counter\n",
    "window_counter = 0\n",
    "\n",
    "# Suppress debug messages for the hdmf and pynwb libraries.\n",
    "logging.getLogger(\"hdmf\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pynwb\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.WARNING)\n",
    "\n",
    "# NWB stores an int16 ADC value NOT the voltage\n",
    "ADBitVolts = 0.00000030517578125\n",
    "ADBitmilliVolts = 0.00030517578125\n",
    "ADBitmicroVolts = 0.30517578125\n",
    "\n",
    "termes_font = FontProperties(fname=\"fonts/texgyretermes-regular.otf\")\n",
    "termes_font_bold = FontProperties(fname=\"fonts/texgyretermes-bold.otf\")\n",
    "#plt.rcParams['font.family'] = termes_font.get_name()\n",
    "\n",
    "error_string = \"Rats! An error has occured in the program. Sometimes this is ok, but \\\n",
    "it's best to reboot when this happens becuase the error could cause more errors.\\n\\n\\\n",
    "You can send an error log to Kyle to figure out what went wrong. They are located \\\n",
    "in chronological order in the 'Logs' folder. I recommend organizing by date modified \\\n",
    "to always get the most recent one. \\n\\nIt may also help to describe what you were doing \\\n",
    "when this happened so Kyle can try to reproduce the issue, thanks!.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d744ecb7-76df-4975-8317-7e4e0b5452c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################################\n",
    "# Themes\n",
    "#######################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65521de-b0c4-49dd-afc3-20f9dc7d1cfd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Color Palettes\n",
    "TABLEAU_COLORS = [\n",
    "    '#1f77b4',  # Blue\n",
    "    '#ff7f0e',  # Orange\n",
    "    '#2ca02c',  # Green\n",
    "    '#d62728',  # Red\n",
    "    '#9467bd',  # Purple\n",
    "    '#8c564b',  # Brown\n",
    "    '#e377c2',  # Pink\n",
    "    '#7f7f7f',  # Gray\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "BRIGHT_COLORS = [\n",
    "    '#e41a1c',  # Red\n",
    "    '#377eb8',  # Blue\n",
    "    '#4daf4a',  # Green\n",
    "    '#984ea3',  # Purple\n",
    "    '#ff7f00',  # Orange\n",
    "    '#ffff33',  # Yellow\n",
    "    '#a65628',  # Brown\n",
    "    '#f781bf',  # Pink\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "BRIGHT_COLORS = [\n",
    "    '#e41a1c',  # Red\n",
    "    '#ff7f00',  # Orange\n",
    "    '#d9d904',  # Yellow\n",
    "    '#4daf4a',  # Green\n",
    "    '#377eb8',  # Blue\n",
    "    '#984ea3',  # Purple\n",
    "    '#a65628',  # Brown\n",
    "    '#f781bf',  # Pink\n",
    "]\n",
    "\n",
    "COLORBLIND_OKABE_ITO = [\n",
    "    '#E69F00',  # Orange\n",
    "    '#56B4E9',  # Sky Blue\n",
    "    '#009E73',  # Bluish Green\n",
    "    '#F0E442',  # Yellow\n",
    "    '#0072B2',  # Blue\n",
    "    '#D55E00',  # Vermilion\n",
    "    '#CC79A7',  # Reddish Purple\n",
    "    '#999999',  # Gray\n",
    "]\n",
    "\n",
    "COLORBLIND_CUD = [\n",
    "    '#0072B2',  # Blue\n",
    "    '#009E73',  # Green\n",
    "    '#D55E00',  # Vermilion\n",
    "    '#CC79A7',  # Purple\n",
    "    '#F0E442',  # Yellow\n",
    "    '#56B4E9',  # Cyan\n",
    "    '#E69F00',  # Orange\n",
    "    '#999999',  # Gray\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d801da73-adfb-4016-9053-fc580d870f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################################\n",
    "# Classes\n",
    "#######################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe8e0adf-11ee-487a-ad80-608b743808be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Plot:\n",
    "    \"\"\"\n",
    "    A class to store information about a plot.\n",
    "    \n",
    "    Attributes:\n",
    "        name : The plot's name, auto-generated as \"Plot #\" if not provided.\n",
    "        ID : Unique ID number given to each plot, integral to tagging system in DearPyGui\n",
    "        plot_type : The chosen plot type.\n",
    "        nwb_file : The chosen NWB file.\n",
    "        channel : The chosen channel (as an int).\n",
    "        electrode_mapping : List that maps human-readable list to deafault list sorting from NWB file\n",
    "        data_start : Starting timestamp for plotted data (in seconds)\n",
    "        data_end : Ending timestamp for plotted data (in seconds)\n",
    "    \"\"\"\n",
    "    # Class-level counter to generate default plot names.\n",
    "    _plot_name_counter = 1\n",
    "    _plot_ID_counter = 1\n",
    "\n",
    "    # Used to update every set axis when shifting one\n",
    "    sync_axis_list = []\n",
    "    \n",
    "    def __init__(self,\n",
    "                 name: Optional[str] = None,\n",
    "                 ID: Optional[str] = None,\n",
    "                 plot_type: Optional[str] = None,\n",
    "                 nwb_file: Optional[str] = None,\n",
    "                 channel: Optional[int] = None,\n",
    "                 electrode_mapping = None,\n",
    "                 data_start: Optional[int]  = None,\n",
    "                 data_end: Optional[int]  = None,\n",
    "                 data_min = None,\n",
    "                 data_max = None,\n",
    "                 sfreq = None,\n",
    "                 lowcut = None,\n",
    "                 highcut = None,\n",
    "                 voltage_scale = None,\n",
    "                 custom_name = None,\n",
    "                 plot_output_path = None,\n",
    "                 export_status = None):\n",
    "        if name is None:\n",
    "            # Auto-generate the plot name as \"Plot #\" and increment the counter.\n",
    "            self.name = f\"Plot {Plot._plot_name_counter}\"\n",
    "            Plot._plot_name_counter += 1\n",
    "        else:\n",
    "            self.name = name\n",
    "\n",
    "        self.ID = Plot._plot_ID_counter\n",
    "        Plot._plot_ID_counter += 1\n",
    "\n",
    "        self.plot_type = None\n",
    "        self.nwb_file = None\n",
    "        self.channel = None\n",
    "        self.data_start = 0\n",
    "        self.data_end = 120\n",
    "        self.data_min = 0\n",
    "        self.data_max = 0\n",
    "        self.sfreq = 0\n",
    "        self.lowcut = 1\n",
    "        self.highcut = 200\n",
    "        self.voltage_scale = 1000\n",
    "        self.custom_name = \"Default\"\n",
    "        self.plot_output_path = \"\"\n",
    "        self.export_status = False\n",
    "\n",
    "    def set_plot_name(self, name):\n",
    "        self.name = name\n",
    "    def get_plot_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def set_plot_type(self, plot_type):\n",
    "        self.plot_type = plot_type\n",
    "    def get_plot_type(self):\n",
    "        return self.plot_type\n",
    "\n",
    "    def set_channel(self, channel):\n",
    "        self.channel = channel\n",
    "    def get_channel(self):\n",
    "        return self.channel\n",
    "\n",
    "    def set_electrode_mapping(self, electrode_mapping):\n",
    "        self.electrode_mapping = electrode_mapping\n",
    "    def get_electrode_mapping(self):\n",
    "        return self.electrode_mapping\n",
    "\n",
    "    def set_data_start(self, data_start):\n",
    "        self.data_start = data_start\n",
    "    def get_data_start(self):\n",
    "        return self.data_start\n",
    "\n",
    "    def set_data_end(self, data_end):\n",
    "        self.data_end = data_end\n",
    "    def get_data_end(self):\n",
    "        return self.data_end\n",
    "\n",
    "    def set_data_min(self, data_min):\n",
    "        self.data_min = data_min\n",
    "    def get_data_min(self):\n",
    "        return self.data_min\n",
    "\n",
    "    def set_data_max(self, data_max):\n",
    "        self.data_max = data_max\n",
    "    def get_data_max(self):\n",
    "        return self.data_max\n",
    "\n",
    "    def set_sfreq(self, sfreq):\n",
    "        self.sfreq = sfreq\n",
    "    def get_sfreq(self):\n",
    "        return self.sfreq\n",
    "\n",
    "    def set_lowcut(self, lowcut):\n",
    "        self.lowcut = lowcut\n",
    "    def get_lowcut(self):\n",
    "        return self.lowcut\n",
    "\n",
    "    def set_highcut(self, highcut):\n",
    "        self.highcut = highcut\n",
    "    def get_highcut(self):\n",
    "        return self.highcut\n",
    "\n",
    "    def set_voltage_scale(self, voltage_scale):\n",
    "            self.voltage_scale = voltage_scale\n",
    "    def get_voltage_scale(self):\n",
    "        return self.voltage_scale\n",
    "\n",
    "    def set_custom_name(self, custom_name):\n",
    "        self.custom_name = custom_name\n",
    "    def get_custom_name(self):\n",
    "        return self.custom_name\n",
    "\n",
    "    def set_plot_output_path(self, plot_output_path):\n",
    "        self.plot_output_path = plot_output_path\n",
    "    def get_plot_output_path(self):\n",
    "        return self.plot_output_path\n",
    "\n",
    "    def set_export_status(self, export_status):\n",
    "        self.export_status = export_status\n",
    "    def get_export_status(self):\n",
    "        return self.export_status\n",
    "\n",
    "    def add_to_sync_list(self, plot_instance_id):\n",
    "        Plot.sync_axis_list.append(plot_instance_id)\n",
    "    def remove_from_sync_list(self, plot_instance_id):\n",
    "        Plot.sync_axis_list.remove(plot_instance_id)\n",
    "    def get_sync_list():\n",
    "        return Plot.sync_axis_list\n",
    "        \n",
    "    @property\n",
    "    def get_folder_path(self):\n",
    "        return NWBFolder.get_folder_path()\n",
    "\n",
    "    @property\n",
    "    def get_file_list(self):\n",
    "        return NWBFolder.get_file_list()\n",
    "\n",
    "    @property\n",
    "    def get_rat_and_probe_from_channel(self):\n",
    "        return NWBFolder.get_rat_and_probe_from_channel(self.nwb_file, self.channel)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"Plot(name={self.name!r}, plot_type={self.plot_type!r}, \"\n",
    "                f\"nwb_file={self.nwb_file!r}, channel={self.channel!r})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680dc4ae-6bca-4067-bd2e-f1d53cf3bb26",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    \"\"\"\n",
    "    A class to store information about analysis settings\n",
    "    \n",
    "    Attributes:\n",
    "\n",
    "    \"\"\"\n",
    "    _analysis_ID_counter = 1\n",
    "    \n",
    "    def __init__(self,\n",
    "                 name='PAC',\n",
    "                 ID=None,\n",
    "                 nwb_list=None,\n",
    "                 lowfreq_override='Custom',\n",
    "                 lowfreq_lowpass=5,\n",
    "                 lowfreq_highpass=17,\n",
    "                 lowfreq_width=2,\n",
    "                 lowfreq_step=1,\n",
    "                 \n",
    "                 highfreq_override='Custom',\n",
    "                 highfreq_lowpass=40,\n",
    "                 highfreq_highpass=100,\n",
    "                 highfreq_width=10,\n",
    "                 highfreq_step=5,\n",
    "                 \n",
    "                 PAC_method='Modulation Index',\n",
    "                 surrogate_method='Swap Phase/Ampl. Across Trials',\n",
    "                 normalization_method='Z-score',\n",
    "                 dcomplex_method='Wavelet',\n",
    "                 phase_cycles=3,\n",
    "                 amplitude_cycles=6,\n",
    "                 morlet_width=7,\n",
    "                 KLD_or_HRPAC_bins=18,\n",
    "                 \n",
    "                 high_pass_filter = 0.0,\n",
    "                 filter_notch_frequencies='[60, 120, 180, 240, 300]',\n",
    "                 rejection_threshold=0,\n",
    "                 detrend_epochs = True,\n",
    "                 apply_autofilter = False,\n",
    "\n",
    "                 skip_PAC=False,\n",
    "                 inject_PAC=False,\n",
    "                 seed=0,\n",
    "                 surrogate_count=200,\n",
    "                 parallel_processes=8,\n",
    "                 data_length=10,\n",
    "                 minimum_length=5,\n",
    "                 epoch_length=20,\n",
    "                 sampling_rate=2000,\n",
    "                 downsample_rate=500,\n",
    "\n",
    "                 PAC_custom_colormap=False,\n",
    "                 PAC_vmin=-1.96,\n",
    "                 PAC_vmax=1.96,\n",
    "                 PAC_comod_interpolation=0.0,\n",
    "                 \n",
    "                 PSD_notch_frequencies='None',\n",
    "                 PSD_high_pass_filter=0.0,\n",
    "                 PSD_fmin=0.0,\n",
    "                 PSD_fmax=200.0,\n",
    "                 PSD_voltage_scale=\"Microvolts\",\n",
    "                 PSD_FFT_resolution=8,\n",
    "                 PSD_plot_raw=True,\n",
    "                 PSD_plot_filtered=False,\n",
    "                 PSD_plot_grouping_method='No Grouping',\n",
    "                 PSD_correct_1overf=False,\n",
    "\n",
    "                 output_directory=None,\n",
    "                 output_folder_name=\"GUIDA PAC Session [Timestamp]\",\n",
    "                 save_data=False,\n",
    "                 export_PNG=True,\n",
    "                 export_PDF=False,\n",
    "                 export_SVG=False,\n",
    "                 export_EPS=False,\n",
    "                 image_height=5.0,\n",
    "                 image_width=6.0,\n",
    "                 image_DPI=300,\n",
    "                 color_palette='Bright Colors',\n",
    "                 y_custom_axis=False,\n",
    "                 yaxis_top=10.0,\n",
    "                 yaxis_bottom=0.0,\n",
    "                 alpha=1,\n",
    "                 \n",
    "                 selected_channels=None,\n",
    "                 selected_rats=None):\n",
    "\n",
    "        self.name = name\n",
    "        self.ID = Analysis._analysis_ID_counter if ID is None else ID\n",
    "        self.nwb_list = nwb_list if nwb_list is not None else []\n",
    "\n",
    "        # Low frequency settings\n",
    "        self.lowfreq_override = lowfreq_override\n",
    "        self.lowfreq_lowpass = lowfreq_lowpass\n",
    "        self.lowfreq_highpass = lowfreq_highpass\n",
    "        self.lowfreq_width = lowfreq_width\n",
    "        self.lowfreq_step = lowfreq_step\n",
    "\n",
    "        # High frequency settings\n",
    "        self.highfreq_override = highfreq_override\n",
    "        self.highfreq_lowpass = highfreq_lowpass\n",
    "        self.highfreq_highpass = highfreq_highpass\n",
    "        self.highfreq_width = highfreq_width\n",
    "        self.highfreq_step = highfreq_step\n",
    "\n",
    "        # Filtering settings\n",
    "        self.high_pass_filter = high_pass_filter\n",
    "        self.filter_notch_frequencies = filter_notch_frequencies\n",
    "        self.rejection_threshold = rejection_threshold\n",
    "        self.detrend_epochs = detrend_epochs\n",
    "        self.apply_autofilter = apply_autofilter\n",
    "\n",
    "        # PAC method settings\n",
    "        self.PAC_method = PAC_method\n",
    "        self.surrogate_method = surrogate_method\n",
    "        self.normalization_method = normalization_method\n",
    "        self.dcomplex_method = dcomplex_method\n",
    "        self.phase_cycles = phase_cycles\n",
    "        self.amplitude_cycles = amplitude_cycles\n",
    "        self.morlet_width = morlet_width\n",
    "        self.KLD_or_HRPAC_bins = KLD_or_HRPAC_bins\n",
    "\n",
    "        # Processing parameters\n",
    "        self.skip_PAC = skip_PAC\n",
    "        self.inject_PAC = inject_PAC\n",
    "        self.seed = seed\n",
    "        self.surrogate_count = surrogate_count\n",
    "        self.parallel_processes = parallel_processes\n",
    "        self.data_length = data_length\n",
    "        self.minimum_length = minimum_length\n",
    "        self.epoch_length = epoch_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.downsample_rate = downsample_rate\n",
    "\n",
    "        # PAC export parameters\n",
    "        self.PAC_custom_colormap = PAC_custom_colormap\n",
    "        self.PAC_vmin = PAC_vmin\n",
    "        self.PAC_vmax = PAC_vmax\n",
    "        self.PAC_comod_interpolation = PAC_comod_interpolation\n",
    "\n",
    "        # PSD parameters\n",
    "        self.PSD_notch_frequencies = PSD_notch_frequencies\n",
    "        self.PSD_high_pass_filter = PSD_high_pass_filter\n",
    "        self.PSD_fmin = PSD_fmin\n",
    "        self.PSD_fmax = PSD_fmax\n",
    "        self.PSD_voltage_scale = PSD_voltage_scale\n",
    "        self.PSD_FFT_resolution = PSD_FFT_resolution\n",
    "        self.PSD_plot_raw = PSD_plot_raw\n",
    "        self.PSD_plot_filtered = PSD_plot_filtered\n",
    "        self.PSD_plot_grouping_method = PSD_plot_grouping_method\n",
    "        self.PSD_correct_1overf = PSD_correct_1overf\n",
    "\n",
    "        # Export parameters\n",
    "        self.output_directory = output_directory\n",
    "        self.output_folder_name = output_folder_name\n",
    "        self.save_data = save_data\n",
    "        self.export_PNG = export_PNG\n",
    "        self.export_PDF = export_PDF\n",
    "        self.export_SVG = export_SVG\n",
    "        self.export_EPS = export_EPS\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.image_DPI = image_DPI\n",
    "        self.color_palette = color_palette\n",
    "        self.y_custom_axis = y_custom_axis\n",
    "        self.yaxis_top = yaxis_top\n",
    "        self.yaxis_bottom = yaxis_bottom\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.selected_channels = selected_channels if selected_channels is not None else {}\n",
    "        self.selected_rats = selected_rats if selected_rats is not None else {}\n",
    "\n",
    "        Analysis._analysis_ID_counter += 1\n",
    "\n",
    "    def set_name(self, name):\n",
    "        self.name = name\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    # Low frequency settings\n",
    "\n",
    "    def set_lowfreq_override(self, lowfreq_override):\n",
    "        self.lowfreq_override = lowfreq_override\n",
    "    def get_lowfreq_override(self):\n",
    "        return self.lowfreq_override\n",
    "\n",
    "    def set_lowfreq_lowpass(self, lowfreq_lowpass):\n",
    "        self.lowfreq_lowpass = lowfreq_lowpass\n",
    "    def get_lowfreq_lowpass(self):\n",
    "        return self.lowfreq_lowpass\n",
    "\n",
    "    def set_lowfreq_highpass(self, lowfreq_highpass):\n",
    "        self.lowfreq_highpass = lowfreq_highpass\n",
    "    def get_lowfreq_highpass(self):\n",
    "        return self.lowfreq_highpass\n",
    "\n",
    "    def set_lowfreq_width(self, lowfreq_width):\n",
    "        self.lowfreq_width = lowfreq_width\n",
    "    def get_lowfreq_width(self):\n",
    "        return self.lowfreq_width\n",
    "\n",
    "    def set_lowfreq_step(self, lowfreq_step):\n",
    "        self.lowfreq_step = lowfreq_step\n",
    "    def get_lowfreq_step(self):\n",
    "        return self.lowfreq_step\n",
    "\n",
    "    # High frequency settings\n",
    "\n",
    "    def set_highfreq_override(self, highfreq_override):\n",
    "        self.highfreq_override = highfreq_override\n",
    "    def get_highfreq_override(self):\n",
    "        return self.highfreq_override\n",
    "\n",
    "    def set_highfreq_lowpass(self, highfreq_lowpass):\n",
    "        self.highfreq_lowpass = highfreq_lowpass\n",
    "    def get_highfreq_lowpass(self):\n",
    "        return self.highfreq_lowpass\n",
    "\n",
    "    def set_highfreq_highpass(self, highfreq_highpass):\n",
    "        self.highfreq_highpass = highfreq_highpass\n",
    "    def get_highfreq_highpass(self):\n",
    "        return self.highfreq_highpass\n",
    "\n",
    "    def set_highfreq_width(self, highfreq_width):\n",
    "        self.highfreq_width = highfreq_width\n",
    "    def get_highfreq_width(self):\n",
    "        return self.highfreq_width\n",
    "\n",
    "    def set_highfreq_step(self, highfreq_step):\n",
    "        self.highfreq_step = highfreq_step\n",
    "    def get_highfreq_step(self):\n",
    "        return self.highfreq_step\n",
    "\n",
    "    # Filtering settings\n",
    "\n",
    "    def set_high_pass_filter(self, high_pass_filter):\n",
    "        self.high_pass_filter = high_pass_filter\n",
    "    def get_high_pass_filter(self):\n",
    "        return self.high_pass_filter\n",
    "\n",
    "    def set_filter_notch_frequencies(self, filter_notch_frequencies):\n",
    "        self.filter_notch_frequencies = filter_notch_frequencies\n",
    "    def get_filter_notch_frequencies(self):\n",
    "        return self.filter_notch_frequencies\n",
    "\n",
    "    def set_rejection_threshold(self, rejection_threshold):\n",
    "        self.rejection_threshold = rejection_threshold\n",
    "    def get_rejection_threshold(self):\n",
    "        return self.rejection_threshold\n",
    "\n",
    "    def set_detrend_epochs(self, detrend_epochs):\n",
    "        self.detrend_epochs = detrend_epochs\n",
    "    def get_detrend_epochs(self):\n",
    "        return self.detrend_epochs\n",
    "\n",
    "    def set_apply_autofilter(self, apply_autofilter):\n",
    "        self.apply_autofilter = apply_autofilter\n",
    "    def get_apply_autofilter(self):\n",
    "        return self.apply_autofilter\n",
    "\n",
    "    # PAC method settings\n",
    "\n",
    "    def set_PAC_method(self, PAC_method):\n",
    "        self.PAC_method = PAC_method\n",
    "    def get_PAC_method(self):\n",
    "        return self.PAC_method\n",
    "\n",
    "    def set_surrogate_method(self, surrogate_method):\n",
    "        self.surrogate_method = surrogate_method\n",
    "    def get_surrogate_method(self):\n",
    "        return self.surrogate_method\n",
    "\n",
    "    def set_normalization_method(self, normalization_method):\n",
    "        self.normalization_method = normalization_method\n",
    "    def get_normalization_method(self):\n",
    "        return self.normalization_method\n",
    "\n",
    "    def set_dcomplex_method(self, dcomplex_method):\n",
    "        self.dcomplex_method = dcomplex_method\n",
    "    def get_dcomplex_method(self):\n",
    "        return self.dcomplex_method\n",
    "\n",
    "    def set_phase_cycles(self, phase_cycles):\n",
    "        self.phase_cycles = phase_cycles\n",
    "    def get_phase_cycles(self):\n",
    "        return self.phase_cycles\n",
    "\n",
    "    def set_amplitude_cycles(self, amplitude_cycles):\n",
    "        self.amplitude_cycles = amplitude_cycles\n",
    "    def get_amplitude_cycles(self):\n",
    "        return self.amplitude_cycles\n",
    "\n",
    "    def set_morlet_width(self, morlet_width):\n",
    "        self.morlet_width = morlet_width\n",
    "    def get_morlet_width(self):\n",
    "        return self.morlet_width\n",
    "\n",
    "    def set_KLD_or_HRPAC_bins(self, KLD_or_HRPAC_bins):\n",
    "        self.KLD_or_HRPAC_bins = KLD_or_HRPAC_bins\n",
    "    def get_KLD_or_HRPAC_bins(self):\n",
    "        return self.KLD_or_HRPAC_bins\n",
    "\n",
    "    # Processing parameters\n",
    "\n",
    "    def set_skip_PAC(self, skip_PAC):\n",
    "        self.skip_PAC = skip_PAC\n",
    "    def get_skip_PAC(self):\n",
    "        return self.skip_PAC\n",
    "\n",
    "    def set_inject_PAC(self, inject_PAC):\n",
    "        self.inject_PAC = inject_PAC\n",
    "    def get_inject_PAC(self):\n",
    "        return self.inject_PAC\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        self.seed = seed\n",
    "    def get_seed(self):\n",
    "        return self.seed\n",
    "    \n",
    "    def set_downsample_rate(self, downsample_rate):\n",
    "        self.downsample_rate = downsample_rate\n",
    "    def get_downsample_rate(self):\n",
    "        return self.downsample_rate\n",
    "\n",
    "    def set_sampling_rate(self, sampling_rate):\n",
    "        self.sampling_rate = sampling_rate\n",
    "    def get_sampling_rate(self):\n",
    "        return self.sampling_rate\n",
    "\n",
    "    def set_epoch_length(self, epoch_length):\n",
    "        self.epoch_length = epoch_length\n",
    "    def get_epoch_length(self):\n",
    "        return self.epoch_length\n",
    "\n",
    "    def set_data_length(self, data_length):\n",
    "        self.data_length = data_length\n",
    "    def get_data_length(self):\n",
    "        return self.data_length\n",
    "        \n",
    "    def set_minimum_length(self, minimum_length):\n",
    "        self.minimum_length = minimum_length\n",
    "    def get_minimum_length(self):\n",
    "        return self.minimum_length\n",
    "\n",
    "    def set_parallel_processes(self, parallel_processes):\n",
    "        self.parallel_processes = parallel_processes\n",
    "    def get_parallel_processes(self):\n",
    "        return self.parallel_processes\n",
    "\n",
    "    def set_surrogate_count(self, surrogate_count):\n",
    "        self.surrogate_count = surrogate_count\n",
    "    def get_surrogate_count(self):\n",
    "        return self.surrogate_count\n",
    "        \n",
    "    # PAC export parameters\n",
    "\n",
    "    def set_PAC_custom_colormap(self, PAC_custom_colormap):\n",
    "        self.PAC_custom_colormap = PAC_custom_colormap\n",
    "    def get_PAC_custom_colormap(self):\n",
    "        return self.PAC_custom_colormap\n",
    "\n",
    "    def set_PAC_vmin(self, PAC_vmin):\n",
    "        self.PAC_vmin = PAC_vmin\n",
    "    def get_PAC_vmin(self):\n",
    "        return self.PAC_vmin\n",
    "\n",
    "    def set_PAC_vmax(self, PAC_vmax):\n",
    "        self.PAC_vmax = PAC_vmax\n",
    "    def get_PAC_vmax(self):\n",
    "        return self.PAC_vmax\n",
    "\n",
    "    def set_PAC_comod_interpolation(self, PAC_comod_interpolation):\n",
    "        self.PAC_comod_interpolation = PAC_comod_interpolation\n",
    "    def get_PAC_comod_interpolation(self):\n",
    "        return self.PAC_comod_interpolation\n",
    "\n",
    "    # PSD parameters\n",
    "\n",
    "    def set_PSD_notch_frequencies(self, PSD_notch_frequencies):\n",
    "        self.PSD_notch_frequencies = PSD_notch_frequencies\n",
    "    def get_PSD_notch_frequencies(self):\n",
    "        return self.PSD_notch_frequencies\n",
    "\n",
    "    def set_PSD_high_pass_filter(self, PSD_high_pass_filter):\n",
    "        self.PSD_high_pass_filter = PSD_high_pass_filter\n",
    "    def get_PSD_high_pass_filter(self):\n",
    "        return self.PSD_high_pass_filter\n",
    "\n",
    "    def set_PSD_fmin(self, PSD_fmin):\n",
    "        self.PSD_fmin = PSD_fmin\n",
    "    def get_PSD_fmin(self):\n",
    "        return self.PSD_fmin\n",
    "\n",
    "    def set_PSD_fmax(self, PSD_fmax):\n",
    "        self.PSD_fmax = PSD_fmax\n",
    "    def get_PSD_fmax(self):\n",
    "        return self.PSD_fmax\n",
    "\n",
    "    def set_PSD_voltage_scale(self, PSD_voltage_scale):\n",
    "        self.PSD_voltage_scale = PSD_voltage_scale\n",
    "    def get_PSD_voltage_scale(self):\n",
    "        return self.PSD_voltage_scale\n",
    "\n",
    "    def set_PSD_FFT_resolution(self, PSD_FFT_resolution):\n",
    "        self.PSD_FFT_resolution = PSD_FFT_resolution\n",
    "    def get_PSD_FFT_resolution(self):\n",
    "        return self.PSD_FFT_resolution\n",
    "\n",
    "    def set_PSD_plot_raw(self, PSD_plot_raw):\n",
    "        self.PSD_plot_raw = PSD_plot_raw\n",
    "    def get_PSD_plot_raw(self):\n",
    "        return self.PSD_plot_raw\n",
    "\n",
    "    def set_PSD_plot_filtered(self, PSD_plot_filtered):\n",
    "        self.PSD_plot_filtered = PSD_plot_filtered\n",
    "    def get_PSD_plot_filtered(self):\n",
    "        return self.PSD_plot_filtered\n",
    "\n",
    "    def set_PSD_plot_grouping_method(self, PSD_plot_grouping_method):\n",
    "        self.PSD_plot_grouping_method = PSD_plot_grouping_method\n",
    "    def get_PSD_plot_grouping_method(self):\n",
    "        return self.PSD_plot_grouping_method\n",
    "    \n",
    "    def set_PSD_correct_1overf(self, PSD_correct_1overf):\n",
    "        self.PSD_correct_1overf = PSD_correct_1overf\n",
    "    def get_PSD_correct_1overf(self):\n",
    "        return self.PSD_correct_1overf\n",
    "    \n",
    "    # Export parameters\n",
    "\n",
    "    def set_output_directory(self, output_directory):\n",
    "        self.output_directory = output_directory\n",
    "    def get_output_directory(self):\n",
    "        return self.output_directory\n",
    "\n",
    "    def set_output_folder_name(self, output_folder_name):\n",
    "        self.output_folder_name = output_folder_name\n",
    "    def get_output_folder_name(self):\n",
    "        return self.output_folder_name\n",
    "\n",
    "    def set_save_data(self, save_data):\n",
    "        self.save_data = save_data\n",
    "    def get_save_data(self):\n",
    "        return self.save_data\n",
    "\n",
    "    def set_export_PNG(self, export_PNG):\n",
    "        self.export_PNG = export_PNG\n",
    "    def get_export_PNG(self):\n",
    "        return self.export_PNG\n",
    "\n",
    "    def set_export_PDF(self, export_PDF):\n",
    "        self.export_PDF = export_PDF\n",
    "    def get_export_PDF(self):\n",
    "        return self.export_PDF\n",
    "\n",
    "    def set_export_SVG(self, export_SVG):\n",
    "        self.export_SVG = export_SVG\n",
    "    def get_export_SVG(self):\n",
    "        return self.export_SVG\n",
    "\n",
    "    def set_export_EPS(self, export_EPS):\n",
    "        self.export_EPS = export_EPS\n",
    "    def get_export_EPS(self):\n",
    "        return self.export_EPS\n",
    "\n",
    "    def set_image_height(self, image_height):\n",
    "        self.image_height = image_height\n",
    "    def get_image_height(self):\n",
    "        return self.image_height\n",
    "\n",
    "    def set_image_width(self, image_width):\n",
    "        self.image_width = image_width\n",
    "    def get_image_width(self):\n",
    "        return self.image_width\n",
    "\n",
    "    def set_image_DPI(self, image_DPI):\n",
    "        self.image_DPI = image_DPI\n",
    "    def get_image_DPI(self):\n",
    "        return self.image_DPI\n",
    "\n",
    "    def set_color_palette(self, color_palette):\n",
    "        self.color_palette = color_palette\n",
    "    def get_color_palette(self):\n",
    "        return self.color_palette\n",
    "\n",
    "    def set_y_custom_axis(self, y_custom_axis):\n",
    "        self.y_custom_axis = y_custom_axis\n",
    "    def get_y_custom_axis(self):\n",
    "        return self.y_custom_axis\n",
    "\n",
    "    def set_yaxis_top(self, yaxis_top):\n",
    "        self.yaxis_top = yaxis_top\n",
    "    def get_yaxis_top(self):\n",
    "        return self.yaxis_top\n",
    "\n",
    "    def set_yaxis_bottom(self, yaxis_bottom):\n",
    "        self.yaxis_bottom = yaxis_bottom\n",
    "    def get_yaxis_bottom(self):\n",
    "        return self.yaxis_bottom\n",
    "\n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    def get_alpha(self):\n",
    "        return self.alpha\n",
    "\n",
    "    # Additional Class Functions\n",
    "\n",
    "    def add_to_nwb_list(self, nwb_list_item):\n",
    "        self.nwb_list.append(nwb_list_item)\n",
    "    def remove_from_nwb_list(self, nwb_list_item):\n",
    "        self.nwb_list.remove(nwb_list_item)\n",
    "    def get_nwb_list(self):\n",
    "        return self.nwb_list\n",
    "    \n",
    "    def add_selected_channel(self, file, channel_name, data_index):\n",
    "        if file not in self.selected_channels:\n",
    "            self.selected_channels[file] = {}\n",
    "        self.selected_channels[file][channel_name] = data_index\n",
    "    \n",
    "    def remove_selected_channel(self, file, channel_name):\n",
    "        if file in self.selected_channels and channel_name in self.selected_channels[file]:\n",
    "            del self.selected_channels[file][channel_name]\n",
    "            if not self.selected_channels[file]:  # If no channels left\n",
    "                del self.selected_channels[file]\n",
    "\n",
    "    def get_selected_channels(self):\n",
    "        return self.selected_channels\n",
    "\n",
    "    def add_selected_rat(self, file, rat, rat_channels, data_columns):\n",
    "        if file not in self.selected_rats:\n",
    "            self.selected_rats[file] = {}\n",
    "        self.selected_rats[file][rat] = {'channels' : rat_channels, 'data_columns' : data_columns }\n",
    "    \n",
    "    def remove_selected_rat(self, file, rat):\n",
    "        if file in self.selected_rats and rat in self.selected_rats[file]:\n",
    "            del self.selected_rats[file][rat]\n",
    "            if not self.selected_rats[file]:  # If no rats left\n",
    "                del self.selected_rats[file]\n",
    "\n",
    "    def get_selected_rats(self):\n",
    "        return self.selected_rats\n",
    "\n",
    "    def get_all_PAC_parameters(self):\n",
    "\n",
    "        lowfreq_override = self.lowfreq_override\n",
    "        lowfreq_lowpass = self.lowfreq_lowpass\n",
    "        lowfreq_highpass = self.lowfreq_highpass\n",
    "        lowfreq_width = self.lowfreq_width\n",
    "        lowfreq_step = self.lowfreq_step\n",
    "\n",
    "        highfreq_override = self.highfreq_override\n",
    "        highfreq_lowpass = self.highfreq_lowpass\n",
    "        highfreq_highpass = self.highfreq_highpass\n",
    "        highfreq_width = self.highfreq_width\n",
    "        highfreq_step = self.highfreq_step\n",
    "\n",
    "        high_pass_filter = self.high_pass_filter\n",
    "        filter_notch_frequencies = self.filter_notch_frequencies\n",
    "        rejection_threshold = self.rejection_threshold\n",
    "        detrend_epochs = self.detrend_epochs\n",
    "        apply_autofilter = self.apply_autofilter\n",
    "        \n",
    "        PAC_method = self.PAC_method\n",
    "        surrogate_method = self.surrogate_method\n",
    "        normalization_method = self.normalization_method\n",
    "        dcomplex_method = self.dcomplex_method\n",
    "        phase_cycles = self.phase_cycles\n",
    "        amplitude_cycles = self.amplitude_cycles\n",
    "        morlet_width = self.morlet_width\n",
    "        KLD_or_HRPAC_bins = self.KLD_or_HRPAC_bins\n",
    "\n",
    "        skip_PAC = self.skip_PAC\n",
    "        inject_PAC = self.inject_PAC\n",
    "        seed = self.seed\n",
    "        surrogate_count = self.surrogate_count\n",
    "        parallel_processes = self.parallel_processes\n",
    "        data_length = self.data_length\n",
    "        minimum_length = self.minimum_length\n",
    "        epoch_length = self.epoch_length\n",
    "        sampling_rate = self.sampling_rate\n",
    "        downsample_rate = self.downsample_rate\n",
    "\n",
    "        if lowfreq_override == 'Custom':\n",
    "            phase_frequency = (lowfreq_lowpass, lowfreq_highpass, lowfreq_width, lowfreq_step)\n",
    "        else:\n",
    "            phase_frequency = lowfreq_override\n",
    "\n",
    "        if highfreq_override == 'Custom':\n",
    "            amplitude_frequency = (highfreq_lowpass, highfreq_highpass, highfreq_width, highfreq_step)\n",
    "        else:\n",
    "            amplitude_frequency = highfreq_override\n",
    "        \n",
    "        if PAC_method == 'Mean Vector Length':\n",
    "            idpac_a = 1\n",
    "        elif PAC_method == 'Modulation Index':\n",
    "            idpac_a = 2\n",
    "        elif PAC_method == 'Heights Ratio':\n",
    "            idpac_a = 3\n",
    "        elif PAC_method == 'ndPAC':\n",
    "            idpac_a = 4\n",
    "        elif PAC_method == 'Phase-Locking Value':\n",
    "            idpac_a = 5\n",
    "        elif PAC_method == 'Gaussian Copula PAC':\n",
    "            idpac_a = 6\n",
    "\n",
    "        if surrogate_method == 'No Surrogates':\n",
    "            idpac_b = 0\n",
    "        elif surrogate_method == 'Swap Phase/Ampl. Across Trials':\n",
    "            idpac_b = 1\n",
    "        elif surrogate_method == 'Swap Amplitude Time Blocks':\n",
    "            idpac_b = 2\n",
    "        elif surrogate_method == 'Time Lag':\n",
    "            idpac_b = 3\n",
    "\n",
    "        if normalization_method == 'No Normalization':\n",
    "            idpac_c = 0\n",
    "        elif normalization_method == 'Subtract Mean of Surrogtes':\n",
    "            idpac_c = 1\n",
    "        elif normalization_method == 'Divide Mean of Surrogates':\n",
    "            idpac_c = 2\n",
    "        elif normalization_method == 'Sub+Div Mean of Surrogates':\n",
    "            idpac_c = 3\n",
    "        elif normalization_method == 'Z-score':\n",
    "            idpac_c = 4\n",
    "\n",
    "        if dcomplex_method == 'Wavelet':\n",
    "            dcomplex = 'wavelet'\n",
    "        elif dcomplex_method == 'Hilbert':\n",
    "            dcomplex = 'hilbert'\n",
    "\n",
    "        idpac = (idpac_a, idpac_b, idpac_c)\n",
    "        cycles = (phase_cycles, amplitude_cycles)\n",
    "\n",
    "        if filter_notch_frequencies == 'None':\n",
    "            filter_notch_frequencies = []\n",
    "        elif filter_notch_frequencies == '[60]':\n",
    "            filter_notch_frequencies = [60]\n",
    "        elif filter_notch_frequencies == '[60, 120]':\n",
    "            filter_notch_frequencies = [60, 120]\n",
    "        elif filter_notch_frequencies == '[60, 120, 180]':\n",
    "            filter_notch_frequencies = [60, 120, 180]\n",
    "        elif filter_notch_frequencies == '[60, 120, 180, 240]':\n",
    "            filter_notch_frequencies = [60, 120, 180, 240]\n",
    "        elif filter_notch_frequencies == '[60, 120, 180, 240, 300]':\n",
    "            filter_notch_frequencies = [60, 120, 180, 240, 300]\n",
    "\n",
    "        return (phase_frequency, amplitude_frequency, idpac, dcomplex, cycles, morlet_width, KLD_or_HRPAC_bins, \n",
    "                sampling_rate, epoch_length, data_length, minimum_length, parallel_processes, surrogate_count, \n",
    "                filter_notch_frequencies, high_pass_filter, detrend_epochs, apply_autofilter, downsample_rate,\n",
    "                seed, rejection_threshold, skip_PAC, inject_PAC)\n",
    "\n",
    "    def get_export_parameters(self):\n",
    "\n",
    "        output_folder_name = self.output_folder_name\n",
    "        save_data = self.save_data\n",
    "        export_PNG = self.export_PNG\n",
    "        export_PDF = self.export_PDF\n",
    "        export_SVG = self.export_SVG\n",
    "        export_EPS = self.export_EPS\n",
    "        image_height = self.image_height\n",
    "        image_width = self.image_width\n",
    "        image_DPI = self.image_DPI\n",
    "        color_palette = self.color_palette\n",
    "        y_custom_axis = self.y_custom_axis\n",
    "        yaxis_top = self.yaxis_top\n",
    "        yaxis_bottom = self.yaxis_bottom\n",
    "        alpha = self.alpha\n",
    "\n",
    "        if color_palette == 'Bright Colors':\n",
    "            color_palette = BRIGHT_COLORS\n",
    "        elif color_palette == 'Tableau Colors':\n",
    "            color_palette = TABLEAU_COLORS\n",
    "        elif color_palette == 'Colorblind Okabe Ito':\n",
    "            color_palette = COLORBLIND_OKABE_ITO\n",
    "        elif color_palette == 'Colorblind Cud':\n",
    "            color_palette = COLORBLIND_CUD\n",
    "\n",
    "        return (output_folder_name, \n",
    "                export_PNG, \n",
    "                export_PDF, \n",
    "                export_SVG, \n",
    "                export_EPS, \n",
    "                image_height, \n",
    "                image_width, \n",
    "                image_DPI, \n",
    "                color_palette, \n",
    "                y_custom_axis, \n",
    "                yaxis_top, \n",
    "                yaxis_bottom,\n",
    "                alpha,\n",
    "                save_data)\n",
    "\n",
    "    def get_all_PSD_parameters(self):\n",
    "        \n",
    "        PSD_notch_frequencies = self.PSD_notch_frequencies\n",
    "        PSD_high_pass_filter = self.PSD_high_pass_filter\n",
    "        sampling_rate = self.sampling_rate\n",
    "        output_directory = self.output_directory\n",
    "        PSD_fmin = self.PSD_fmin\n",
    "        PSD_fmax = self.PSD_fmax\n",
    "        PSD_voltage_scale = self.PSD_voltage_scale\n",
    "        PSD_FFT_resolution = self.PSD_FFT_resolution\n",
    "        PSD_plot_raw = self.PSD_plot_raw\n",
    "        PSD_plot_filtered = self.PSD_plot_filtered\n",
    "        PSD_plot_grouping_method = self.PSD_plot_grouping_method\n",
    "        PSD_correct_1overf = self.PSD_correct_1overf \n",
    "\n",
    "        if PSD_notch_frequencies == 'None':\n",
    "            PSD_notch_frequencies = []\n",
    "        elif PSD_notch_frequencies == '[60]':\n",
    "            PSD_notch_frequencies = [60]\n",
    "        elif PSD_notch_frequencies == '[60, 120]':\n",
    "            PSD_notch_frequencies = [60, 120]\n",
    "        elif PSD_notch_frequencies == '[60, 120, 180]':\n",
    "            PSD_notch_frequencies = [60, 120, 180]\n",
    "        elif PSD_notch_frequencies == '[60, 120, 180, 240]':\n",
    "            PSD_notch_frequencies = [60, 120, 180, 240]\n",
    "        elif PSD_notch_frequencies == '[60, 120, 180, 240, 300]':\n",
    "            PSD_notch_frequencies = [60, 120, 180, 240, 300]\n",
    "\n",
    "        if PSD_voltage_scale == \"Volts\":\n",
    "            PSD_voltage_scale = 1\n",
    "        elif PSD_voltage_scale == \"Millivolts\":\n",
    "            PSD_voltage_scale = 1000\n",
    "        elif PSD_voltage_scale == \"Microvolts\":\n",
    "            PSD_voltage_scale = 1000000\n",
    "\n",
    "        return (PSD_notch_frequencies, PSD_high_pass_filter, PSD_correct_1overf, sampling_rate, \n",
    "                output_directory, PSD_fmin, PSD_fmax, PSD_voltage_scale, PSD_FFT_resolution,\n",
    "                PSD_plot_raw, PSD_plot_filtered, PSD_plot_grouping_method)\n",
    "\n",
    "    def get_all_PAC_export_parameters(self):\n",
    "\n",
    "        PAC_custom_colormap = self.PAC_custom_colormap\n",
    "        PAC_vmin = self.PAC_vmin\n",
    "        PAC_vmax = self.PAC_vmax\n",
    "        PAC_comod_interpolation = self.PAC_comod_interpolation\n",
    "\n",
    "        return (PAC_custom_colormap, PAC_vmin, PAC_vmax, PAC_comod_interpolation)\n",
    "\n",
    "    @property\n",
    "    def get_folder_path(self):\n",
    "        return NWBFolder.get_folder_path()\n",
    "\n",
    "    @property\n",
    "    def get_file_list(self):\n",
    "        return NWBFolder.get_file_list()\n",
    "\n",
    "    def get_rats_in_file(self, file_name):\n",
    "        return NWBFolder.get_rats_in_file(file_name)\n",
    "\n",
    "    def get_day_from_file(file):\n",
    "        return NWBFolder.get_day_from_file(file)\n",
    "\n",
    "    def get_rat_and_probe_from_channel(nwb_file, channel):\n",
    "        return NWBFolder.get_rat_and_probe_from_channel(nwb_file, channel)\n",
    "\n",
    "    def get_channels_from_rat(self, file_name, rat_number):\n",
    "        return NWBFolder.get_channels_from_rat(file_name, rat_number)\n",
    "\n",
    "    def get_channel_names_from_rat(self, rat_number):\n",
    "        return NWBFolder.get_channel_names_from_rat(rat_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b94b46-5701-45e5-9162-6dcbb4b2f9c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class JSON:\n",
    "    \"\"\"\n",
    "    A class to store information about JSON file analysis.\n",
    "    \"\"\"\n",
    "    _session_ID_counter = 1\n",
    "    \n",
    "    def __init__(self,\n",
    "                 custom_name = None,\n",
    "                 plot_output_path = None,\n",
    "                 session_ID_counter = 1):\n",
    "        self.plot_output_path = \"\"\n",
    "        self.export_status = False\n",
    "        self.session_ID_counter = session_ID_counter\n",
    "\n",
    "    def set_custom_name(self, custom_name):\n",
    "        self.custom_name = custom_name\n",
    "    def get_custom_name(self):\n",
    "        return self.custom_name\n",
    "\n",
    "    def set_plot_output_path(self, plot_output_path):\n",
    "        self.plot_output_path = plot_output_path\n",
    "    def get_plot_output_path(self):\n",
    "        return self.plot_output_path\n",
    "\n",
    "    def set_session_ID_counter(self, session_ID_counter):\n",
    "        self.session_ID_counter = session_ID_counter\n",
    "    def get_session_ID_counter(self):\n",
    "        return self.session_ID_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efafdf48-b286-4417-a6b8-0628cc1ea64a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class NWBFolder:\n",
    "    '''\n",
    "    Class for access to folder path to NWB files\n",
    "    '''\n",
    "    folder_path = None\n",
    "    file_list = None\n",
    "\n",
    "    # Static rat-to-probe mapping from Excel\n",
    "    rat_probe_map = {\n",
    "        1: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 9, 'RDHPC': 7, 'LHPCSCREW': 15, 'RHPCSCREW': 3, 'LPFCSCREW': 16, 'RPFCSCREW': 2},\n",
    "        2: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 9, 'RDHPC': 7, 'LHPCSCREW': 15, 'RHPCSCREW': 3, 'LPFCSCREW': 16, 'RPFCSCREW': 2},\n",
    "        3: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 9, 'RDHPC': 7, 'LHPCSCREW': 15, 'RHPCSCREW': 3, 'LPFCSCREW': 16, 'RPFCSCREW': 2},\n",
    "        4: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 9, 'RDHPC': 7, 'LHPCSCREW': 15, 'RHPCSCREW': 3, 'LPFCSCREW': 16, 'RPFCSCREW': 2},\n",
    "        5: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 9, 'RDHPC': 7, 'LHPCSCREW': 15, 'RHPCSCREW': 3, 'LPFCSCREW': 16, 'RPFCSCREW': 2},\n",
    "        6: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 9, 'RDHPC': 7, 'LHPCSCREW': 15, 'RHPCSCREW': 3, 'LPFCSCREW': 16, 'RPFCSCREW': 2},\n",
    "        7: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 9, 'RDHPC': 7, 'LHPCSCREW': 15, 'RHPCSCREW': 3, 'LPFCSCREW': 16, 'RPFCSCREW': 2},\n",
    "        8: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 9, 'RDHPC': 7, 'LHPCSCREW': 15, 'RHPCSCREW': 3, 'LPFCSCREW': 16, 'RPFCSCREW': 2},\n",
    "        9: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        10: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        11: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 2, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        12: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        13: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        14: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        15: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        16: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        17: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        18: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        19: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        20: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 9, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        21: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 16, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        22: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        23: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        24: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 16, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        25: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        26: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        27: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        28: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        29: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        30: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        31: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 16, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        32: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        33: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 16, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        34: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        35: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        36: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        37: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        38: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        39: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        40: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        41: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        42: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        43: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        44: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        45: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        46: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 16, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        47: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        48: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        49: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        50: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        51: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        52: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        53: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 9, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        54: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 16, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        55: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        56: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        57: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        58: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        59: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "        60: {'RAMG': 4, 'RPFC': 1, 'RVHPC': 5, 'RDHPC': 8, 'LHPCSCREW': 12, 'RHPCSCREW': 9, 'LPFCSCREW': 13, 'RPFCSCREW': 2},\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def set_folder_path(cls, path):\n",
    "        cls.folder_path = path\n",
    "\n",
    "    @classmethod\n",
    "    def get_folder_path(cls):\n",
    "        return cls.folder_path\n",
    "\n",
    "    @classmethod\n",
    "    def set_file_list(cls, list):\n",
    "        cls.file_list = list\n",
    "\n",
    "    @classmethod\n",
    "    def get_file_list(cls):\n",
    "        return cls.file_list\n",
    "\n",
    "    @staticmethod\n",
    "    def get_rats_in_file(file_name):\n",
    "        base = file_name.split('_')\n",
    "        if base[0].startswith(\"DFPCT\"):\n",
    "            first_rat = base[0].split('DFPCT')[1]\n",
    "            numbers = [first_rat] + [ b for b in base if b.isdigit()]\n",
    "            return [int(n) for n in numbers if n.isdigit()]\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def get_day_from_file(file):\n",
    "        split_name = file.split('_')\n",
    "        for name_piece in split_name:\n",
    "            if \"Day\" in name_piece:\n",
    "                day = ''.join(filter(str.isdigit, name_piece))\n",
    "                return day, name_piece\n",
    "    \n",
    "    @classmethod\n",
    "    def get_rat_and_probe_from_channel(cls, file_name, global_channel):\n",
    "        # Pull list of rats from working NWB file\n",
    "        rat_list = cls.get_rats_in_file(file_name)\n",
    "\n",
    "        # Convert channel string to int, i.e. 'CSC20' to 20.\n",
    "        if global_channel.startswith(\"CSC\"):\n",
    "            global_channel = int(global_channel[3:])\n",
    "\n",
    "        # Determine what set of 16 channels channel is in, return if invalid\n",
    "        index = (global_channel - 1) // 16\n",
    "        if index >= len(rat_list):\n",
    "            return None, None\n",
    "\n",
    "        # Shift based on bin, include probe i.e. 'RPFCSCREW' if found\n",
    "        rat = rat_list[index]\n",
    "        local_channel = (global_channel - 1) % 16 + 1\n",
    "        for probe, ch in cls.rat_probe_map.get(rat, {}).items():\n",
    "            if ch == local_channel:\n",
    "                return rat, probe\n",
    "        probe = 'NaN Probe'\n",
    "        return rat, probe\n",
    "\n",
    "    @classmethod\n",
    "    def get_channels_from_rat(cls, file_name, rat_number):\n",
    "        rat_list = cls.get_rats_in_file(file_name)\n",
    "        if rat_number not in rat_list:\n",
    "            return None\n",
    "        rat_index = rat_list.index(rat_number)\n",
    "        rat_channels = list(cls.rat_probe_map.get(rat_number, {}).values())\n",
    "\n",
    "        if rat_channels:\n",
    "            for i in range(len(rat_channels)):\n",
    "                rat_channels[i] = rat_index * 16 + rat_channels[i]\n",
    "        return rat_channels\n",
    "\n",
    "    @classmethod\n",
    "    def get_channel_names_from_rat(cls, rat_number):\n",
    "        return list(cls.rat_probe_map.get(rat_number, {}).keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e221a7b-b7b6-4387-9148-e6421b3b1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Class to store global info on project files at runtime\n",
    "NWBFolder_Class = NWBFolder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff8a823-83c0-4960-80a7-43134ea642c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################################\n",
    "# Functions\n",
    "#######################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c22092de-548e-4498-a4db-4d9b33843975",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SYNTHETIC Plotting Functions #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a346d3b2-9ef5-483c-87d7-bc20ed747a08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_custom_pac(f_pha=12, f_amp=75, sf=2000, duration=600, amp_strength=1.0, noise_std=0.5, coupling_strength=1.0):\n",
    "    \"\"\"\n",
    "    Generate a synthetic PAC signal manually using an amplitude-modulated carrier.\n",
    "\n",
    "    Returns:\n",
    "        pac_signal (1D np.array)\n",
    "        time vector (1D np.array)\n",
    "    \"\"\"\n",
    "    n_samples = int(duration * sf)\n",
    "    t = np.arange(n_samples) / sf\n",
    "\n",
    "    # Phase-providing low-frequency signal\n",
    "    phase_signal = 10 * np.sin(2 * np.pi * f_pha * t)\n",
    "\n",
    "    # Amplitude modulation envelope using the low-freq phase\n",
    "    envelope = 1 + coupling_strength * np.sin(2 * np.pi * f_pha * t)\n",
    "\n",
    "    # High-frequency carrier\n",
    "    hf_signal = amp_strength * envelope * np.sin(2 * np.pi * f_amp * t)\n",
    "\n",
    "    # Additive white noise\n",
    "    noise = np.random.normal(0, noise_std, size=n_samples)\n",
    "\n",
    "    pac_signal = hf_signal + noise\n",
    "\n",
    "    return pac_signal, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20e1836e-52f5-4eaf-92c5-7d01f878dfc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_sinusoidal_noise(scaling_factor, t):\n",
    "    # Add additional noise to the signal for more authenticity\n",
    "    power = scaling_factor * np.sin(2 * np.pi * 60 * t)         # 60 Hz interference\n",
    "    power_harmonic = scaling_factor / 2 * np.sin(2 * np.pi * 120 * t)  # 120 Hz harmonic\n",
    "\n",
    "    # Add low-frequency movement noise\n",
    "    low_freq_noise = (\n",
    "        scaling_factor * 2 * np.sin(2 * np.pi * 0.3 * t) +\n",
    "        scaling_factor * np.sin(2 * np.pi * 0.7 * t)\n",
    "    )\n",
    "    \n",
    "    # Add signal drift\n",
    "    drift = 0.5 * np.cumsum(np.random.randn(len(t)))\n",
    "    drift = drift / np.max(np.abs(drift))  # normalize\n",
    "    drift *= scaling_factor  # scale\n",
    "\n",
    "    # --- Add broad-spectrum sinusoidal noise to mimic 1/f ---\n",
    "    broadband_noise = np.zeros_like(t)\n",
    "    \n",
    "    # Loop from 1 Hz to 200 Hz\n",
    "    freqs = np.arange(1, 201) + np.random.uniform(-0.4, 0.4, 200)\n",
    "    size = t.shape\n",
    "    \n",
    "    for f in freqs:\n",
    "        for jitter in np.linspace(-0.1, 0.1, 15):  # 15 small neighboring freqs\n",
    "            freq = f + jitter\n",
    "            amp = scaling_factor / f\n",
    "            phase = 2 * np.pi * np.random.rand()\n",
    "            sine_wave = np.sin(2 * np.pi * freq * t + phase)\n",
    "            white_noise = np.random.normal(0, 0.5, size)\n",
    "            noisy_signal = sine_wave + white_noise\n",
    "            broadband_noise += amp * noisy_signal\n",
    "    \n",
    "    # --- Combine everything ---\n",
    "    sinusoidal_noise = power + power_harmonic + low_freq_noise + drift + broadband_noise\n",
    "    return sinusoidal_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b404b646-a2ef-4ad1-99ba-6034600e877e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_1overf_noise(n_samples, sf, exponent, amplitude, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Generate frequency vector\n",
    "    freqs = np.fft.rfftfreq(n_samples, d=1. / sf)\n",
    "\n",
    "    # Generate white noise spectrum\n",
    "    spectrum = np.random.randn(len(freqs)) + 1j * np.random.randn(len(freqs))\n",
    "\n",
    "    # Avoid division by 0 at DC\n",
    "    freqs[0] = freqs[1]\n",
    "\n",
    "    # 1/f scaling with proper amplitude shaping (magnitude = sqrt(power))\n",
    "    scale = 1.0 / (freqs**(exponent / 2.0))\n",
    "\n",
    "    # Shape the spectrum\n",
    "    shaped_spectrum = spectrum * scale\n",
    "\n",
    "    # Inverse FFT to time domain\n",
    "    shaped_noise = np.fft.irfft(shaped_spectrum, n=n_samples)\n",
    "\n",
    "    # Normalize and scale\n",
    "    shaped_noise -= np.mean(shaped_noise)\n",
    "    shaped_noise /= np.std(shaped_noise)\n",
    "    shaped_noise *= amplitude\n",
    "\n",
    "    return shaped_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6124eb1e-0ed3-451d-9861-477741a72dc1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_plot(t, signal, output_dir, filename, title, ylabel=\"Amplitude\", xlabel=\"Time (s)\", legend_label=None, show_plot=False, figsize=(16, 4)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(t, signal, label=legend_label if legend_label else None, color=\"black\")\n",
    "    #ax.set_ylim(top=0.6, bottom=-0.6)\n",
    "    ax.set_xlabel(xlabel).set_fontproperties(termes_font_bold)\n",
    "    ax.set_ylabel(ylabel).set_fontproperties(termes_font_bold)\n",
    "    ax.set_title(title).set_fontproperties(termes_font_bold)\n",
    "    ax.grid(visible=True, which='major')\n",
    "    if legend_label:\n",
    "        ax.legend()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    fig.savefig(filepath + \".png\", dpi=600)\n",
    "    fig.savefig(filepath + \".pdf\")\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acf08c1f-8ebf-4a3d-a03c-b84d11f6f9e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_minimal_traces(traces, labels, sampling_rate, output_path, figure_letter, name):\n",
    "    \"\"\"\n",
    "    Plot multiple EEG/LFP traces in minimal publication style.\n",
    "\n",
    "    Parameters:\n",
    "        traces         : list of 1D numpy arrays (voltage signals)\n",
    "        labels         : list of strings (region/channel names)\n",
    "        sampling_rate  : Hz (float)\n",
    "        output_path    : full path to save figure (e.g., 'Figure_A.png')\n",
    "    \"\"\"\n",
    "    n_traces = len(traces)\n",
    "    trace_length = len(traces[0])\n",
    "    duration = trace_length / sampling_rate\n",
    "    time = np.linspace(0, duration, trace_length)\n",
    "\n",
    "    # Define spacing between traces\n",
    "    amplitude = max(np.ptp(tr) for tr in traces)\n",
    "    bottom_amplitude = np.ptp(traces[0])\n",
    "    spacing = amplitude * 1.5\n",
    "    \n",
    "    # Prepare figure\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))  # Adjust size as needed\n",
    "    for i, trace in enumerate(traces):\n",
    "        offset = i * spacing\n",
    "        ax.plot(time, trace + offset, color='black', linewidth=0.75)\n",
    "\n",
    "        # Add label to the left\n",
    "        txt = ax.text(-0.05 * duration, \n",
    "                offset, \n",
    "                labels[i], \n",
    "                va='center', \n",
    "                ha='right', \n",
    "                fontsize=10, \n",
    "                rotation='vertical')\n",
    "        txt.set_fontproperties(termes_font_bold)\n",
    "        txt.set_fontsize(16)\n",
    "\n",
    "    bar_left = duration - (duration / 5) + (duration / 25)\n",
    "    bar_right = duration + (duration / 25)\n",
    "    bar_bottom = 0 - bottom_amplitude * 1\n",
    "    bar_top = bottom_amplitude * 0.7\n",
    "    x_center = (bar_right - bar_left) / 2\n",
    "    y_center = (bar_top - bar_bottom) / 2\n",
    "    # Position bars\n",
    "    ax.plot([bar_left, bar_right], [bar_bottom, bar_bottom], color='black', linewidth=2)\n",
    "    ax.plot([bar_right, bar_right], [bar_bottom, bar_top], color='black', linewidth=2)\n",
    "\n",
    "    xscale = ax.text(bar_left + x_center, \n",
    "            bar_bottom + (bar_bottom / 10), \n",
    "            f'{round(bar_right - bar_left, 1)} s', \n",
    "            ha='center', \n",
    "            va='top', \n",
    "            fontsize=9)\n",
    "    xscale.set_fontproperties(termes_font_bold)\n",
    "    xscale.set_fontsize(16)\n",
    "    \n",
    "    yscale = ax.text(bar_right + (bar_right / 25), \n",
    "            bar_bottom + y_center, \n",
    "            f'{round(bar_top - bar_bottom, 1)} mV', \n",
    "            ha='right', \n",
    "            va='center', \n",
    "            rotation='vertical', \n",
    "            fontsize=9)\n",
    "    yscale.set_fontproperties(termes_font_bold)\n",
    "    yscale.set_fontsize(16)\n",
    "    \n",
    "    letter = ax.text(-0.05 * duration, \n",
    "                     len(traces) * (amplitude * 1.35), \n",
    "                     f'{figure_letter}', \n",
    "                     ha='center', \n",
    "                     va='top',  \n",
    "                     fontsize=32)\n",
    "\n",
    "    letter.set_fontproperties(termes_font_bold)\n",
    "    letter.set_fontsize(32)\n",
    "\n",
    "    # Remove all axis elements\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filepath = os.path.join(output_path, f\"synthetic_multiplot_{name}\")\n",
    "    fig.savefig(filepath + \".png\", dpi=900, bbox_inches='tight')\n",
    "    fig.savefig(filepath + \".pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccca8909-0757-4a29-9236-3dfd824439d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_PAC_of_channels_SYNTHETIC(Analysis_class):\n",
    "    \"\"\"\n",
    "    This function processes PAC on channels individually. It uses synthetic data to\n",
    "    validate a correct process for detected PAC.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve PAC settings\n",
    "        (pha_freqs, amp_freqs, idpac, dcomplex, cycles, width, n_bins,\n",
    "         sampling_rate, epoch_len, data_length, minimum_length, parallel_processes,\n",
    "         surrogate_permutations, notch_freqs, high_pass_filter, detrend_epochs,\n",
    "         apply_autofilter, downsample_rate, seed, rejection_threshold, skip_PAC, \n",
    "         inject_PAC)= Analysis_class.get_all_PAC_parameters()\n",
    "\n",
    "        # Prepare IO paths\n",
    "        timestamp = datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "        folder_path = Analysis_class.get_folder_path\n",
    "        nwb_list = Analysis_class.get_nwb_list()\n",
    "        channels = Analysis_class.get_selected_channels()\n",
    "\n",
    "        output_directory = Analysis_class.get_output_directory()\n",
    "        output_folder_name = Analysis_class.get_output_folder_name()\n",
    "        if \"[Timestamp]\" in output_folder_name:\n",
    "            output_folder_name = output_folder_name.split(\"[Timestamp]\")[0]\n",
    "            output_folder_name = f\"{output_folder_name}{timestamp} SYNTHETIC TEST\"\n",
    "        \n",
    "        output_directory = os.path.join(output_directory, output_folder_name)\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        #=========================================================================================================\n",
    "        ### Make raw data ###\n",
    "\n",
    "        # TensorPAC method\n",
    "        f_pha = 12     # frequency phase for the coupling\n",
    "        f_amp = 75     # frequency amplitude for the coupling\n",
    "        n_epochs = 30  # number of trials\n",
    "        epoch_len = 20 # length of epoch in seconds\n",
    "        sf = 2000.     # sampling frequency\n",
    "        n_times = int(epoch_len * sf)  # number of time points\n",
    "        sampling_rate = sf\n",
    "        tensorpac_epochs, time = pac_signals_wavelet(sf=sf, f_pha=f_pha, f_amp=f_amp, noise=1.0,\n",
    "                                      n_epochs=n_epochs, n_times=n_times)\n",
    "        raw_data = tensorpac_epochs.flatten()\n",
    "        \n",
    "        # Manual method\n",
    "        #raw_data, _ = generate_custom_pac(f_pha, f_amp, sf, n_epochs * epoch_len)\n",
    "\n",
    "        # Scale synthetic PAC signal to realistic amplitude (e.g., ±0.5 mV)\n",
    "        target_peak_to_peak = 10  # mV total span (±5 mV)\n",
    "        actual_ptp = np.ptp(raw_data)\n",
    "        scale_factor = target_peak_to_peak / actual_ptp\n",
    "        raw_data *= scale_factor * 0.015 # Adjust const to weaken coupling\n",
    "        \n",
    "        #=========================================================================================================\n",
    "        ### Define variables for plotting ###\n",
    "\n",
    "        # Generate plot of synthetic data\n",
    "        duration_sec = raw_data.shape[0] / sampling_rate\n",
    "        t = np.linspace(0, duration_sec, raw_data.shape[0], endpoint=False)\n",
    "        \n",
    "        # Define how many seconds to view\n",
    "        view_seconds = 2\n",
    "        wide_view_scaling = 5\n",
    "        samples_to_plot = int(view_seconds * sampling_rate)\n",
    "        \n",
    "        # Generate time vector for plotting\n",
    "        t_plot = t[:samples_to_plot]\n",
    "        t_plot_wide = t[:samples_to_plot * wide_view_scaling]\n",
    "\n",
    "        # Save current data\n",
    "        raw_unfiltered = raw_data.copy()\n",
    "        signal_01_plot = raw_unfiltered[:samples_to_plot]\n",
    "        signal_01_plot_wide = raw_unfiltered[:samples_to_plot * wide_view_scaling]\n",
    "\n",
    "        #=========================================================================================================\n",
    "        ### Add noise to signal ###\n",
    "        \n",
    "        scaling_factor = 0.025\n",
    "\n",
    "        # Compute noise using both custom methods\n",
    "        aperiodic = generate_1overf_noise(n_samples=len(t), sf=sf, exponent=1, amplitude=2.0)\n",
    "        # Add additional noise to the signal for more authenticity\n",
    "        power = scaling_factor * np.sin(2 * np.pi * 60 * t)         # 60 Hz interference\n",
    "        power_harmonic = scaling_factor * 0.6 * np.sin(2 * np.pi * 120 * t)  # 120 Hz harmonic\n",
    "        power_harmonic_2 = scaling_factor * 0.3 * np.sin(2 * np.pi * 180 * t)  # 120 Hz harmonic\n",
    "        \n",
    "        #sinusoidal_noise = generate_sinusoidal_noise(scaling_factor, t)\n",
    "\n",
    "        # Combine with coupled data\n",
    "        raw_data += scaling_factor * aperiodic\n",
    "        raw_data += power + power_harmonic + power_harmonic_2\n",
    "\n",
    "        # Save current data\n",
    "        raw_noisy = raw_data.copy()\n",
    "        signal_02_plot = raw_noisy[:samples_to_plot]\n",
    "        signal_02_plot_wide = raw_noisy[:samples_to_plot * wide_view_scaling]\n",
    "        \n",
    "        #=========================================================================================================\n",
    "        ### Apply notch filter ###\n",
    "\n",
    "        # Wrap raw data into MNE object for less manual processing steps\n",
    "        info = mne.create_info([\"Synthetic Data\"], sampling_rate, ch_types=\"eeg\")\n",
    "        raw = mne.io.RawArray(raw_data[np.newaxis, :], info, verbose=False)\n",
    "\n",
    "        # Apply notch filters\n",
    "        notch_freqs = [60, 120, 180, 240, 300]\n",
    "        raw.notch_filter(freqs=notch_freqs, picks=\"Synthetic Data\", method='fir', verbose=False)\n",
    "\n",
    "        # Save current data\n",
    "        raw_after_notch = raw.get_data()\n",
    "        signal_03_plot = raw_after_notch[0, :samples_to_plot]\n",
    "        signal_03_plot_wide = raw_after_notch[0, :samples_to_plot * wide_view_scaling]\n",
    "        \n",
    "        #=========================================================================================================\n",
    "        ### Apply bandpass, downsample, detrend ###\n",
    "\n",
    "        # Apply bandpass filter\n",
    "        raw.filter(l_freq=1.0, h_freq=200, picks=\"Synthetic Data\", verbose=False)\n",
    "\n",
    "        # Save current data\n",
    "        raw_after_bandpass = raw.get_data()\n",
    "        signal_04_plot = raw_after_bandpass[0, :samples_to_plot]\n",
    "        signal_04_plot_wide = raw_after_bandpass[0, :samples_to_plot * wide_view_scaling]\n",
    "\n",
    "        # Downsample data\n",
    "        if downsample_rate:\n",
    "            raw.resample(sfreq=downsample_rate)\n",
    "        downsampled_sampling_rate = raw.info['sfreq']\n",
    "\n",
    "        # Epoch data\n",
    "        events = mne.make_fixed_length_events(raw, duration=epoch_len)\n",
    "        epochs = mne.Epochs(raw, events, tmin=0, tmax=epoch_len, baseline=None, preload=True)\n",
    "    \n",
    "        # Detrend data\n",
    "        if detrend_epochs:\n",
    "            data = epochs.get_data()\n",
    "            order = 1\n",
    "            data_detrended = mne.filter.detrend(data, order=order)\n",
    "            cleaned_epochs = data_detrended.squeeze(axis=1)\n",
    "        else:\n",
    "            cleaned_epochs = epochs.get_data().squeeze(axis=1)\n",
    "        \n",
    "        #=========================================================================================================\n",
    "        ### Compute PSD using MNE welch method ###\n",
    "        \n",
    "        nperseg = int(sf * 8)\n",
    "        scaled_raw_data = raw_noisy[np.newaxis, :] * 1000\n",
    "        scaled_filter_data = raw_after_bandpass * 1000\n",
    "        psd_raw, freqs_raw = psd_array_welch(scaled_raw_data, sfreq=sf, n_fft=nperseg, fmin=0, fmax=200, verbose=False)\n",
    "        psd_filter, freqs_filter = psd_array_welch(scaled_filter_data, sfreq=sf, n_fft=nperseg, fmin=0, fmax=200, verbose=False)\n",
    "        \n",
    "        # Convert to dB\n",
    "        psd_raw_db = 10 * np.log10(np.where(psd_raw[0] > 0, psd_raw[0], np.nan))\n",
    "        psd_filter_db = 10 * np.log10(np.where(psd_filter[0] > 0, psd_filter[0], np.nan))\n",
    "\n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.plot(freqs_raw, psd_raw_db, color=\"red\", label=\"Raw\")\n",
    "        ax.plot(freqs_filter, psd_filter_db, label=\"Filtered\")\n",
    "        title = ax.set_title(\"Synthetic Data PSD\")\n",
    "        xl = ax.set_xlabel(\"Frequency (Hz)\")\n",
    "        yl = ax.set_ylabel(\"Power Spectral Density (dB)\")\n",
    "        title.set_fontproperties(termes_font_bold)\n",
    "        title.set_fontsize(24)\n",
    "        xl.set_fontproperties(termes_font_bold)\n",
    "        xl.set_fontsize(18)\n",
    "        yl.set_fontproperties(termes_font_bold)\n",
    "        yl.set_fontsize(18)\n",
    "        ax.set_ylim(top=35, bottom=0)\n",
    "        legend = ax.legend()\n",
    "        for text in legend.get_texts():\n",
    "            text.set_fontproperties(termes_font)\n",
    "            text.set_fontsize(14)\n",
    "        for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "            label.set_fontproperties(termes_font)\n",
    "            label.set_fontsize(14)\n",
    "        ax.grid(True)\n",
    "        base_name = \"raw_synthetic_PSD\"\n",
    "        fig_path = os.path.join(output_directory, base_name + \".png\")\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(fig_path, dpi=600) \n",
    "        fig_path = os.path.join(output_directory, base_name + \".pdf\")\n",
    "        fig.savefig(fig_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        #=========================================================================================================\n",
    "        ### Plot Results ###\n",
    "\n",
    "        # Short plots\n",
    "        save_plot(\n",
    "            t_plot, signal_01_plot, output_directory,\n",
    "            filename=\"01_raw_synthetic_signal\",\n",
    "            title=f\"Raw Synthetic Coupled Signal (First {view_seconds} sec)\",\n",
    "        )\n",
    "\n",
    "        save_plot(\n",
    "            t_plot, signal_02_plot, output_directory,\n",
    "            filename=\"02_noisy_synthetic_signal\",\n",
    "            title=f\"Noisy Synthetic Coupled Signal (First {view_seconds} sec)\",\n",
    "        )\n",
    "\n",
    "        save_plot(\n",
    "            t_plot, signal_03_plot, output_directory,\n",
    "            filename=\"03_notch_filter_synthetic_signal\",\n",
    "            title=f\"Notch Filter Synthetic Coupled Signal (First {view_seconds} sec)\",\n",
    "        )\n",
    "\n",
    "        save_plot(\n",
    "            t_plot, signal_04_plot, output_directory,\n",
    "            filename=\"04_bandpass_filter_synthetic_signal\",\n",
    "            title=f\"Bandpass Filter Synthetic Coupled Signal (First {view_seconds} sec)\",\n",
    "        )\n",
    "\n",
    "        traces = [signal_04_plot, signal_03_plot, signal_02_plot, signal_01_plot]\n",
    "        labels = ['Bandpass', 'Notch', 'Noisy', 'Raw' ]\n",
    "        plot_minimal_traces(traces, labels, sf, output_directory, \"A\", \"short\")\n",
    "\n",
    "        # Wide plots\n",
    "        save_plot(\n",
    "            t_plot_wide, signal_01_plot_wide, output_directory,\n",
    "            filename=\"wide_01_raw_synthetic_signal\",\n",
    "            title=f\"Raw Synthetic Coupled Signal (First {view_seconds * wide_view_scaling} sec)\",\n",
    "        )\n",
    "\n",
    "        save_plot(\n",
    "            t_plot_wide, signal_02_plot_wide, output_directory,\n",
    "            filename=\"wide_02_noisy_synthetic_signal\",\n",
    "            title=f\"Noisy Synthetic Coupled Signal (First {view_seconds * wide_view_scaling} sec)\",\n",
    "        )\n",
    "\n",
    "        save_plot(\n",
    "            t_plot_wide, signal_03_plot_wide, output_directory,\n",
    "            filename=\"wide_03_notch_filter_synthetic_signal\",\n",
    "            title=f\"Notch Filter Synthetic Coupled Signal (First {view_seconds * wide_view_scaling} sec)\",\n",
    "        )\n",
    "\n",
    "        save_plot(\n",
    "            t_plot_wide, signal_04_plot_wide, output_directory,\n",
    "            filename=\"wide_04_bandpass_filter_synthetic_signal\",\n",
    "            title=f\"Bandpass Filter Synthetic Coupled Signal (First {view_seconds * wide_view_scaling} sec)\",\n",
    "        )\n",
    "\n",
    "        traces = [signal_04_plot_wide, signal_03_plot_wide, signal_02_plot_wide, signal_01_plot_wide]\n",
    "        labels = ['Bandpass', 'Notch', 'Noisy', 'Raw' ]\n",
    "        plot_minimal_traces(traces, labels, sf, output_directory, \"B\", \"wide\")\n",
    "        \n",
    "\n",
    "        #=========================================================================================================\n",
    "        def compute_PAC(data, name, sampling_rate, minimal_plots=False):\n",
    "            # Initialize TensorPAC object\n",
    "            PAC_method, surg_method, norm_method = idpac\n",
    "            p = Pac(idpac=idpac, f_pha=pha_freqs, f_amp=amp_freqs, dcomplex=dcomplex, cycle=cycles, width=width, n_bins=n_bins, verbose=False)\n",
    "            \n",
    "            phases     = p.filter(sampling_rate, data, ftype='phase',     n_jobs=parallel_processes)\n",
    "            amplitudes = p.filter(sampling_rate, data, ftype='amplitude', n_jobs=parallel_processes)\n",
    "            pac_map    = p.fit(phases, amplitudes, n_perm=surrogate_permutations, n_jobs=parallel_processes)\n",
    "            del phases, amplitudes\n",
    "    \n",
    "            # Generate comodulogram\n",
    "            pvals = p.infer_pvalues(p=0.05)\n",
    "            comod = pac_map.mean(-1)\n",
    "        \n",
    "            # Plot Comodulogram\n",
    "            #plt.figure(figsize=(6,5)) if minimal_plots else plt.figure(figsize=(6,5))\n",
    "            fig = plt.figure(figsize=(5.236,5), constrained_layout=True)\n",
    "            fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.01, wspace=0.01, hspace=0.01)\n",
    "\n",
    "            # Hide axis labels in minimal mode; keep ticks + title\n",
    "            xlabel = None if minimal_plots else \"Frequency for Phase (Hz)\"\n",
    "            ylabel = None if minimal_plots else \"Frequency for Amplitude (Hz)\"\n",
    "            \n",
    "            p.comodulogram(comod,\n",
    "                           title=f\"{name}\",\n",
    "                           xlabel=xlabel,\n",
    "                           ylabel=ylabel,\n",
    "                           fz_labels=10,\n",
    "                           cmap='viridis', \n",
    "                           colorbar=False,\n",
    "                           vmin=-1.96,\n",
    "                           vmax=1.96,\n",
    "                           interp=None)\n",
    "    \n",
    "            # Grab the axis and apply fonts\n",
    "            ax = plt.gca()\n",
    "\n",
    "            im = ax.images[0] if ax.images else ax.collections[0]  # the heatmap\n",
    "            if not minimal_plots:\n",
    "                cbar = ax.figure.colorbar(\n",
    "                    im, ax=ax,\n",
    "                    orientation=\"horizontal\",\n",
    "                    location=\"bottom\",   # Matplotlib ≥3.6; omit if older\n",
    "                    pad=0.02,            # distance from axes\n",
    "                    fraction=0.045,      # size of colorbar relative to axes\n",
    "                    aspect=40            # length/thickness ratio\n",
    "                )\n",
    "                # Label font\n",
    "                cbar.set_label(\"Z-Score\", fontproperties=termes_font_bold, fontsize=24)\n",
    "                \n",
    "                # Tick label font/size\n",
    "                if cbar.orientation == \"horizontal\":\n",
    "                    cbar.ax.xaxis.labelpad = 2    # tighten spacing if desired\n",
    "                    for lab in cbar.ax.get_xticklabels():\n",
    "                        lab.set_fontproperties(termes_font)\n",
    "                        lab.set_fontsize(12)\n",
    "                else:\n",
    "                    cbar.ax.yaxis.labelpad = 2\n",
    "                    for lab in cbar.ax.get_yticklabels():\n",
    "                        lab.set_fontproperties(termes_font)\n",
    "                        lab.set_fontsize(12)\n",
    "            \n",
    "            # Title and axis labels\n",
    "            ax.set_title(ax.get_title(), fontproperties=termes_font_bold, fontsize=24)\n",
    "            if not minimal_plots:\n",
    "                ax.set_xlabel(ax.get_xlabel(), fontproperties=termes_font_bold, fontsize=16)\n",
    "                ax.set_ylabel(ax.get_ylabel(), fontproperties=termes_font_bold, fontsize=16)\n",
    "            \n",
    "            # Tick labels\n",
    "            for tick in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "                tick.set_fontproperties(termes_font)\n",
    "                tick.set_fontsize(12)\n",
    "            \n",
    "            PAC_method = Analysis_class.get_PAC_method()\n",
    "            norm_method = Analysis_class.get_normalization_method()\n",
    "    \n",
    "            # Set Colormap name based on PAC settings\n",
    "            if PAC_method == 'Mean Vector Length':\n",
    "                PAC_method = 'MVL'\n",
    "            elif PAC_method == 'Modulation Index':\n",
    "                PAC_method = 'MI'\n",
    "            elif PAC_method == 'Heights Ratio':\n",
    "                PAC_method = 'HR'\n",
    "            elif PAC_method == 'ndPAC':\n",
    "                pass\n",
    "            elif PAC_method == 'Phase-Locking Value':\n",
    "                PAC_method = 'PLV'\n",
    "            elif PAC_method == 'Gaussian Copula PAC':\n",
    "                PAC_method = 'GC PAC'\n",
    "    \n",
    "            if norm_method == 'No Normalization':\n",
    "                norm_method = ''\n",
    "            elif norm_method == 'Subtract Mean of Surrogtes':\n",
    "                norm_method = 'Sub Mean of Surg.'\n",
    "            elif norm_method == 'Divide Mean of Surrogates':\n",
    "                norm_method = 'Div Mean of Surg.'\n",
    "            elif norm_method == 'Sub+Div Mean of Surrogates':\n",
    "                norm_method = 'Sub+Div Mean of Surg.'\n",
    "            elif norm_method == 'Z-score':\n",
    "                norm_method = 'Z-Score'\n",
    "            \n",
    "            # Colorbar (if shown)\n",
    "            if ax.images and not minimal_plots:\n",
    "                cbar = ax.figure.axes[-1]  # Last axis is usually colorbar\n",
    "                if norm_method == 'Z-Score':\n",
    "                    cbar.set_ylabel(norm_method, fontproperties=termes_font_bold, fontsize=16)\n",
    "                    #cbar.set_fontsize(16)\n",
    "                else:\n",
    "                    cbar.set_ylabel(f\"{PAC_method} ({norm_method})\", fontproperties=termes_font_bold, fontsize=16)\n",
    "                    #cbar.set_fontsize(16)\n",
    "                for label in cbar.get_yticklabels():\n",
    "                    label.set_fontproperties(termes_font)\n",
    "                    label.set_fontsize(12)\n",
    "    \n",
    "            # Save comodulogram\n",
    "            base_name = f\"{name}_PAC\"\n",
    "            fig_path = os.path.join(output_directory, base_name + \".pdf\")\n",
    "            plt.savefig(fig_path, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "            fig_path = os.path.join(output_directory, base_name + \".png\")\n",
    "            plt.savefig(fig_path, dpi=600, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "            \n",
    "\n",
    "        # Build raw arrays\n",
    "        info = mne.create_info([\"Synthetic Data\"], sampling_rate, ch_types=\"eeg\")\n",
    "        mne_raw_noisy = mne.io.RawArray(raw_noisy[np.newaxis, :], info, verbose=False)\n",
    "        mne_filter_notch = mne.io.RawArray(raw_after_notch, info, verbose=False)\n",
    "        mne_filter_bandpass = mne.io.RawArray(raw_after_bandpass, info, verbose=False)\n",
    "\n",
    "        # Create events per object\n",
    "        ev_noisy = mne.make_fixed_length_events(mne_raw_noisy, duration=epoch_len)\n",
    "        ev_notch = mne.make_fixed_length_events(mne_filter_notch, duration=epoch_len)\n",
    "        ev_bp = mne.make_fixed_length_events(mne_filter_bandpass, duration=epoch_len)\n",
    "                \n",
    "        epochs_raw_noisy = mne.Epochs(mne_raw_noisy, ev_noisy, tmin=0, tmax=epoch_len, baseline=None, preload=True)\n",
    "        epochs_filter_notch = mne.Epochs(mne_filter_notch, ev_notch, tmin=0, tmax=epoch_len, baseline=None, preload=True)\n",
    "        epochs_filter_bandpass = mne.Epochs(mne_filter_bandpass, ev_bp, tmin=0, tmax=epoch_len, baseline=None, preload=True)\n",
    "        \n",
    "        compute_PAC(epochs_raw_noisy.get_data().squeeze(axis=1), \"Raw Noisy Signal\", sampling_rate, minimal_plots=True)\n",
    "        compute_PAC(epochs_filter_notch.get_data().squeeze(axis=1), \"Add Notch Filter\", sampling_rate, minimal_plots=True)\n",
    "        compute_PAC(epochs_filter_bandpass.get_data().squeeze(axis=1), \"Add Bandpass Filter\", sampling_rate, minimal_plots=True)\n",
    "        compute_PAC(cleaned_epochs, \"Add Downsample & Detrend\", downsampled_sampling_rate, minimal_plots=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "            logging.error(f\"Error in compute_PAC_of_channels: {e}\", exc_info=True)\n",
    "            show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dc86a8f-7249-476b-8996-5cb72d3a2d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### JSON Plotting Functions #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1086735-a235-4ec2-9271-bc4806879e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSD Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "552fb190-00a1-40c2-b80f-53470cd71e5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_rows(d):\n",
    "    rows = []\n",
    "    for key, item in d[\"Data\"].items():\n",
    "        info   = item[\"Info\"]\n",
    "        rat    = info[\"Rat\"]\n",
    "        day    = info[\"Day\"]                    # e.g. \"Day0\"\n",
    "        sess   = info[\"Session\"]                # e.g. \"0001\"\n",
    "        probe  = info[\"Probe\"]                  # e.g. \"RAMG\"\n",
    "\n",
    "        bio = item[\"BiomarkerSummary\"]\n",
    "        for metric, val in bio.items():         # Broadband_dB, ThetaDelta_dB, …\n",
    "            if isinstance(val, dict):           # handles {\"Raw\": .., \"Filtered\": ..}\n",
    "                for subk, v in val.items():\n",
    "                    rows.append([rat, day, sess, probe,\n",
    "                                 f\"{metric}_{subk}\",   # → Broadband_dB_Raw\n",
    "                                 float(v)])\n",
    "            else:                               # single float\n",
    "                rows.append([rat, day, sess, probe,\n",
    "                             metric, float(val)])\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ca32557-7b82-4a81-b867-d7b9c966542e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_case_json(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        raw = json.load(f)\n",
    "    rows = extract_rows(raw)\n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        \"Rat\", \"Day\", \"Session\", \"Probe\", \"Metric\", \"Value\"\n",
    "    ])\n",
    "\n",
    "    # numeric day for sorting (Day0→0, Day1→1, etc.)\n",
    "    df[\"DayNum\"] = df[\"Day\"].str.extract(r\"Day(\\d+)\").astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ad0d39-f619-4753-822c-e69f4b1fd42f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_trajectories(json_path,\n",
    "                      metrics=(\"Broadband_dB_Raw\", \"ThetaDelta_dB_Raw\"),\n",
    "                      probes=None,\n",
    "                      agg_fn=\"mean\",\n",
    "                      save=False,\n",
    "                      output_dir=None,\n",
    "                      palette=None):\n",
    "\n",
    "    # ---- 1) hard-coded colors (edit here) --------------------------\n",
    "    SERIES_COLOR = \"#111111\"                   # one color for all figures\n",
    "    # Or metric-specific:\n",
    "    METRIC_COLORS = {\n",
    "        \"Broadband_dB_Raw\":   \"#111111\",\n",
    "        \"ThetaDelta_dB_Raw\":  \"#1f78b4\",\n",
    "        \"Aperiodic_Exponent\": \"#e31a1c\",\n",
    "    }\n",
    "        \n",
    "    df = load_case_json(json_path)\n",
    "\n",
    "    # ---- Day0 session split and even-spacing keys ----------------\n",
    "    # D0 sessions become D0:0..D0:3 using last digit; others become Day N\n",
    "    def time_key(row):\n",
    "        d = row[\"DayNum\"]\n",
    "        if d == 0:\n",
    "            return f\"D0:{int(str(row['Session'])[-1])}\"\n",
    "        return f\"Day {d}\"\n",
    "\n",
    "    df[\"TimeKey\"] = df.apply(time_key, axis=1)\n",
    "\n",
    "    # Desired order (only keep keys that exist in this file)\n",
    "    desired_order = [\"D0:0\", \"D0:1\", \"D0:2\", \"D0:3\",\n",
    "                     \"Day 1\", \"Day 3\", \"Day 7\", \"Day 14\"]\n",
    "    present = [k for k in desired_order if k in df[\"TimeKey\"].unique()]\n",
    "    df[\"TimeKey\"] = pd.Categorical(df[\"TimeKey\"], categories=present, ordered=True)\n",
    "\n",
    "    if probes is not None:\n",
    "        df = df[df[\"Probe\"].isin(probes)]\n",
    "\n",
    "    # ---- aggregate: mean ± SEM over rats/probes available --------\n",
    "    agg = df.groupby([\"TimeKey\", \"Metric\"]).agg(\n",
    "        mean=(\"Value\", \"mean\"),\n",
    "        sem =(\"Value\", lambda x: x.std(ddof=1) / np.sqrt(len(x)))\n",
    "    ).reset_index()\n",
    "\n",
    "    # ---- output folder (once per call) ---------------------------\n",
    "    if save and output_dir:\n",
    "        output_dir = Path(output_dir)\n",
    "        timestamp      = datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "        session_folder = output_dir / f\"PSD JSON Statistics {timestamp}\"\n",
    "        session_folder.mkdir(parents=True, exist_ok=True)\n",
    "        stats_path = session_folder / f\"{Path(json_path).stem}_trajectory_stats.tsv\"\n",
    "        stats_rows = []            # collect rows, write at the end\n",
    "\n",
    "    # ---- plotting ------------------------------------------------\n",
    "    for metric in metrics:\n",
    "        # Messy exception for the TDR since it should be linear not logarithmic\n",
    "        if metric in (\"ThetaDelta_dB_Raw\", \"ThetaDelta_dB_Filtered\"):\n",
    "            \"\"\"\n",
    "            # Convert EACH value from dB → linear ratio, then aggregate\n",
    "            sub_df = df[df[\"Metric\"] == metric].copy()\n",
    "            sub_df[\"Value_lin\"] = 10 ** (sub_df[\"Value\"] / 10.0)\n",
    "    \n",
    "            g = sub_df.groupby(\"TimeKey\")[\"Value_lin\"]\n",
    "            sub = g.mean().reset_index(name=\"mean\")\n",
    "            sub[\"sem\"] = g.std(ddof=1) / np.sqrt(g.count())\n",
    "    \n",
    "            sub = sub.sort_values(\"TimeKey\")\n",
    "            x_labels = sub[\"TimeKey\"].astype(str).tolist()\n",
    "            x_pos    = np.arange(len(x_labels))\n",
    "            y        = sub[\"mean\"].to_numpy()\n",
    "            e        = sub[\"sem\"].to_numpy()\n",
    "            \"\"\"\n",
    "\n",
    "            sub_df = df[df[\"Metric\"] == metric].copy()\n",
    "            sub_df[\"Value_lin\"] = 10 ** (sub_df[\"Value\"] / 10.0)\n",
    "            \n",
    "            grp = (\n",
    "                sub_df.groupby(\"TimeKey\")[\"Value_lin\"]\n",
    "                .agg(mean=\"mean\", std=\"std\", n=\"count\")\n",
    "            )\n",
    "            \n",
    "            grp[\"sem\"] = grp[\"std\"] / np.sqrt(grp[\"n\"])\n",
    "            grp[\"sem\"] = grp[\"sem\"].fillna(0.0)     # in case n==1 → std NaN\n",
    "            \n",
    "            sub = grp.reset_index().sort_values(\"TimeKey\")\n",
    "            \n",
    "            x_labels = sub[\"TimeKey\"].astype(str).tolist()\n",
    "            x_pos    = np.arange(len(x_labels))\n",
    "            y        = sub[\"mean\"].to_numpy(dtype=float)\n",
    "            e        = sub[\"sem\"].to_numpy(dtype=float)\n",
    "\n",
    "            # record rows for the stats table\n",
    "            for _, row in sub.iterrows():\n",
    "                stats_rows.append({\n",
    "                    \"Metric\"  : metric.replace(\"_\", \" \"),\n",
    "                    \"TimeKey\" : row[\"TimeKey\"],\n",
    "                    \"Mean\"    : row[\"mean\"],\n",
    "                    \"SEM\"     : row[\"sem\"],\n",
    "                    \"N\"       : row[\"n\"] if \"n\" in row else np.nan   # optional\n",
    "                })\n",
    "    \n",
    "            fig = plt.figure(figsize=(10, 5), constrained_layout=True)\n",
    "            fig.set_constrained_layout_pads(w_pad=0.05, h_pad=0.05, wspace=0.05, hspace=0.05)\n",
    "            color = METRIC_COLORS.get(metric, SERIES_COLOR)\n",
    "            plt.errorbar(x_pos, y, yerr=e, marker=\"o\", linestyle=\"-\",\n",
    "                         color=color, capsize=4)\n",
    "\n",
    "            title = f\"Trajectory of {metric}\"\n",
    "            ylabel = metric.replace(\"_\", \" \")\n",
    "            if metric == \"ThetaDelta_dB_Raw\":\n",
    "                title = \"PSD Theta-Delta Ratio (4-12 Hz, 1-4 Hz)\"\n",
    "                ylabel = \"Theta-Delta Ratio (Unitless)\"\n",
    "    \n",
    "            fig = plt.gcf(); ax = plt.gca()\n",
    "            xl = ax.set_xlabel(\"Time-point\", fontproperties=termes_font_bold)\n",
    "            yl = ax.set_ylabel(f\"{ylabel}\", fontproperties=termes_font_bold)\n",
    "            t = ax.set_title(f\"{title}\", fontproperties=termes_font_bold)\n",
    "            xl.set_fontsize(20)\n",
    "            yl.set_fontsize(20)\n",
    "            t.set_fontsize(26)\n",
    "\n",
    "            # Hard-coded fix to label actualy shown on x-axis, donn't want to disturb df access\n",
    "            x_map = {\"D0:0\": \"Control Case\", \"D0:1\": \"DFP Inj.\", \"D0:2\": \"MDZ Interv.\"}\n",
    "            x_labels_shown = [x_map.get(lbl, lbl) for lbl in x_labels]\n",
    "\n",
    "            ax.set_xticks(x_pos, x_labels_shown)\n",
    "            for lab in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "                lab.set_fontproperties(termes_font)\n",
    "                lab.set_fontsize(16)\n",
    "            ax.grid(True, alpha=.3)\n",
    "            # Optional: clamp y to start at 0\n",
    "            ax.set_ylim(bottom=0)\n",
    "    \n",
    "            fig.tight_layout()\n",
    "    \n",
    "            if save:\n",
    "                png_name = session_folder / f\"{Path(json_path).stem}_{metric}_LINEAR.png\"\n",
    "                pdf_name = session_folder / f\"{Path(json_path).stem}_{metric}_LINEAR.pdf\"\n",
    "                plt.savefig(png_name, dpi=600, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "                plt.savefig(pdf_name, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "                plt.close(fig)\n",
    "            else:\n",
    "                plt.show()\n",
    "                plt.close(fig)\n",
    "    \n",
    "            continue  # skip the default dB path below\n",
    "        \n",
    "        sub = agg[agg[\"Metric\"] == metric].sort_values(\"TimeKey\")\n",
    "        x_labels = sub[\"TimeKey\"].astype(str).tolist()\n",
    "        x_pos    = np.arange(len(x_labels))            # even spacing\n",
    "        y        = sub[\"mean\"].to_numpy()\n",
    "        e        = sub[\"sem\"].to_numpy()\n",
    "\n",
    "        # record rows for the stats table\n",
    "        for _, row in sub.iterrows():\n",
    "            stats_rows.append({\n",
    "                \"Metric\"  : metric.replace(\"_\", \" \"),\n",
    "                \"TimeKey\" : row[\"TimeKey\"],\n",
    "                \"Mean\"    : row[\"mean\"],\n",
    "                \"SEM\"     : row[\"sem\"],\n",
    "                \"N\"       : row[\"n\"] if \"n\" in row else np.nan   # optional\n",
    "            })\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 5), constrained_layout=True)\n",
    "        fig.set_constrained_layout_pads(w_pad=0.05, h_pad=0.05, wspace=0.05, hspace=0.05)\n",
    "        color = METRIC_COLORS.get(metric, SERIES_COLOR)\n",
    "        plt.errorbar(x_pos, y, yerr=e, marker=\"o\", linestyle=\"-\",\n",
    "                     color=color, capsize=4)\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        ax  = plt.gca()\n",
    "\n",
    "        # axis labels & title (your thesis fonts)\n",
    "        title = f\"Trajectory of {metric}\"\n",
    "        ylabel = metric.replace(\"_\", \" \")\n",
    "        if metric == \"Broadband_dB_Raw\":\n",
    "            title = \"PSD Broadband Power (1-200Hz)\"\n",
    "            ylabel = \"Broadband (dB µV²/Hz)\"\n",
    "        elif metric == \"ThetaDelta_dB_Raw\":\n",
    "            title = \"PSD Theta-Delta Ratio (1-200Hz)\"\n",
    "            ylabel = \"Theta-Delta Ratio (Unitless)\"\n",
    "        elif metric == \"Aperiodic_Exponent\":\n",
    "            title = \"PSD Aperiodic Exponent (2-40Hz)\"\n",
    "            ylabel = \"Aperiodic Exponent (Unitless)\"\n",
    "        \n",
    "        xl = ax.set_xlabel(\"Time-point\", fontproperties=termes_font_bold)\n",
    "        yl = ax.set_ylabel(ylabel, fontproperties=termes_font_bold)\n",
    "        t = ax.set_title(f\"{title}\", fontproperties=termes_font_bold)\n",
    "        xl.set_fontsize(20)\n",
    "        yl.set_fontsize(20)\n",
    "        t.set_fontsize(26)\n",
    "\n",
    "        # Hard-coded fix to label actualy shown on x-axis, donn't want to disturb df access\n",
    "        x_map = {\"D0:0\": \"Control Case\", \"D0:1\": \"DFP Inj.\", \"D0:2\": \"MDZ Interv.\"}\n",
    "        x_labels_shown = [x_map.get(lbl, lbl) for lbl in x_labels]\n",
    "        \n",
    "        # evenly spaced categorical ticks\n",
    "        ax.set_xticks(x_pos, x_labels_shown)\n",
    "\n",
    "        # tick label fonts\n",
    "        for lab in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "            lab.set_fontproperties(termes_font)\n",
    "            lab.set_fontsize(16)\n",
    "\n",
    "        ax.grid(True, alpha=.3)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # write stats table\n",
    "        if save and stats_rows:\n",
    "            import csv\n",
    "            with stats_path.open(\"w\", newline=\"\") as f:\n",
    "                writer = csv.DictWriter(f,\n",
    "                                        fieldnames=[\"Metric\", \"TimeKey\", \"Mean\", \"SEM\", \"N\"],\n",
    "                                        delimiter=\"\\t\")\n",
    "                writer.writeheader()\n",
    "                writer.writerows(stats_rows)\n",
    "\n",
    "        if save:\n",
    "            png_name = session_folder / f\"{Path(json_path).stem}_{metric}.png\"\n",
    "            pdf_name = session_folder / f\"{Path(json_path).stem}_{metric}.pdf\"\n",
    "            plt.savefig(png_name, dpi=600, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "            plt.savefig(pdf_name, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc80939a-8510-46d5-acdc-ccedca89b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAC Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4d35d12-4413-4246-84ce-d2afdabe52d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_pac_summary_json(json_path):\n",
    "    \"\"\"\n",
    "    Load PAC summary data from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        metadata (dict): Dictionary containing session-level metadata.\n",
    "        pac_values (list of dict): List of entries with rat, day, channel, probe, and average_pac.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        metadata = data.get(\"Metadata\", {})\n",
    "        pac_values = data.get(\"Data\", {})\n",
    "        return metadata, pac_values\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in load_pac_summary_json: {e}\", exc_info=True)\n",
    "        return {}, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcf197dd-179b-4fd2-9069-549ff7b75ebe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_JSON_window(JSON_class):\n",
    "    \"\"\"\n",
    "    Create an analysis window that displays JSON file data and plots average PAC scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dpg.add_button(label=\"Import JSON File\",\n",
    "                       parent=\"JSON_buttons_group\",\n",
    "                       callback=import_JSON_file_callback,\n",
    "                       user_data=(JSON_class, \"JSON_meta_window\", \"JSON_data_window\", \"JSON_plot_window\"))\n",
    "\n",
    "        with dpg.group(tag=\"JSON_window_group\",\n",
    "                           parent=\"JSON_child_window\",\n",
    "                           horizontal=True):\n",
    "\n",
    "            with dpg.child_window(tag=\"JSON_meta_window\", \n",
    "                                  border=True, \n",
    "                                  width=300, \n",
    "                                  autosize_y=True, \n",
    "                                  horizontal_scrollbar=True, \n",
    "                                  no_scrollbar=False, \n",
    "                                  menubar=True, \n",
    "                                  parent=\"JSON_window_group\"):\n",
    "                with dpg.menu_bar():\n",
    "                        dpg.add_menu(label=\"JSON File Metadata\")\n",
    "\n",
    "            with dpg.child_window(tag=\"JSON_data_window\", \n",
    "                                  border=True, \n",
    "                                  width=300, \n",
    "                                  autosize_y=True, \n",
    "                                  horizontal_scrollbar=True, \n",
    "                                  no_scrollbar=False, \n",
    "                                  menubar=True, \n",
    "                                  parent=\"JSON_window_group\"):\n",
    "                with dpg.menu_bar():\n",
    "                        dpg.add_menu(label=\"JSON Data\")\n",
    "\n",
    "            with dpg.child_window(tag=\"JSON_plot_window\", \n",
    "                                  border=True, \n",
    "                                  autosize_x=True,\n",
    "                                  autosize_y=True, \n",
    "                                  parent=\"JSON_window_group\"):\n",
    "                with dpg.plot(label=\"Plot\", tag=\"JSON_plot\", width=-1, height=-1,no_title=True):        \n",
    "                    with dpg.plot_axis(dpg.mvXAxis, tag=\"JSON_x_axis\"):\n",
    "                        pass \n",
    "                    with dpg.plot_axis(dpg.mvYAxis, tag=\"JSON_y_axis\"):\n",
    "                        pass\n",
    "                        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in create_JSON_window: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19480132-b6a9-4747-be8b-808b70d99645",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PAC Sanity Check Functions #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be82c95b-0135-4839-84df-c9bff8e667c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _bandpass_mne(x, fs, l_freq, h_freq):\n",
    "    # Zero-phase FIR like your pipeline; same defaults each time\n",
    "    return mne.filter.filter_data(\n",
    "        x, sfreq=fs, l_freq=l_freq, h_freq=h_freq,\n",
    "        method='fir', phase='zero', fir_window='hamming', verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3d48b2c-57c9-479d-9529-3fbd550b1adc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def inject_spike_into_raw_data(\n",
    "    x, fs,\n",
    "    phase_band=(6, 10),          # driver (e.g., theta)\n",
    "    amp_band=(40, 80),           # modulated gamma\n",
    "    depth=0.3,                   # modulation depth in [0, 1)\n",
    "    target_hf_snr_db=None,       # added HF power (in amp_band) relative to *existing* HF power\n",
    "    hf_scale=None,               # alternative: scale of injected HF vs sd(x) if SNR not given\n",
    "    burst_duty=1.0,              # fraction of time with PAC on (0..1]\n",
    "    burst_len_s=2.0,             # length of each burst if duty<1\n",
    "    burst_random=True,           # randomize burst placement\n",
    "    fade_s=0.05,                 # taper edges of bursts\n",
    "    random_state=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Injects PAC into real data:\n",
    "      - Envelope e(t) = 1 + depth*cos(phi(t)), where phi(t) is low-freq phase from `phase_band`\n",
    "      - Carrier: band-passed noise in `amp_band`\n",
    "      - Optional bursts via `burst_duty`, `burst_len_s`\n",
    "      - Optional target SNR in `amp_band`\n",
    "\n",
    "    Returns y (x with PAC added) and a dict with details + the injected component y_add.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    n = x.size\n",
    "\n",
    "    # 1) Low-frequency phase from the *real* signal\n",
    "    lf = _bandpass_mne(x, fs, phase_band[0], phase_band[1])\n",
    "    phi = np.angle(hilbert(lf))  # [-pi, pi]\n",
    "\n",
    "    # 2) High-frequency carrier (band-limited noise)\n",
    "    carrier = rng.normal(size=n)\n",
    "    carrier = _bandpass_mne(carrier, fs, amp_band[0], amp_band[1])\n",
    "    # normalize carrier variance\n",
    "    carrier /= np.std(carrier) + 1e-12\n",
    "\n",
    "    # 3) Phase-locked envelope (strictly positive)\n",
    "    depth = float(np.clip(depth, 0.0, 0.99))\n",
    "    env = 1.0 + depth * np.cos(phi)\n",
    "\n",
    "    # 4) Optional bursts mask\n",
    "    mask = np.ones(n, dtype=float)\n",
    "    if burst_duty < 1.0:\n",
    "        burst_len = max(1, int(round(burst_len_s * fs)))\n",
    "        total_on = int(round(burst_duty * n))\n",
    "        n_bursts = max(1, total_on // burst_len)\n",
    "        mask[:] = 0.0\n",
    "        indices = []\n",
    "        if burst_random:\n",
    "            # choose non-overlapping starts as best as possible\n",
    "            max_start = max(1, n - burst_len - 1)\n",
    "            starts = rng.choice(max_start, size=n_bursts, replace=False)\n",
    "            starts.sort()\n",
    "            indices = [slice(int(s), int(s) + burst_len) for s in starts]\n",
    "        else:\n",
    "            # evenly spaced\n",
    "            step = max(1, (n - burst_len) // n_bursts)\n",
    "            starts = np.arange(0, n_bursts * step, step)\n",
    "            indices = [slice(int(s), int(s) + burst_len) for s in starts]\n",
    "\n",
    "        for sl in indices:\n",
    "            mask[sl] = 1.0\n",
    "\n",
    "        # fade edges for continuity\n",
    "        fade = int(max(1, round(fade_s * fs)))\n",
    "        if fade > 1:\n",
    "            win = get_window((\"tukey\", 1.0), 2 * fade)\n",
    "            up, down = win[:fade], win[fade:]\n",
    "            for sl in indices:\n",
    "                a, b = sl.start, min(sl.stop, n)\n",
    "                # fade in\n",
    "                a2 = a\n",
    "                b2 = min(a + fade, b)\n",
    "                mask[a2:b2] *= up[: b2 - a2]\n",
    "                # fade out\n",
    "                a3 = max(a, b - fade)\n",
    "                b3 = b\n",
    "                mask[a3:b3] *= down[fade - (b3 - a3):]\n",
    "\n",
    "    # 5) Form the injected HF component\n",
    "    y_add = env * carrier * mask\n",
    "\n",
    "    # 6) Scale to target SNR in amp_band (preferred), else hf_scale\n",
    "    def _band_power(sig):\n",
    "        sig_hf = _bandpass_mne(sig, fs, amp_band[0], amp_band[1])\n",
    "        _, pxx = _safe_welch(sig_hf, fs)\n",
    "        return float(np.trapz(pxx))  # rough but consistent for matching\n",
    "\n",
    "    if target_hf_snr_db is not None:\n",
    "        Px = _band_power(x)\n",
    "        Py = _band_power(y_add)\n",
    "        if Py < 1e-18:\n",
    "            scale = 0.0\n",
    "        else:\n",
    "            target_ratio = 10 ** (target_hf_snr_db / 10.0)\n",
    "            scale = np.sqrt((target_ratio * Px) / (Py + 1e-18))\n",
    "        y_add *= scale\n",
    "        scale_info = {\"mode\": \"snr_db\", \"target_hf_snr_db\": float(target_hf_snr_db), \"scale\": float(scale)}\n",
    "    elif hf_scale is not None:\n",
    "        y_add *= float(hf_scale) * (np.std(x) + 1e-12)\n",
    "        scale_info = {\"mode\": \"hf_scale\", \"hf_scale\": float(hf_scale)}\n",
    "    else:\n",
    "        # default: roughly match HF band power to 25% of existing\n",
    "        Px = _band_power(x)\n",
    "        Py = _band_power(y_add)\n",
    "        scale = 0.5 if Py < 1e-18 else np.sqrt((0.25 * Px) / (Py + 1e-18))\n",
    "        y_add *= scale\n",
    "        scale_info = {\"mode\": \"auto_0.25_bandpower\", \"scale\": float(scale)}\n",
    "\n",
    "    y = x + y_add\n",
    "\n",
    "    info = {\n",
    "        \"phase_band\": tuple(phase_band),\n",
    "        \"amp_band\": tuple(amp_band),\n",
    "        \"depth\": float(depth),\n",
    "        \"burst_duty\": float(burst_duty),\n",
    "        \"burst_len_s\": float(burst_len_s),\n",
    "        \"fade_s\": float(fade_s),\n",
    "        \"random_state\": int(random_state) if random_state is not None else None,\n",
    "        **scale_info,\n",
    "    }\n",
    "    return y, info, y_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "093a8f7d-5344-4721-b2dd-9c73d44ab55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions run during PAC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3913af98-3ab9-4a09-a346-039ece7f410e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _safe_welch(x, fs, nperseg=None, noverlap=None):\n",
    "    try:\n",
    "        n = len(x)\n",
    "        if nperseg is None:\n",
    "            # ~4–8 seconds is a decent default; clamp by length\n",
    "            nperseg = int(min(n, max(fs * 4, 256)))\n",
    "        if noverlap is None:\n",
    "            noverlap = int(0.5 * nperseg)\n",
    "        f, pxx = welch(x, fs=fs, nperseg=nperseg, noverlap=noverlap, detrend='constant')\n",
    "        return f, pxx\n",
    "    except Exception as e:\n",
    "        import logging\n",
    "        logging.error(f\"Error in _safe_welch: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a4ec329-389a-45b4-a9d9-31af8f1d9eaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def phase_band_snr(\n",
    "    x, fs, band=(4, 10), neighbor_bw=2.0, exclude_bw=0.5,\n",
    "    remove_aperiodic=True, fit_range=(2, 200)\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate SNR (dB) of the dominant peak inside `band` vs adjacent flanks.\n",
    "    If remove_aperiodic=True, fit 1/f on log-log outside the band and subtract.\n",
    "    Returns (snr_db, info_dict).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        f, pxx = _safe_welch(x, fs)\n",
    "    \n",
    "        # Optional 1/f correction\n",
    "        pxx_work = pxx.copy()\n",
    "        if remove_aperiodic:\n",
    "            vr = (f >= fit_range[0]) & (f <= fit_range[1]) & ~((f >= band[0]) & (f <= band[1]))\n",
    "            vf = f[vr]\n",
    "            yp = np.log10(pxx[vr] + np.finfo(float).eps)\n",
    "            xp = np.log10(vf + np.finfo(float).eps)\n",
    "            # robust-ish linear fit\n",
    "            A = np.vstack([np.ones_like(xp), xp]).T\n",
    "            coeff, *_ = np.linalg.lstsq(A, yp, rcond=None)  # y = a + b*x\n",
    "            a, b = coeff\n",
    "            pxx_fit = 10 ** (a + b * np.log10(f + 1e-12))\n",
    "            pxx_work = pxx / (pxx_fit + 1e-18)\n",
    "    \n",
    "        # Peak in band\n",
    "        in_band = (f >= band[0]) & (f <= band[1])\n",
    "        if not np.any(in_band):\n",
    "            return np.nan, {\"reason\": \"band outside PSD range\"}\n",
    "    \n",
    "        idx_peak = np.argmax(pxx_work[in_band])\n",
    "        f_band = f[in_band]\n",
    "        p_band = pxx_work[in_band]\n",
    "        f0 = f_band[idx_peak]\n",
    "    \n",
    "        # Define small windows around peak and its neighbors\n",
    "        pk_win = (f >= max(f0 - exclude_bw, f[1])) & (f <= f0 + exclude_bw)\n",
    "        lo_win = (f >= max(f0 - neighbor_bw - exclude_bw, f[1])) & (f < f0 - exclude_bw)\n",
    "        hi_win = (f > f0 + exclude_bw) & (f <= f0 + neighbor_bw)\n",
    "    \n",
    "        if not (np.any(lo_win) and np.any(hi_win) and np.any(pk_win)):\n",
    "            return np.nan, {\"reason\": \"insufficient bins around peak\", \"f0\": float(f0)}\n",
    "    \n",
    "        p_peak = np.mean(pxx_work[pk_win])\n",
    "        p_flank = np.mean(np.concatenate([pxx_work[lo_win], pxx_work[hi_win]]))\n",
    "    \n",
    "        snr_db = 10.0 * np.log10((p_peak + 1e-18) / (p_flank + 1e-18))\n",
    "        info = {\n",
    "            \"SNR dB\" : snr_db,\n",
    "            \"f0\": float(f0),\n",
    "            \"p_peak\": float(p_peak),\n",
    "            \"p_flank\": float(p_flank),\n",
    "            \"aperiodic_removed\": bool(remove_aperiodic),\n",
    "            \"band\": tuple(band),\n",
    "            \"neighbor_bw\": float(neighbor_bw),\n",
    "            \"exclude_bw\": float(exclude_bw),\n",
    "        }\n",
    "        return snr_db, info\n",
    "    except Exception as e:\n",
    "        import logging\n",
    "        logging.error(f\"Error in phase_band_snr: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfbae4c5-6633-41cc-95c5-e5a1db54a579",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def hf_band_power_metrics(\n",
    "    x, fs,\n",
    "    hf_band=(50, 120),          # your amplitude band of interest\n",
    "    ref_band=(20, 150),         # broader HF reference for context\n",
    "    remove_aperiodic=True,\n",
    "    fit_range=(2, 200),         # range to fit 1/f outside bands\n",
    "    _welch_fn=_safe_welch       # reuse the one we already wrote\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a dict with integrated HF power (aperiodic-corrected),\n",
    "    its ratio to a broader HF reference band, and a percentile score\n",
    "    telling you how 'elevated' the HF band is within the reference band.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = np.asarray(x, dtype=float)\n",
    "    \n",
    "        f, pxx = _welch_fn(x, fs)\n",
    "        pxx_work = pxx.copy()\n",
    "    \n",
    "        if remove_aperiodic:\n",
    "            # Fit log10(pxx) = a + b*log10(f) outside both bands\n",
    "            mask_fit = (f >= fit_range[0]) & (f <= fit_range[1])\n",
    "            # Exclude hf_band from the fit (optional: also exclude ref_band edges)\n",
    "            ex = (f >= hf_band[0]) & (f <= hf_band[1])\n",
    "            vr = mask_fit & (~ex)\n",
    "            vf = f[vr]\n",
    "            if np.count_nonzero(vf) >= 10:\n",
    "                xp = np.log10(vf + 1e-12)\n",
    "                yp = np.log10(pxx[vr] + 1e-18)\n",
    "                A = np.vstack([np.ones_like(xp), xp]).T\n",
    "                a, b = np.linalg.lstsq(A, yp, rcond=None)[0]\n",
    "                pxx_fit = 10 ** (a + b * np.log10(f + 1e-12))\n",
    "                pxx_work = pxx / (pxx_fit + 1e-18)\n",
    "    \n",
    "        # Slices\n",
    "        hf_sel  = (f >= hf_band[0])  & (f <= hf_band[1])\n",
    "        ref_sel = (f >= ref_band[0]) & (f <= ref_band[1])\n",
    "    \n",
    "        if not np.any(hf_sel) or not np.any(ref_sel):\n",
    "            return {\n",
    "                \"reason\": \"no bins in band(s)\",\n",
    "                \"hf_band\": tuple(hf_band), \"ref_band\": tuple(ref_band),\n",
    "                \"aperiodic_removed\": bool(remove_aperiodic)\n",
    "            }\n",
    "    \n",
    "        # Integrated 'periodic' power (area under PSD)\n",
    "        hf_power  = float(np.trapz(pxx_work[hf_sel], f[hf_sel]))\n",
    "        ref_power = float(np.trapz(pxx_work[ref_sel], f[ref_sel]))\n",
    "        rel_power = hf_power / (ref_power + 1e-18)\n",
    "    \n",
    "        # Percentile of HF mean within the reference-bin distribution\n",
    "        hf_mean = float(np.mean(pxx_work[hf_sel]))\n",
    "        ref_vals = pxx_work[ref_sel]\n",
    "        hf_percentile = float(100.0 * np.mean(ref_vals <= hf_mean))\n",
    "    \n",
    "        # Optional: dB of integrated power (on corrected spectrum)\n",
    "        hf_power_db = float(10.0 * np.log10(hf_power + 1e-18))\n",
    "    \n",
    "        return {\n",
    "            \"hf_band\": tuple(hf_band),\n",
    "            \"ref_band\": tuple(ref_band),\n",
    "            \"aperiodic_removed\": bool(remove_aperiodic),\n",
    "            \"hf_power\": hf_power,\n",
    "            \"hf_power_db\": hf_power_db,\n",
    "            \"ref_power\": ref_power,\n",
    "            \"hf_rel_power\": rel_power,             # fraction of ref power\n",
    "            \"hf_mean_bin_power\": hf_mean,          # mean per-bin power in HF\n",
    "            \"hf_percentile_in_ref\": hf_percentile  # 0–100\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import logging\n",
    "        logging.error(f\"Error in hf_band_power_metrics: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f3b8477-7884-4f54-9e17-79f6723099b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_freq_at(freq_vector, i, mode=\"center\"):\n",
    "    \"\"\"\n",
    "    freq_vector: p.f_pha or p.f_amp. Elements may be scalars or [low, high].\n",
    "    i: index along that axis\n",
    "    mode: \"center\" | \"band\" | \"low\" | \"high\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Robust indexing (handles lists, np arrays, object arrays)\n",
    "        f_i = np.asarray(freq_vector, dtype=object)[i]\n",
    "    \n",
    "        f_i = np.asarray(f_i)\n",
    "        if f_i.ndim == 0:\n",
    "            lo = hi = float(f_i)                     # scalar -> same low/high\n",
    "        elif f_i.size == 2:\n",
    "            lo, hi = float(f_i[0]), float(f_i[1])    # band [low, high]\n",
    "        else:\n",
    "            # Very rare: if something odd comes back, be defensive\n",
    "            lo, hi = float(np.min(f_i)), float(np.max(f_i))\n",
    "    \n",
    "        center = (lo + hi) / 2.0\n",
    "    \n",
    "        if mode == \"center\":\n",
    "            value = center\n",
    "        elif mode == \"low\":\n",
    "            value = lo\n",
    "        elif mode == \"high\":\n",
    "            value = hi\n",
    "        elif mode == \"band\":\n",
    "            value = (lo, hi)\n",
    "        else:\n",
    "            value = center\n",
    "    \n",
    "        return {\"low\": lo, \"high\": hi, \"center\": center, \"value\": value}\n",
    "    except Exception as e:\n",
    "        import logging\n",
    "        logging.error(f\"Error in parse_freq_at: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "844bafe5-f1c3-4902-a311-b3eaaf72e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtions run during Crunch PAC JSON Data - Coupling AVG Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05238277-dda8-4eca-a2cd-1ddffc763a90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _apply_thesis_fonts(ax, title, xlabel, ylabel, termes_font=None, termes_font_bold=None):\n",
    "    if termes_font_bold:\n",
    "        title = ax.set_title(title, fontproperties=termes_font_bold)\n",
    "        xl = ax.set_xlabel(xlabel, fontproperties=termes_font_bold)\n",
    "        yl = ax.set_ylabel(ylabel, fontproperties=termes_font_bold)\n",
    "    else:\n",
    "        title = ax.set_title(title)\n",
    "        xl = ax.set_xlabel(xlabel)\n",
    "        yl = ax.set_ylabel(ylabel)\n",
    "    if termes_font:\n",
    "        for lab in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "            lab.set_fontproperties(termes_font)\n",
    "\n",
    "    xl.set_fontsize(20)\n",
    "    yl.set_fontsize(20)\n",
    "    title.set_fontsize(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2c1c5b3-78f6-40f6-8fcd-c2273779fb50",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _find_session_summary(path_like: str | os.PathLike) -> Path:\n",
    "    \"\"\"Accept a folder OR a specific json. Return the Session Summary path.\"\"\"\n",
    "    p = Path(path_like)\n",
    "    if p.is_file():\n",
    "        return p\n",
    "    # Look for a likely “Session Summary” JSON in the folder tree\n",
    "    candidates = list(p.glob(\"**/Session Summary.json\")) + list(p.glob(\"**/*Session Summary*.json\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"No 'Session Summary' JSON found under folder.\")\n",
    "    # Prefer the most-recent\n",
    "    candidates.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    return candidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c702a91-1808-4680-952e-ed7728632a1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_pac_json_to_long_df(json_or_folder: str | os.PathLike,\n",
    "                             metrics=None,\n",
    "                             probes: list[str] | None = None,\n",
    "                             rats: list[int] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the Session Summary JSON and return a long DataFrame with columns:\n",
    "      ['Rat','DayNum','Session','Channel','Probe','TimeKey',\n",
    "       'primary_summary_name','Metric','Value']\n",
    "    \"\"\"\n",
    "    path = _find_session_summary(json_or_folder)\n",
    "    with open(path, \"r\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    # Check if JSON file contains PAC analysis\n",
    "    skip = bool(root.get(\"Metadata\", {}).get(\"skip_PAC\", False))\n",
    "    if skip:\n",
    "        return None\n",
    "\n",
    "    data = root.get(\"Data\", {})\n",
    "    if metrics is None:\n",
    "        # Default set you asked for\n",
    "        metrics = [\n",
    "            \"primary_summary_value\",\n",
    "            \"peak_value\",\n",
    "            \"peak_phase_hz\",\n",
    "            \"peak_amplitude_hz\",\n",
    "            \"z_abs_mean\",\n",
    "            \"z_topk_mean\",\n",
    "            \"frac_sig_ge_1.96\",\n",
    "        ]\n",
    "\n",
    "    rows = []\n",
    "    # Expect structure: Data[rat][day][session][channel] -> dict\n",
    "    for rat_str, days_dict in data.items():\n",
    "        try:\n",
    "            rat = int(rat_str)\n",
    "        except Exception:\n",
    "            rat = rat_str  # fallback if non-numeric\n",
    "        for day_str, sessions_dict in days_dict.items():\n",
    "            try:\n",
    "                day = int(day_str)\n",
    "            except Exception:\n",
    "                day = day_str\n",
    "            for sess_code, chans_dict in sessions_dict.items():\n",
    "                for ch_name, ch_info in chans_dict.items():\n",
    "                    if not isinstance(ch_info, dict):\n",
    "                        continue\n",
    "                    probe = ch_info.get(\"Probe Type\") or ch_info.get(\"Probe\")\n",
    "                    # Optional filtering\n",
    "                    if probes is not None and probe not in probes:\n",
    "                        continue\n",
    "                    if rats is not None and (isinstance(rat, int) and rat not in rats):\n",
    "                        continue\n",
    "\n",
    "                    # Collect chosen metrics present for this channel\n",
    "                    present_vals = {m: ch_info[m] for m in metrics if m in ch_info}\n",
    "                    if not present_vals:\n",
    "                        continue\n",
    "\n",
    "                    primary_name = ch_info.get(\"primary_summary_name\")\n",
    "                    \n",
    "                    # Build time key: D0:x for Day 0 sessions, else \"Day N\"\n",
    "                    if day == 0:\n",
    "                        # 0000 -> D0:0, 0001 -> D0:1, 0002 -> D0:2, etc.\n",
    "                        try:\n",
    "                            suffix = str(sess_code)[-1]\n",
    "                        except Exception:\n",
    "                            suffix = sess_code\n",
    "\n",
    "                        #time_key = f\"D0:{suffix}\"\n",
    "                        if suffix == \"0\":\n",
    "                            time_key = \"CRTL\"\n",
    "                        elif suffix == \"1\":\n",
    "                            time_key = \"DFP\"\n",
    "                        elif suffix == \"2\":\n",
    "                            time_key = \"MDZ\"\n",
    "                        \n",
    "                    else:\n",
    "                        time_key = f\"Day {day}\"\n",
    "\n",
    "                    base = {\n",
    "                        \"Rat\": rat,\n",
    "                        \"DayNum\": day,\n",
    "                        \"Session\": str(sess_code),\n",
    "                        \"Channel\": str(ch_name),\n",
    "                        \"Probe\": probe,\n",
    "                        \"TimeKey\": time_key,\n",
    "                        \"primary_summary_name\": primary_name,\n",
    "                    }\n",
    "                    # wide → rows (we’ll melt to long below; this is still fine)\n",
    "                    base.update(present_vals)\n",
    "                    rows.append(base)\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"No rows parsed; check filters, metrics, or JSON structure.\")\n",
    "\n",
    "    df_wide = pd.DataFrame(rows)\n",
    "\n",
    "    # Melt to long\n",
    "    value_vars = [m for m in metrics if m in df_wide.columns]\n",
    "    df_long = df_wide.melt(\n",
    "        id_vars=[\"Rat\", \"DayNum\", \"Session\", \"Channel\", \"Probe\", \"TimeKey\", \"primary_summary_name\"],\n",
    "        value_vars=value_vars,\n",
    "        var_name=\"Metric\",\n",
    "        value_name=\"Value\",\n",
    "    )\n",
    "\n",
    "    # Enforce consistent categorical order only when you plot (case-specific)\n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1404f75-276f-4eca-9119-0993401938fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_mean_sem(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate mean ± SEM by TimeKey, Metric.\"\"\"\n",
    "    g = (\n",
    "        df_long.groupby([\"TimeKey\", \"Metric\"])[\"Value\"]\n",
    "        .agg(mean=\"mean\", std=\"std\", n=\"count\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    g[\"sem\"] = g[\"std\"] / np.sqrt(g[\"n\"])\n",
    "    g[\"sem\"] = g[\"sem\"].fillna(0.0)  # n==1 -> std NaN -> sem 0\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d551e236-fcb6-4e21-b1e5-b5eaa7a2d28d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _desired_order_for_case(df_long: pd.DataFrame, case: str) -> list[str]:\n",
    "    uniq = df_long[\"TimeKey\"].unique().tolist()\n",
    "    if case == \"D0_sessions\":\n",
    "        order = [\"D0:0\", \"D0:1\", \"D0:2\", \"D0:3\"]\n",
    "    elif case == \"D0_1_to_14\":\n",
    "        order = [\"D0:1\", \"Day 1\", \"Day 3\", \"Day 7\", \"Day 14\"]\n",
    "    elif case == \"Full_House\":\n",
    "        order = [\"CRTL\", \"DFP\", \"MDZ\", \"Day 1\", \"Day 3\", \"Day 7\", \"Day 14\"]\n",
    "    else:\n",
    "        # Fallback: natural-ish order: D0:* first, then Day N ascending\n",
    "        d0 = sorted([x for x in uniq if str(x).startswith(\"D0:\")])\n",
    "        later = sorted(\n",
    "            [x for x in uniq if str(x).startswith(\"Day \")],\n",
    "            key=lambda s: int(str(s).split()[1])\n",
    "        )\n",
    "        order = d0 + later\n",
    "    return [x for x in order if x in uniq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d65d9f1-2a83-47ab-a360-4b0d95d9b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pac_trajectories(df_long: pd.DataFrame,\n",
    "                          metrics: list[str] | None = None,\n",
    "                          case: str = \"D0_sessions\",\n",
    "                          save: bool = False,\n",
    "                          output_dir: str | os.PathLike | None = None,\n",
    "                          palette: dict[str, str] | None = None,\n",
    "                          termes_font=None,\n",
    "                          termes_font_bold=None,\n",
    "                          clamp_y0: bool = False):\n",
    "    \"\"\"\n",
    "    Plot mean ± SEM trajectories for selected metrics.\n",
    "      case = \"D0_sessions\" or \"D0_1_to_14\" (your two study setups)\n",
    "    \"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = sorted(df_long[\"Metric\"].unique())\n",
    "\n",
    "    order = _desired_order_for_case(df_long, case)\n",
    "    if not order:\n",
    "        raise ValueError(\"No timepoints found for the requested case.\")\n",
    "\n",
    "    # Filter to the timepoints of interest and lock the plotting order\n",
    "    df_plot = df_long[df_long[\"TimeKey\"].isin(order)].copy()\n",
    "    df_plot[\"TimeKey\"] = pd.Categorical(df_plot[\"TimeKey\"], categories=order, ordered=True)\n",
    "\n",
    "    agg = aggregate_mean_sem(df_plot)\n",
    "\n",
    "    # Prepare optional save folder and stats file\n",
    "    if save and output_dir:\n",
    "        out = Path(output_dir)\n",
    "        ts = datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "        session_folder = out / f\"Z-score Trajectories\"\n",
    "        session_folder.mkdir(parents=True, exist_ok=True)\n",
    "        stats_path = session_folder / \"trajectory_stats.tsv\"\n",
    "        # Write one combined stats table for everything we plot\n",
    "        agg_sorted = agg.sort_values([\"Metric\", \"TimeKey\"])\n",
    "        agg_sorted.to_csv(stats_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    # Helper to title/labels by metric\n",
    "    def _metric_labels(metric: str) -> tuple[str, str]:\n",
    "        # Y-axis label and title\n",
    "        if metric == \"primary_summary_value\":\n",
    "            # Try to surface the primary_summary_name for clarity\n",
    "            names = df_plot.loc[df_plot[\"Metric\"] == metric, \"primary_summary_name\"].dropna().unique()\n",
    "            detail = f\" ({names[0]})\" if len(names) else \"\"\n",
    "            return f\"Primary{detail}\", f\"Primary Summary Value{detail}\"\n",
    "        elif metric == \"z_abs_mean\":\n",
    "            return \"Z-score (|Z|)\", \"Mean Comodulogram Z-score\"\n",
    "        elif metric == \"z_topk_mean\":\n",
    "            return \"Z-score (top-k mean)\", \"Top-k Mean Z\"\n",
    "        elif metric == \"peak_value\":\n",
    "            return \"Z-score\", \"Peak Z-score\"\n",
    "        elif metric == \"peak_phase_hz\":\n",
    "            return \"Frequency (Hz)\", \"Average PSD Theta Frequency Peak (4-12 Hz)\"\n",
    "        elif metric == \"peak_amplitude_hz\":\n",
    "            return \"Frequency (Hz)\", \"Peak Amplitude Frequency\"\n",
    "        elif metric == \"frac_sig_ge_1.96\":\n",
    "            return \"Fraction of significant bins (z≥1.96)\", \"Number of Bins (z≥1.96) Over Total Bins\"\n",
    "        else:\n",
    "            return \"Value\", metric\n",
    "\n",
    "    BRIGHT_COLORS = [\n",
    "        '#d9d904',  # Yellow\n",
    "        '#ff7f00',  # Orange\n",
    "        '#e41a1c',  # Red\n",
    "        '#a65628',  # Brown\n",
    "        '#377eb8',  # Blue\n",
    "        '#984ea3',  # Purple\n",
    "        '#4daf4a',  # Green\n",
    "        '#f781bf',  # Pink\n",
    "    ]\n",
    "\n",
    "    # Colors\n",
    "    SERIES_COLOR = \"#111111\"\n",
    "    default_palette = {m: BRIGHT_COLORS[i] for i, m in enumerate(metrics)}\n",
    "    if palette:\n",
    "        default_palette.update(palette)\n",
    "\n",
    "    # Plot one figure per metric\n",
    "    for metric in metrics:\n",
    "        sub = agg[agg[\"Metric\"] == metric].sort_values(\"TimeKey\")\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        x_labels = sub[\"TimeKey\"].astype(str).tolist()\n",
    "        x = np.arange(len(x_labels))\n",
    "        y = sub[\"mean\"].to_numpy(dtype=float)\n",
    "        e = sub[\"sem\"].to_numpy(dtype=float)\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 5), constrained_layout=True)\n",
    "        fig.set_constrained_layout_pads(w_pad=0.05, h_pad=0.05, wspace=0.05, hspace=0.05)\n",
    "        ax = plt.gca()\n",
    "        ax.errorbar(x, y, yerr=e, marker=\"o\", linestyle=\"-\",\n",
    "                    color=default_palette.get(metric, SERIES_COLOR), capsize=4)\n",
    "\n",
    "        ylabel, title = _metric_labels(metric)\n",
    "        _apply_thesis_fonts(ax, title=title, xlabel=\"Time-point\", ylabel=ylabel,\n",
    "                            termes_font=termes_font, termes_font_bold=termes_font_bold)\n",
    "\n",
    "        # Hard-coded fix to label actualy shown on x-axis, donn't want to disturb df access\n",
    "        x_map = {\"CRTL\": \"Control Case\", \"DFP\": \"DFP Inj.\", \"MDZ\": \"MDZ Interv.\"}\n",
    "        x_labels_shown = [x_map.get(lbl, lbl) for lbl in x_labels]\n",
    "\n",
    "        ax.set_xticks(x, x_labels_shown)\n",
    "        for lab in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "            lab.set_fontproperties(termes_font)\n",
    "            lab.set_fontsize(16)\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if clamp_y0:\n",
    "            ax.set_ylim(bottom=0)\n",
    "\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        if save and output_dir:\n",
    "            base = f\"{case}_{metric}\".replace(\" \", \"_\")\n",
    "            png = session_folder / f\"{base}.png\"\n",
    "            pdf = session_folder / f\"{base}.pdf\"\n",
    "            plt.savefig(png, dpi=600, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "            plt.savefig(pdf, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            plt.show()\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f34d7bed-253c-42cc-8a62-b82ddd194612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtions run during Crunch PAC JSON Data - SNR LF Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03973f99-2756-4467-95f8-ce2dbdeeaaeb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_snr_df(json_path, condition_map=None, bands_keep=None):\n",
    "    \"\"\"\n",
    "    # --- 1) Parse SNR from your JSON into a tidy DataFrame ---\n",
    "    Reads your Session Summary.json and returns a DataFrame with columns:\n",
    "      ['rat','day','session','condition','channel','probe','band','snr_db','f0',\n",
    "       'z_abs_mean','z_topk_mean','frac_sig_ge_1.96','run_json_path']\n",
    "    condition_map maps session codes to labels, e.g. {'0000':'Control','0001':'DFP','0002':'MDZ'}.\n",
    "    bands_keep limits to certain band names (as stored in JSON: 'Delta','Theta',...)\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    D = obj.get(\"Data\", {})\n",
    "    rows = []\n",
    "\n",
    "    # If not provided, default to your three conditions; unknown codes pass through\n",
    "    if condition_map is None:\n",
    "        #condition_map = {\"0000\": \"Control\", \"0001\": \"DFP\", \"0002\": \"MDZ\"}\n",
    "        condition_map = {\"0000\": \"CRTL\", \n",
    "                         \"0001\": \"DFP\", \n",
    "                         \"0002\": \"MDZ\",\n",
    "                         \"1\": \"Day 1\",\n",
    "                         \"3\": \"Day 3\",\n",
    "                         \"7\": \"Day 7\",\n",
    "                         \"14\": \"Day 14\"}\n",
    "\n",
    "    # Walk nested dicts: rat -> day -> session -> channel\n",
    "    for rat_key, rat_val in D.items():\n",
    "        for day_key, day_val in rat_val.items():\n",
    "            for session_key, session_val in day_val.items():\n",
    "                if day_key != \"0\":\n",
    "                    condition = condition_map.get(day_key, day_key)\n",
    "                else:\n",
    "                    condition = condition_map.get(session_key, session_key)\n",
    "                if not isinstance(session_val, dict):\n",
    "                    continue\n",
    "\n",
    "                for channel, ch_dict in session_val.items():\n",
    "                    if not isinstance(ch_dict, dict):\n",
    "                        continue\n",
    "\n",
    "                    # Pull optional PAC metrics\n",
    "                    probe   = ch_dict.get(\"Probe Type\", \"\")\n",
    "                    z_abs   = ch_dict.get(\"z_abs_mean\", np.nan)\n",
    "                    z_topk  = ch_dict.get(\"z_topk_mean\", np.nan)\n",
    "                    frac196 = ch_dict.get(\"frac_sig_ge_1.96\", np.nan)\n",
    "                    runjp   = ch_dict.get(\"run_json_path\", \"\")\n",
    "\n",
    "                    # Find SNR band blocks (they’re the dicts with \"SNR dB\")\n",
    "                    for band_name, band_dict in ch_dict.items():\n",
    "                        if bands_keep is not None and band_name not in bands_keep:\n",
    "                            continue\n",
    "                        if isinstance(band_dict, dict) and \"SNR dB\" in band_dict:\n",
    "                            rows.append({\n",
    "                                \"rat\": str(rat_key),\n",
    "                                \"day\": str(day_key),\n",
    "                                \"session\": str(session_key),\n",
    "                                \"condition\": str(condition),\n",
    "                                \"channel\": str(channel),\n",
    "                                \"probe\": str(probe),\n",
    "                                \"band\": str(band_name),\n",
    "                                \"snr_db\": float(band_dict.get(\"SNR dB\", np.nan)),\n",
    "                                \"f0\": float(band_dict.get(\"f0\", np.nan)),\n",
    "                                \"z_abs_mean\": float(z_abs) if z_abs is not None else np.nan,\n",
    "                                \"z_topk_mean\": float(z_topk) if z_topk is not None else np.nan,\n",
    "                                \"frac_sig_ge_1.96\": float(frac196) if frac196 is not None else np.nan,\n",
    "                                \"run_json_path\": str(runjp),\n",
    "                            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Optional: order condition\n",
    "    cond_order = [\"CRTL\", \"DFP\", \"MDZ\", \"Day 1\", \"Day 3\", \"Day 7\", \"Day 14\"]\n",
    "    df[\"condition\"] = pd.Categorical(df[\"condition\"], categories=cond_order, ordered=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d19eef1-a103-4f17-b41c-c422ae6cd8b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def summarize_snr(df):\n",
    "    \"\"\"\n",
    "    # --- 3) Tabular summary (median/IQR) you can paste into the thesis ---\n",
    "    Returns a wide table with metrics (n, median, q1, q3, iqr) by band x condition.\n",
    "    Columns are a MultiIndex: (metric, condition).\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Make sure expected columns exist\n",
    "    needed = {\"band\", \"condition\", \"snr_db\"}\n",
    "    missing = needed - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"summarize_snr: missing columns {missing}\")\n",
    "\n",
    "    #print(\"summarize_snr df cols:\", df.columns.tolist())\n",
    "\n",
    "    # Compute stats cleanly with agg (no weird column levels)\n",
    "    stats = (\n",
    "        df.dropna(subset=[\"snr_db\"])\n",
    "          .groupby([\"band\", \"condition\"])[\"snr_db\"]\n",
    "          .agg(\n",
    "              n=\"count\",\n",
    "              median=\"median\",\n",
    "              q1=lambda s: np.percentile(s, 25),\n",
    "              q3=lambda s: np.percentile(s, 75),\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "    #print(\"stats cols:\", stats.columns.tolist())\n",
    "    stats[\"iqr\"] = stats[\"q3\"] - stats[\"q1\"]\n",
    "\n",
    "    # Wide table (band as rows, metrics x condition as columns)\n",
    "    wide = stats.pivot(index=\"band\", columns=\"condition\", values=[\"n\", \"median\", \"q1\", \"q3\", \"iqr\"])\n",
    "    # Optional: sort column levels consistently\n",
    "    wide = wide.sort_index(axis=1, level=0)\n",
    "\n",
    "    return wide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdec111f-e9e1-4e4b-a0e0-5c1db817d3bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_snr_distributions(df, save_dir=None, kind=\"box\", thresholds=(3.0, 6.0), show_probe_legend=True):\n",
    "    BRIGHT_COLORS = [\n",
    "        '#e41a1c',  # Red\n",
    "        '#ff7f00',  # Orange\n",
    "        '#d9d904',  # Yellow\n",
    "        '#4daf4a',  # Green\n",
    "        '#377eb8',  # Blue\n",
    "        '#984ea3',  # Purple\n",
    "        '#a65628',  # Brown\n",
    "        '#f781bf',  # Pink\n",
    "    ]\n",
    "\n",
    "    band_range_str = {'Delta' : '(1-4 Hz)', 'Theta' : '(6-10 Hz)', 'Wide Theta' : '(4-12 Hz)', 'High Theta': '(10-16 Hz)', 'Beta' : '(13-30 Hz)'}\n",
    "    PROBES = ['RAMG', 'RPFC', 'RVHPC', 'RDHPC', 'LHPCSCREW', 'RHPCSCREW', 'LPFCSCREW', 'RPFCSCREW']\n",
    "    probe_color_map = {probe: BRIGHT_COLORS[i % len(BRIGHT_COLORS)] for i, probe in enumerate(PROBES)}\n",
    "\n",
    "    figs = {}\n",
    "    bands = list(df[\"band\"].dropna().unique())\n",
    "    # If condition is categorical great; if not, just use unique order:\n",
    "    conds = list(df[\"condition\"].cat.categories) if hasattr(df[\"condition\"], \"cat\") else list(df[\"condition\"].unique())\n",
    "    conds = [c for c in conds if c in df[\"condition\"].unique()]\n",
    "\n",
    "    for band in bands:\n",
    "        sub = df[df[\"band\"] == band]\n",
    "        data = [sub[sub[\"condition\"] == c][\"snr_db\"].dropna().values for c in conds]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4), constrained_layout=True)\n",
    "        fig.set_constrained_layout_pads(w_pad=0.05, h_pad=0.05, wspace=0.05, hspace=0.05)\n",
    "        if kind == \"violin\":\n",
    "            ax.violinplot(data, showmeans=True, showextrema=False)\n",
    "            ax.set_xticks(range(1, len(conds) + 1))\n",
    "            ax.set_xticklabels(conds)\n",
    "        else:\n",
    "            ax.boxplot(data, labels=conds, showfliers=False)\n",
    "\n",
    "        # ---- probe-colored jitter points ----\n",
    "        handles_needed = {}\n",
    "        for i, c in enumerate(conds, start=1):\n",
    "            cdf = sub[(sub[\"condition\"] == c) & (~sub[\"snr_db\"].isna())]\n",
    "            if cdf.empty:\n",
    "                continue\n",
    "            # jittered x per-point\n",
    "            xj = np.random.normal(loc=i, scale=0.05, size=len(cdf))\n",
    "            colors = [probe_color_map.get(p, \"#444444\") for p in cdf[\"probe\"]]\n",
    "            ax.scatter(xj, cdf[\"snr_db\"].values, s=12, alpha=0.7, zorder=3, c=colors, edgecolors=\"none\")\n",
    "            # track probes present for legend\n",
    "            for p in cdf[\"probe\"].unique():\n",
    "                if p not in handles_needed:\n",
    "                    handles_needed[p] = Line2D([0], [0], marker='o', linestyle='',\n",
    "                                               markerfacecolor=probe_color_map.get(p, \"#444444\"),\n",
    "                                               markersize=6, label=p)\n",
    "\n",
    "        yl = ax.set_ylabel(\"Phase-band SNR (dB)\")\n",
    "        yl.set_fontproperties(termes_font_bold)\n",
    "        yl.set_fontsize(16)\n",
    "        \n",
    "        title = ax.set_title(f\"SNR: {band} {band_range_str[band]}\")\n",
    "        # your fixed y-lims (adjust if you want negatives visible)\n",
    "        ax.set_ylim(top=8, bottom=-2)\n",
    "\n",
    "        for thr in thresholds:\n",
    "            ax.axhline(thr, linestyle=\"--\", linewidth=1)\n",
    "            #ax.text(0.02, (thr - ax.get_ylim()[0])/(ax.get_ylim()[1]-ax.get_ylim()[0]),\n",
    "            #        f\"{thr:.0f} dB\", transform=ax.transAxes, va=\"bottom\", fontsize=8)\n",
    "\n",
    "        if show_probe_legend and handles_needed:\n",
    "            legend = ax.legend(handles=list(handles_needed.values()), loc='upper right', ncols=2, fontsize=8)\n",
    "\n",
    "        # Adjust font\n",
    "        for text in legend.get_texts():\n",
    "            text.set_fontproperties(termes_font)\n",
    "        for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "            label.set_fontproperties(termes_font_bold)\n",
    "        title.set_fontproperties(termes_font_bold)\n",
    "        title.set_fontsize(20)\n",
    "        \n",
    "        #ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            base = os.path.join(save_dir, f\"SNR_{band.replace(' ','_')}_{kind}\")\n",
    "            fig.savefig(base + \".png\", dpi=600, bbox_inches=\"tight\", pad_inches=0.2)\n",
    "            fig.savefig(base + \".pdf\", bbox_inches=\"tight\", pad_inches=0.2)\n",
    "\n",
    "        figs[band] = fig\n",
    "        plt.close(fig)\n",
    "\n",
    "    return figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb658956-bb78-475a-b95d-21f16647e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtions run during Crunch PAC JSON Data - SNR HF Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbb5930d-b717-44a0-9574-e7f1e8b9dbbc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_hf_df(json_path, condition_map=None):\n",
    "    \"\"\"\n",
    "    Parse 'HF Power' blocks from Session Summary.json into a tidy DataFrame.\n",
    "    Columns:\n",
    "      ['rat','day','session','condition','channel','probe',\n",
    "       'hf_band_low','hf_band_high','ref_band_low','ref_band_high',\n",
    "       'hf_power','hf_power_db','ref_power','hf_rel_power','hf_percentile_in_ref',\n",
    "       'run_json_path']\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    D = obj.get(\"Data\", {})\n",
    "    rows = []\n",
    "\n",
    "    if condition_map is None:\n",
    "        condition_map = {\"0000\": \"CRTL\", \n",
    "                         \"0001\": \"DFP\", \n",
    "                         \"0002\": \"MDZ\",\n",
    "                         \"1\": \"Day 1\",\n",
    "                         \"3\": \"Day 3\",\n",
    "                         \"7\": \"Day 7\",\n",
    "                         \"14\": \"Day 14\"}\n",
    "\n",
    "    for rat_key, rat_val in D.items():\n",
    "        for day_key, day_val in rat_val.items():\n",
    "            for session_key, session_val in day_val.items():\n",
    "                if day_key != \"0\":\n",
    "                    condition = condition_map.get(day_key, day_key)\n",
    "                else:\n",
    "                    condition = condition_map.get(session_key, session_key)\n",
    "                if not isinstance(session_val, dict):\n",
    "                    continue\n",
    "\n",
    "                for channel, ch_dict in session_val.items():\n",
    "                    if not isinstance(ch_dict, dict):\n",
    "                        continue\n",
    "\n",
    "                    hf = ch_dict.get(\"HF Power\")\n",
    "                    if not isinstance(hf, dict):\n",
    "                        continue  # skip channels without HF entry\n",
    "\n",
    "                    probe   = ch_dict.get(\"Probe Type\", \"\")\n",
    "                    runjp   = ch_dict.get(\"run_json_path\", \"\")\n",
    "\n",
    "                    hf_band = hf.get(\"hf_band\", [np.nan, np.nan])\n",
    "                    ref_band = hf.get(\"ref_band\", [np.nan, np.nan])\n",
    "\n",
    "                    rows.append({\n",
    "                        \"rat\": str(rat_key),\n",
    "                        \"day\": str(day_key),\n",
    "                        \"session\": str(session_key),\n",
    "                        \"condition\": str(condition),\n",
    "                        \"channel\": str(channel),\n",
    "                        \"probe\": str(probe),\n",
    "\n",
    "                        \"hf_band_low\":  float(hf_band[0]) if len(hf_band) >= 2 else np.nan,\n",
    "                        \"hf_band_high\": float(hf_band[1]) if len(hf_band) >= 2 else np.nan,\n",
    "                        \"ref_band_low\":  float(ref_band[0]) if len(ref_band) >= 2 else np.nan,\n",
    "                        \"ref_band_high\": float(ref_band[1]) if len(ref_band) >= 2 else np.nan,\n",
    "\n",
    "                        \"hf_power\": float(hf.get(\"hf_power\", np.nan)),\n",
    "                        \"hf_power_db\": float(hf.get(\"hf_power_db\", np.nan)),\n",
    "                        \"ref_power\": float(hf.get(\"ref_power\", np.nan)),\n",
    "                        \"hf_rel_power\": float(hf.get(\"hf_rel_power\", np.nan)),\n",
    "                        \"hf_percentile_in_ref\": float(hf.get(\"hf_percentile_in_ref\", np.nan)),\n",
    "\n",
    "                        \"run_json_path\": str(runjp),\n",
    "                    })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Optional: ordered categories for condition\n",
    "    cond_order = [\"CRTL\", \"DFP\", \"MDZ\", \"Day 1\", \"Day 3\", \"Day 7\", \"Day 14\"]\n",
    "    df[\"condition\"] = pd.Categorical(df[\"condition\"], categories=cond_order, ordered=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "edd7b33d-3ff6-4e2d-ac7c-a51d90387d96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_hf_distributions(df, metric=\"hf_rel_power\", save_dir=None, kind=\"box\",\n",
    "                          thresholds=None, probe_color_map=None, show_probe_legend=True):\n",
    "    \"\"\"\n",
    "    One figure per metric (grouped by condition), dots colored by probe.\n",
    "    thresholds: e.g., for rel_power use (0.1, 0.2), for percentile use (50,), etc.\n",
    "    \"\"\"\n",
    "    BRIGHT_COLORS = [\n",
    "        '#e41a1c',  # Red\n",
    "        '#ff7f00',  # Orange\n",
    "        '#d9d904',  # Yellow\n",
    "        '#4daf4a',  # Green\n",
    "        '#377eb8',  # Blue\n",
    "        '#984ea3',  # Purple\n",
    "        '#a65628',  # Brown\n",
    "        '#f781bf',  # Pink\n",
    "    ]\n",
    "\n",
    "    PROBES = ['RAMG', 'RPFC', 'RVHPC', 'RDHPC', 'LHPCSCREW', 'RHPCSCREW', 'LPFCSCREW', 'RPFCSCREW']\n",
    "    probe_color_map = {probe: BRIGHT_COLORS[i % len(BRIGHT_COLORS)] for i, probe in enumerate(PROBES)}\n",
    "\n",
    "    figs = {}\n",
    "    # Guard\n",
    "    if metric not in df.columns:\n",
    "        raise ValueError(f\"metric '{metric}' not found in DataFrame\")\n",
    "\n",
    "    conds = list(df[\"condition\"].cat.categories) if hasattr(df[\"condition\"], \"cat\") else list(df[\"condition\"].unique())\n",
    "    conds = [c for c in conds if c in df[\"condition\"].unique()]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    data = [df[df[\"condition\"] == c][metric].dropna().values for c in conds]\n",
    "\n",
    "    if kind == \"violin\":\n",
    "        ax.violinplot(data, showmeans=True, showextrema=False)\n",
    "        ax.set_xticks(range(1, len(conds) + 1))\n",
    "        ax.set_xticklabels(conds)\n",
    "    else:\n",
    "        ax.boxplot(data, labels=conds, showfliers=False)\n",
    "\n",
    "    # probe-colored dots\n",
    "    handles_needed = {}\n",
    "    for i, c in enumerate(conds, start=1):\n",
    "        cdf = df[(df[\"condition\"] == c) & (~df[metric].isna())]\n",
    "        if cdf.empty:\n",
    "            continue\n",
    "        xj = np.random.normal(loc=i, scale=0.05, size=len(cdf))\n",
    "        colors = [probe_color_map.get(p, \"#444444\") for p in cdf[\"probe\"]]\n",
    "        ax.scatter(xj, cdf[metric].values, s=12, alpha=0.7, zorder=3, c=colors, edgecolors=\"none\")\n",
    "        for p in cdf[\"probe\"].unique():\n",
    "            if p not in handles_needed:\n",
    "                handles_needed[p] = Line2D([0], [0], marker='o', linestyle='',\n",
    "                                           markerfacecolor=probe_color_map.get(p, \"#444444\"),\n",
    "                                           markersize=6, label=p)\n",
    "\n",
    "    # Labels\n",
    "    ylab = {\n",
    "        \"hf_rel_power\": \"HF Relative Power (HF / Ref)\",\n",
    "        \"hf_percentile_in_ref\": \"HF Mean Percentile in Ref (%)\",\n",
    "        \"hf_power_db\": \"HF Integrated Power (dB)\"\n",
    "    }.get(metric, metric)\n",
    "    yl = ax.set_ylabel(ylab)\n",
    "    yl.set_fontproperties(termes_font_bold)\n",
    "    yl.set_fontsize(16)\n",
    "    title = ax.set_title(f\"{ylab}\")\n",
    "\n",
    "    if thresholds:\n",
    "        for thr in thresholds:\n",
    "            ax.axhline(thr, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    if show_probe_legend and handles_needed:\n",
    "        legend = ax.legend(handles=list(handles_needed.values()), loc=\"upper right\", ncols=2, fontsize=8)\n",
    "\n",
    "    # Adjust font\n",
    "    for text in legend.get_texts():\n",
    "        text.set_fontproperties(termes_font)\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        label.set_fontproperties(termes_font_bold)\n",
    "    title.set_fontproperties(termes_font_bold)\n",
    "    title.set_fontsize(20)\n",
    "\n",
    "    #ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        base = os.path.join(save_dir, f\"HF_{metric}_{kind}\")\n",
    "        fig.savefig(base + \".png\", dpi=600, bbox_inches=\"tight\", pad_inches=0.2)\n",
    "        fig.savefig(base + \".pdf\", bbox_inches=\"tight\", pad_inches=0.2)\n",
    "\n",
    "    figs[metric] = fig\n",
    "    plt.close(fig)\n",
    "    return figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7189b1d4-d903-47af-b0ab-ccb85721b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Analysis Functions #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3727f606-2041-4fef-be42-4a54c0e0bee1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_data():\n",
    "    try:\n",
    "        # TensorPAC method\n",
    "        f_pha = 12     # frequency phase for the coupling\n",
    "        f_amp = 75     # frequency amplitude for the coupling\n",
    "        n_epochs = 30  # number of trials\n",
    "        epoch_len = 20 # length of epoch in seconds\n",
    "        sf = 2000.     # sampling frequency\n",
    "        n_times = int(epoch_len * sf)  # number of time points\n",
    "        n_samples = int(n_epochs * epoch_len * sf)\n",
    "        sampling_rate = sf\n",
    "        tensorpac_epochs, time = pac_signals_wavelet(sf=sf, f_pha=f_pha, f_amp=f_amp, noise=1.0,\n",
    "                                      n_epochs=n_epochs, n_times=n_times)\n",
    "        raw_data = tensorpac_epochs.flatten()\n",
    "        \n",
    "        # Manual method\n",
    "        #raw_data, _ = generate_custom_pac(f_pha, f_amp, sf, n_epochs * epoch_len)\n",
    "        \n",
    "        # Scale synthetic PAC signal to realistic amplitude (e.g., ±0.5 mV)\n",
    "        target_peak_to_peak = 10  # mV total span (±5 mV)\n",
    "        actual_ptp = np.ptp(raw_data)\n",
    "        scale_factor = target_peak_to_peak / actual_ptp\n",
    "        raw_data *= scale_factor * 0.015 # Adjust const to weaken coupling\n",
    "        \n",
    "        ### Add noise to signal ###\n",
    "        scaling_factor = 0.025\n",
    "        \n",
    "        # Compute noise using both custom methods\n",
    "        aperiodic = generate_1overf_noise(n_samples=n_samples, sf=sf, exponent=1, amplitude=2.0)\n",
    "        # Add additional noise to the signal for more authenticity\n",
    "        power = scaling_factor * np.sin(2 * np.pi * 60 * n_samples)         # 60 Hz interference\n",
    "        power_harmonic = scaling_factor * 0.6 * np.sin(2 * np.pi * 120 * n_samples)  # 120 Hz harmonic\n",
    "        power_harmonic_2 = scaling_factor * 0.3 * np.sin(2 * np.pi * 180 * n_samples)  # 120 Hz harmonic\n",
    "\n",
    "        # Combine with coupled data\n",
    "        raw_data += scaling_factor * aperiodic\n",
    "        raw_data += power + power_harmonic + power_harmonic_2\n",
    "        return raw_data\n",
    "    except Exception as e:\n",
    "        import logging\n",
    "        logging.error(f\"Error in generate_synthetic_data: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29ecd1c1-b6c0-48ff-818b-5cf450c2c249",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def capture_stdout(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Utility function that performs a function but directs the console output\n",
    "    to a buffer that can be stored in a JSON or txt. NOTE data is passed by \n",
    "    reference, the original values are modified, not copies.\n",
    "    \n",
    "    result = original result of function, use _ if no variable is set to result\n",
    "    buf.getvalue() = console output\n",
    "    None/e = returns if an error occurs\n",
    "    \n",
    "    Example use:\n",
    "    raw = mne.io.RawArray(raw_data[np.newaxis, :], info)\n",
    "    Turns into,\n",
    "    raw, log, e = capture_stdout(mne.io.RawArray, raw_data[np.newaxis, :], info)\n",
    "    \"\"\"\n",
    "    buf = io.StringIO()\n",
    "    try:\n",
    "        with contextlib.redirect_stdout(buf):\n",
    "            result = func(*args, **kwargs)\n",
    "        return result, buf.getvalue(), None\n",
    "    except Exception as e:\n",
    "        return None, buf.getvalue(), e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c245802-60ef-430a-8e5f-72620980a0e8",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_pac_outputs(\n",
    "    Analysis_class, output_directory, rat, channel_name, day_num, full_day_desc, start_s, end_s, \n",
    "    timestamp, p, log_lines, pac_map, json_path, probe, day_session, SNR_JSON_data, HF_JSON_data,\n",
    "    skip_PAC\n",
    "):\n",
    "    \"\"\"\n",
    "    Save PAC comodulogram figure(s) and JSON outputs.\n",
    "\n",
    "    Writes:\n",
    "      1) A per-run JSON beside the figure with full details (comod matrix, peak, logs, metadata).\n",
    "      2) Updates the aggregator JSON at `json_path` with compact per-channel/day summaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ---- Export settings\n",
    "        (output_folder_name,\n",
    "         export_PNG, export_PDF, export_SVG, export_EPS,\n",
    "         image_height, image_width, image_DPI, color_palette,\n",
    "         y_custom_axis, yaxis_top, yaxis_bottom,\n",
    "         alpha, save_data) = Analysis_class.get_export_parameters()\n",
    "\n",
    "        # Hard-coded setting to simplify plots for placing in thesis\n",
    "        minimal_plots = True\n",
    "\n",
    "        if skip_PAC == False:\n",
    "            (PAC_custom_colormap, PAC_vmin, PAC_vmax, PAC_comod_interpolation) = Analysis_class.get_all_PAC_export_parameters()\n",
    "    \n",
    "            if PAC_custom_colormap is False:\n",
    "                PAC_vmin, PAC_vmax = None, None\n",
    "    \n",
    "            if PAC_comod_interpolation == 0.0:\n",
    "                PAC_comod_interpolation = None\n",
    "            else:\n",
    "                PAC_comod_interpolation = (PAC_comod_interpolation, PAC_comod_interpolation)\n",
    "    \n",
    "            PAC_method = Analysis_class.get_PAC_method()\n",
    "            norm_method = Analysis_class.get_normalization_method()\n",
    "    \n",
    "            # Label short-hands\n",
    "            PAC_method_map = {\n",
    "                'Mean Vector Length': 'MVL',\n",
    "                'Modulation Index': 'MI',\n",
    "                'Heights Ratio': 'HR',\n",
    "                'ndPAC': 'ndPAC',\n",
    "                'Phase-Locking Value': 'PLV',\n",
    "                'Gaussian Copula PAC': 'GC PAC'\n",
    "            }\n",
    "            PAC_method = PAC_method_map.get(PAC_method, PAC_method)\n",
    "    \n",
    "            norm_method_map = {\n",
    "                'No Normalization': '',\n",
    "                'Subtract Mean of Surrogates': 'Sub Mean of Surg.',\n",
    "                'Divide Mean of Surrogates': 'Div Mean of Surg.',\n",
    "                'Sub+Div Mean of Surrogates': 'Sub+Div Mean of Surg.',\n",
    "                'Z-score': 'Z-Score'\n",
    "            }\n",
    "            norm_method_label = norm_method_map.get(norm_method, norm_method)\n",
    "    \n",
    "            # ---- Comod & p-values\n",
    "            # pac_map: typically (n_phase, n_amp, n_epochs). We average over last axis:\n",
    "            comod = pac_map.mean(-1)  # 2D array [n_phase, n_amp]\n",
    "            # Be careful: infer_pvalues may return numeric p-values or a boolean significance mask.\n",
    "            pvals = p.infer_pvalues(p=0.05)\n",
    "\n",
    "            # Hide axis labels in minimal mode; keep ticks + title\n",
    "            xlabel = None if minimal_plots else \"Frequency for Phase (Hz)\"\n",
    "            ylabel = None if minimal_plots else \"Frequency for Amplitude (Hz)\"\n",
    "    \n",
    "            # ---- Plot\n",
    "            if minimal_plots:\n",
    "                fig = plt.figure(figsize=(5.236,5), constrained_layout=True)\n",
    "                fig.set_constrained_layout_pads(w_pad=0.1, h_pad=0.1, wspace=0.1, hspace=0.1)\n",
    "                title = f\"{probe}\"\n",
    "                colorbar = False\n",
    "            else:\n",
    "                plt.figure(figsize=(image_width, image_height))\n",
    "                title=f\"Rat{rat} | {full_day_desc} | {day_session} | {probe}\"\n",
    "                colorbar = True\n",
    "            p.comodulogram(\n",
    "                comod,\n",
    "                title=title,\n",
    "                xlabel=xlabel,\n",
    "                ylabel=ylabel,\n",
    "                fz_labels=10,\n",
    "                cmap='viridis',\n",
    "                colorbar=colorbar,\n",
    "                vmin=PAC_vmin,\n",
    "                vmax=PAC_vmax,\n",
    "                interp=PAC_comod_interpolation\n",
    "            )\n",
    "    \n",
    "            ax = plt.gca()\n",
    "            ax.set_title(ax.get_title(), fontproperties=termes_font_bold, fontsize=24)\n",
    "            if not minimal_plots:\n",
    "                ax.set_xlabel(ax.get_xlabel(), fontproperties=termes_font_bold, fontsize=16)\n",
    "                ax.set_ylabel(ax.get_ylabel(), fontproperties=termes_font_bold, fontsize=16)\n",
    "            for tick in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "                tick.set_fontproperties(termes_font)\n",
    "    \n",
    "            if ax.images and not minimal_plots:\n",
    "                cbar = ax.figure.axes[-1]\n",
    "                if norm_method_label == 'Z-Score':\n",
    "                    cbar.set_ylabel(norm_method_label, fontproperties=termes_font_bold)\n",
    "                else:\n",
    "                    cbar.set_ylabel((PAC_method + (\"  (\" + norm_method_label + \")\" if norm_method_label else \"\")),\n",
    "                                    fontproperties=termes_font_bold)\n",
    "                for label in cbar.get_yticklabels():\n",
    "                    label.set_fontproperties(termes_font)\n",
    "    \n",
    "            # ---- Paths\n",
    "            rat_folder = os.path.join(output_directory, f\"Rat{rat}\")\n",
    "            day_folder = os.path.join(rat_folder, f\"{full_day_desc}\")\n",
    "            os.makedirs(day_folder, exist_ok=True)\n",
    "    \n",
    "            start_min = int(start_s / 60)\n",
    "            end_min   = int(end_s   / 60)\n",
    "            base_name = f\"Rat{rat}_{channel_name}_min{start_min}-{end_min}_{timestamp}\"\n",
    "    \n",
    "            # ---- Save figure(s)\n",
    "            if export_PNG:\n",
    "                plt.savefig(os.path.join(day_folder, base_name + \".png\"), dpi=image_DPI)\n",
    "            if export_PDF:\n",
    "                plt.savefig(os.path.join(day_folder, base_name + \".pdf\"))\n",
    "            if export_SVG:\n",
    "                plt.savefig(os.path.join(day_folder, base_name + \".svg\"))\n",
    "            if export_EPS:\n",
    "                plt.savefig(os.path.join(day_folder, base_name + \".eps\"))\n",
    "            plt.close()\n",
    "    \n",
    "            # Peak index (guard against all-NaN just in case)\n",
    "            if np.all(np.isnan(comod)):\n",
    "                raise ValueError(\"Comodulogram is all NaN; cannot compute a peak.\")\n",
    "            \n",
    "            idx = np.unravel_index(np.nanargmax(comod, axis=None), comod.shape)\n",
    "            \n",
    "            pha_info = parse_freq_at(p.f_pha, idx[0], mode=\"center\")\n",
    "            amp_info = parse_freq_at(p.f_amp, idx[1], mode=\"center\")\n",
    "            \n",
    "            pha_f    = float(pha_info[\"center\"])   # for single-number reporting\n",
    "            amp_f    = float(amp_info[\"center\"])\n",
    "            peak_val = float(comod[idx])\n",
    "    \n",
    "            # pval/Significance at peak\n",
    "            peak_p = None\n",
    "            peak_sig = None\n",
    "            if pvals is not None:\n",
    "                # Heuristics: detect shape & dtype\n",
    "                pv = pvals[idx]\n",
    "                if isinstance(pv, (float, np.floating)):\n",
    "                    peak_p = float(pv)\n",
    "                elif isinstance(pv, (bool, np.bool_)):\n",
    "                    peak_sig = bool(pv)\n",
    "                # else: leave None\n",
    "    \n",
    "            # ---- Summaries suited for Z-score (fall back gracefully if not Z)\n",
    "            is_z = (norm_method == 'Z-score')\n",
    "            z = comod if is_z else comod  # if not Z, these are just \"values\"; still useful\n",
    "    \n",
    "            # Numerical summaries (NaN-safe)\n",
    "            def safe_mean(a): return float(np.nanmean(a))\n",
    "            def safe_percentile(a, q): return float(np.nanpercentile(a, q))\n",
    "    \n",
    "            z_abs_mean  = safe_mean(np.abs(z))\n",
    "            z_mean_pos  = safe_mean(np.clip(z, 0, None))\n",
    "            z_mean      = safe_mean(z)\n",
    "            z_p95       = safe_percentile(z, 95.0)\n",
    "            # Top-k mean (5% of bins)\n",
    "            flat = z.ravel()\n",
    "            k = max(1, int(0.05 * flat.size))\n",
    "            if k < flat.size:\n",
    "                kth = np.partition(flat, flat.size - k)[-k:]\n",
    "                z_topk_mean = float(np.nanmean(kth))\n",
    "            else:\n",
    "                z_topk_mean = z_p95\n",
    "    \n",
    "            # Fraction significant & excess area (only meaningful if Z)\n",
    "            z_thr = 1.96\n",
    "            frac_sig = float(np.mean((z >= z_thr))) if is_z else None\n",
    "            excess_area = float(np.mean(np.clip(z - z_thr, 0, None))) if is_z else None\n",
    "    \n",
    "            # ---- Per-run JSON (full details; can be large)\n",
    "            # To keep size sane, round to 4 decimals (you can change this).\n",
    "            comod_rounded = np.round(comod.astype(np.float32), 4).tolist()\n",
    "            # You can also choose to omit p-values matrix if it’s large; here we store just peak info.\n",
    "    \n",
    "            run_json = {\n",
    "                \"meta\": {\n",
    "                    \"rat\": int(rat),\n",
    "                    \"day_number\": int(day_num),\n",
    "                    \"full_day_desc\": full_day_desc,\n",
    "                    \"channel\": channel_name,\n",
    "                    \"chunk_min\": [start_min, end_min],\n",
    "                    \"probe\": probe,\n",
    "                    \"day_session\": day_session,\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"pac_method\": PAC_method,\n",
    "                    \"normalization\": norm_method,          # full string\n",
    "                    \"normalization_label\": norm_method_label,  # short label used on plots\n",
    "                    \"vmin\": PAC_vmin,\n",
    "                    \"vmax\": PAC_vmax,\n",
    "                    \"interp\": PAC_comod_interpolation\n",
    "                },\n",
    "                \"comodulogram\": {\n",
    "                    \"shape\": [int(comod.shape[0]), int(comod.shape[1])],\n",
    "                    \"values\": comod_rounded,\n",
    "                },\n",
    "                \"peak\": {\n",
    "                    \"phase_center_hz\": pha_info[\"center\"],\n",
    "                    \"phase_low_hz\": pha_info[\"low\"],\n",
    "                    \"phase_high_hz\": pha_info[\"high\"],\n",
    "                    \"amp_center_hz\": amp_info[\"center\"],\n",
    "                    \"amp_low_hz\": amp_info[\"low\"],\n",
    "                    \"amp_high_hz\": amp_info[\"high\"],\n",
    "                    \"value\": peak_val,\n",
    "                    \"p_value\": peak_p,\n",
    "                    \"significant_at_alpha\": peak_sig\n",
    "                },\n",
    "                \"summaries\": {\n",
    "                    \"is_z\": bool(is_z),\n",
    "                    \"z_mean\": z_mean,\n",
    "                    \"z_mean_pos\": z_mean_pos,\n",
    "                    \"z_abs_mean\": z_abs_mean,\n",
    "                    \"z_topk_mean\": z_topk_mean,\n",
    "                    \"z_p95\": z_p95,\n",
    "                    \"frac_sig_ge_1.96\": frac_sig,\n",
    "                    \"excess_area_over_1.96\": excess_area,\n",
    "                    \"primary_summary\": \"z_mean_pos\" if is_z else \"z_mean\"\n",
    "                },\n",
    "                \"log\": list(log_lines)\n",
    "            }\n",
    "    \n",
    "            run_json_path = os.path.join(day_folder, base_name + \".json\")\n",
    "            with open(run_json_path, \"w\") as f:\n",
    "                json.dump(run_json, f, indent=2)\n",
    "    \n",
    "            # ---- Aggregator JSON (compact; for plotting over time)\n",
    "            rat_str, day_str, day_session_str = str(rat), str(day_num), str(day_session)\n",
    "    \n",
    "            # Load or init aggregator\n",
    "            if os.path.exists(json_path):\n",
    "                with open(json_path, \"r\") as f:\n",
    "                    pac_data = json.load(f)\n",
    "            else:\n",
    "                pac_data = {}\n",
    "    \n",
    "            pac_data.setdefault(\"Data\", {})\n",
    "            pac_data[\"Data\"].setdefault(rat_str, {})\n",
    "            pac_data[\"Data\"][rat_str].setdefault(day_str, {})\n",
    "            pac_data[\"Data\"][rat_str][day_str].setdefault(day_session_str, {})\n",
    "    \n",
    "            # Choose which metric is your “average coupling” line:\n",
    "            # If Z -> use z_mean_pos (no cancellation). Else -> mean of values.\n",
    "            average_coupling = z_mean_pos if is_z else z_mean\n",
    "    \n",
    "            pac_data[\"Data\"][rat_str][day_str][day_session_str][channel_name] = {\n",
    "                \"Probe Type\": probe,\n",
    "                \"normalization\": norm_method,\n",
    "                \"pac_method\": PAC_method,\n",
    "                \"primary_summary_value\": average_coupling,\n",
    "                \"primary_summary_name\": \"mean_positive_z\" if is_z else \"mean_value\",\n",
    "                \"peak_value\": peak_val,\n",
    "                \"peak_phase_hz\": pha_f,\n",
    "                \"peak_amplitude_hz\": amp_f,\n",
    "                \"run_json_path\": run_json_path  # so you can reload full details later\n",
    "            }\n",
    "    \n",
    "            # Optional: stash a couple more summaries you may plot later\n",
    "            pac_data[\"Data\"][rat_str][day_str][day_session_str][channel_name].update({\n",
    "                \"z_abs_mean\": z_abs_mean,\n",
    "                \"z_topk_mean\": z_topk_mean,\n",
    "                \"frac_sig_ge_1.96\": frac_sig\n",
    "            })\n",
    "    \n",
    "            if SNR_JSON_data:\n",
    "                pac_data[\"Data\"][rat_str][day_str][day_session_str][channel_name].update(SNR_JSON_data)\n",
    "    \n",
    "            if HF_JSON_data:\n",
    "                pac_data[\"Data\"][rat_str][day_str][day_session_str][channel_name].update(HF_JSON_data)\n",
    "    \n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(pac_data, f, indent=2)\n",
    "            #####################################################################################################################\n",
    "        else:\n",
    "            # ---- Paths\n",
    "            rat_folder = os.path.join(output_directory, f\"Rat{rat}\")\n",
    "            day_folder = os.path.join(rat_folder, f\"{full_day_desc}\")\n",
    "            os.makedirs(day_folder, exist_ok=True)\n",
    "            base_name = f\"Rat{rat}_{channel_name}_{timestamp}\"\n",
    "            \n",
    "            run_json = {\n",
    "                \"meta\": {\n",
    "                    \"rat\": int(rat),\n",
    "                    \"day_number\": int(day_num),\n",
    "                    \"full_day_desc\": full_day_desc,\n",
    "                    \"channel\": channel_name,\n",
    "                    \"probe\": probe,\n",
    "                    \"day_session\": day_session,\n",
    "                    \"timestamp\": timestamp,\n",
    "                },\n",
    "                \"log\": list(log_lines)\n",
    "            }\n",
    "    \n",
    "            run_json_path = os.path.join(day_folder, base_name + \".json\")\n",
    "            with open(run_json_path, \"w\") as f:\n",
    "                json.dump(run_json, f, indent=2)\n",
    "    \n",
    "            # ---- Aggregator JSON (compact; for plotting over time)\n",
    "            rat_str, day_str, day_session_str = str(rat), str(day_num), str(day_session)\n",
    "    \n",
    "            # Load or init aggregator\n",
    "            if os.path.exists(json_path):\n",
    "                with open(json_path, \"r\") as f:\n",
    "                    pac_data = json.load(f)\n",
    "            else:\n",
    "                pac_data = {}\n",
    "    \n",
    "            pac_data.setdefault(\"Data\", {})\n",
    "            pac_data[\"Data\"].setdefault(rat_str, {})\n",
    "            pac_data[\"Data\"][rat_str].setdefault(day_str, {})\n",
    "            pac_data[\"Data\"][rat_str][day_str].setdefault(day_session_str, {})\n",
    "    \n",
    "            pac_data[\"Data\"][rat_str][day_str][day_session_str][channel_name] = {\n",
    "                \"Probe Type\": probe,\n",
    "                \"run_json_path\": run_json_path  # so you can reload full details later\n",
    "            }\n",
    "    \n",
    "            if SNR_JSON_data:\n",
    "                pac_data[\"Data\"][rat_str][day_str][day_session_str][channel_name].update(SNR_JSON_data)\n",
    "    \n",
    "            if HF_JSON_data:\n",
    "                pac_data[\"Data\"][rat_str][day_str][day_session_str][channel_name].update(HF_JSON_data)\n",
    "    \n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(pac_data, f, indent=2)\n",
    "\n",
    "    except Exception as e:\n",
    "        import logging\n",
    "        logging.error(f\"Error in save_pac_outputs: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ec3ff24-7891-443a-808f-877ad11e3fdc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_PAC_of_channels(Analysis_class):\n",
    "    \"\"\"\n",
    "    This function processes PAC on channels individually. It is the original way of\n",
    "    performing the computation but by design cannot use the autofilter library which\n",
    "    necessitates processing all channels of a rat at once. It is kept to retain the\n",
    "    felxiblity that comes with choosing channels to process but is not able to remove\n",
    "    poor epochs from data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve PAC settings\n",
    "        (pha_freqs, amp_freqs, idpac, dcomplex, cycles, width, n_bins,\n",
    "         sampling_rate, epoch_len, data_length, minimum_length, parallel_processes,\n",
    "         surrogate_permutations, notch_freqs, high_pass_filter, detrend_epochs,\n",
    "         apply_autofilter, downsample_rate, seed, rejection_threshold, skip_PAC,\n",
    "         inject_PAC)= Analysis_class.get_all_PAC_parameters()\n",
    "\n",
    "        # Initialize TensorPAC object\n",
    "        p, PAC_msg, e = capture_stdout(Pac, idpac=idpac, f_pha=pha_freqs, f_amp=amp_freqs,\n",
    "                                   dcomplex=dcomplex, cycle=cycles, width=width, n_bins=n_bins, verbose=True)\n",
    "\n",
    "        # Prepare IO paths\n",
    "        timestamp = datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "        folder_path = Analysis_class.get_folder_path\n",
    "        nwb_list = Analysis_class.get_nwb_list()\n",
    "        channels = Analysis_class.get_selected_channels()\n",
    "\n",
    "        output_directory = Analysis_class.get_output_directory()\n",
    "        output_folder_name = Analysis_class.get_output_folder_name()\n",
    "        if \"[Timestamp]\" in output_folder_name:\n",
    "            output_folder_name = output_folder_name.split(\"[Timestamp]\")[0]\n",
    "            output_folder_name = f\"{output_folder_name}{timestamp}\"\n",
    "        \n",
    "        output_directory = os.path.join(output_directory, output_folder_name)\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        json_path = os.path.join(output_directory, \"Session Summary.json\")\n",
    "\n",
    "        # Add Session-level information to JSON output file\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, \"r\") as f:\n",
    "                pac_data = json.load(f)\n",
    "        else:\n",
    "            pac_data = {}\n",
    "\n",
    "        metadata = \"Metadata\"\n",
    "        pac_data[metadata] = {\n",
    "            \"pha_freqs\" : pha_freqs, \n",
    "            \"amp_freqs\" : amp_freqs, \n",
    "            \"idpac\" : idpac, \n",
    "            \"dcomplex\" : dcomplex, \n",
    "            \"cycles\" : cycles, \n",
    "            \"width\" : width, \n",
    "            \"n_bins\" : n_bins,\n",
    "            \"sampling_rate\" : sampling_rate, \n",
    "            \"epoch_len\" : epoch_len, \n",
    "            \"data_length\" : data_length, \n",
    "            \"minimum_length\" : minimum_length, \n",
    "            \"parallel_processes\" : parallel_processes,\n",
    "            \"surrogate_permutations\"  : surrogate_permutations, \n",
    "            \"notch_freqs\" : notch_freqs, \n",
    "            \"high_pass_filter\" : high_pass_filter, \n",
    "            \"detrend_epochs\" : detrend_epochs,\n",
    "            \"apply_autofilter\" : apply_autofilter, \n",
    "            \"downsample_rate\" : downsample_rate, \n",
    "            \"seed\" : seed, \n",
    "            \"rejection_threshold\" : rejection_threshold,\n",
    "            \"skip_PAC\" : skip_PAC,\n",
    "            \"inject_PAC\" : inject_PAC,\n",
    "            \"timestamp\" : timestamp,\n",
    "            \"folder_path\" : folder_path,\n",
    "            \"nwb_list\" : nwb_list,\n",
    "            \"nwb_list\" : nwb_list,\n",
    "            \"channels\" : channels,\n",
    "            \"output_directory\" : output_directory,\n",
    "            \"output_folder_name\" : output_folder_name,\n",
    "            \"json_path\" : json_path,\n",
    "        }\n",
    "\n",
    "        pac_data[\"Data\"] = {}\n",
    "        \n",
    "        # Write back to JSON\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(pac_data, f, indent=4)\n",
    "\n",
    "        if seed == 0:\n",
    "            seed = None\n",
    "\n",
    "        # Beginning of PAC Algorithm\n",
    "        for file in nwb_list:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            selected  = channels.get(file, {})\n",
    "            if not selected:\n",
    "                # No channels selected for file, skipping.\n",
    "                continue\n",
    "            \n",
    "            for channel_name, data_index in selected.items():\n",
    "                ##### 1) Load Data & Create Objects #####\n",
    "\n",
    "                # Fetch corresponding rat and it's probe from channel/file\n",
    "                rat, probe = Analysis.get_rat_and_probe_from_channel(file, channel_name)\n",
    "                day_num, full_day_desc = Analysis.get_day_from_file(file)\n",
    "                \n",
    "                # Obtain current rat and probe from channel\n",
    "                channel_num = int(channel_name.split('CSC')[1])\n",
    "                current_rat, current_probe = Analysis.get_rat_and_probe_from_channel(file, channel_name)\n",
    "        \n",
    "                # Grab the session (i.e. Day0_0001.nwb -> 0001)\n",
    "                split_name = file.split('_')  \n",
    "                if split_name[-1]:\n",
    "                    day_session = split_name[-1].split('.nwb')[0]\n",
    "                full_day_desc = full_day_desc + f\"_{day_session}\"\n",
    "\n",
    "                # Log information to save to txt\n",
    "                log_lines = []\n",
    "                log_lines.append(f\"############### Started PAC processing for Rat{rat}, Channel {channel_name} on Day {day_num}_{day_session}. ###############\")\n",
    "                log_lines.append(PAC_msg)\n",
    "                \n",
    "                # Load raw data\n",
    "                times, raw_data = get_raw_data(file_path, data_index, start=0, end=-1)\n",
    "                # Determine actual sampling frequency, based on length of data\n",
    "                sampling_rate = 1.0 / np.median(np.diff(times))\n",
    "                del times\n",
    "                log_lines.append(f\"Extracted NWB data from file {file}, column {data_index}, shape {np.shape(raw_data)}.\\n\")\n",
    "        \n",
    "                # Wrap raw data into MNE object for less manual processing steps\n",
    "                info = mne.create_info([channel_name], sampling_rate, ch_types=\"eeg\")\n",
    "                raw = mne.io.RawArray(raw_data[np.newaxis, :], info, verbose=False)\n",
    "                log_lines.append(f\"Wrapped raw data in MNE object with info: {info}\")\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------#\n",
    "                ##### Data tampering section #####\n",
    "\n",
    "                # Debugging w/ synthetic data\n",
    "                #raw_data = generate_synthetic_data()\n",
    "\n",
    "                # Spike injection to raw data\n",
    "                x = raw.get_data(picks=channel_name)[0]   # shape (N,)\n",
    "                \n",
    "                #----- 1.1) Inject PAC for Sensitivity Test -----#\n",
    "                if inject_PAC:\n",
    "                    fs=float(raw.info['sfreq'])\n",
    "                    y, pac_meta, y_add = inject_spike_into_raw_data(\n",
    "                        x, fs, phase_band=(6, 10), amp_band=(80, 120),\n",
    "                        depth=0.3,      # e.g., 0.1 .. 0.6\n",
    "                        target_hf_snr_db=20,  # e.g., +3 to +10 dB\n",
    "                        burst_duty=0.5,  # e.g., 0.3 .. 1.0\n",
    "                        burst_len_s=2.0, burst_random=True, random_state=42\n",
    "                    )\n",
    "                    log_lines.append(f\"Spike-in PAC: {pac_meta}\")\n",
    "                    # replace raw with the spiked data and continue pipeline:\n",
    "                    raw = mne.io.RawArray(y[np.newaxis, :], raw.info, verbose=False)\n",
    "\n",
    "\n",
    "                    # Plot Simple PSD (Injection Visualizer)\n",
    "                    psd_filt, freqs_filt = psd_array_welch(x[np.newaxis, :], \n",
    "                                                           sfreq=fs, \n",
    "                                                           n_fft=int(fs * 4), \n",
    "                                                           fmin=0, \n",
    "                                                           fmax=200, \n",
    "                                                           n_jobs=parallel_processes, \n",
    "                                                           verbose=False)\n",
    "                    psd_injected, freqs_injected = psd_array_welch(y[np.newaxis, :], \n",
    "                                                           sfreq=fs, \n",
    "                                                           n_fft=int(fs * 4), \n",
    "                                                           fmin=0, \n",
    "                                                           fmax=200, \n",
    "                                                           n_jobs=parallel_processes, \n",
    "                                                           verbose=False)\n",
    "                    \n",
    "                    psd_filt_db = 10 * np.log10(np.where(psd_filt[0] > 0, psd_filt[0], np.nan))\n",
    "                    psd_injected_db = 10 * np.log10(np.where(psd_injected[0] > 0, psd_injected[0], np.nan))\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "                    ax.plot(freqs_filt, psd_filt_db, label=f\"Raw Data\", color=\"black\")\n",
    "                    ax.plot(freqs_injected, psd_injected_db, label=f\"PAC Injection\")\n",
    "                    ax.set_title(\"PAC Spike Injection\").set_fontproperties(termes_font_bold)\n",
    "                    ax.set_xlabel(\"Frequency (Hz)\").set_fontproperties(termes_font_bold)\n",
    "                    ax.set_ylabel(\"Power Spectral Density (dB)\").set_fontproperties(termes_font_bold)\n",
    "                    #ax.set_ylim(top=35, bottom=0)\n",
    "                    legend = ax.legend()\n",
    "                    for text in legend.get_texts():\n",
    "                        text.set_fontproperties(termes_font)\n",
    "                    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "                        label.set_fontproperties(termes_font)\n",
    "                    ax.grid(True)\n",
    "                    base_name = \"PAC_spike_injection_PSD\"\n",
    "                    plt.tight_layout()\n",
    "                    fig_path = os.path.join(output_directory, base_name + \".png\")\n",
    "                    fig.savefig(fig_path, dpi=600) \n",
    "                    fig_path = os.path.join(output_directory, base_name + \".pdf\")\n",
    "                    fig.savefig(fig_path)\n",
    "                    plt.close(fig)\n",
    "                \n",
    "                #-------------------------------------------------------------------------------------------------#\n",
    "\n",
    "                ##### 2) Apply Channel-Wide Changes #####\n",
    "        \n",
    "                # Apply notch filters\n",
    "                if notch_freqs:\n",
    "                    raw.notch_filter(notch_freqs, picks=channel_name, method='fir', verbose=False)\n",
    "                    log_lines.append(f\"Data notch filtered for frequencies: {notch_freqs}\")\n",
    "        \n",
    "                # Apply high-pass filter\n",
    "                if high_pass_filter and high_pass_filter > 0.0:\n",
    "                    raw.filter(l_freq=high_pass_filter, h_freq=200, picks=channel_name, verbose=False)\n",
    "                    log_lines.append(f\"Data bandpass filtered from {high_pass_filter} to 200 Hz.\")\n",
    "\n",
    "                # Downsample data\n",
    "                if downsample_rate:\n",
    "                    raw.resample(sfreq=downsample_rate, verbose=False)\n",
    "                    log_lines.append(f\"Data downsampled from {sampling_rate} to {downsample_rate}.\")\n",
    "                downsampled_sampling_rate = raw.info['sfreq']\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------#\n",
    "\n",
    "                ##### 2) Track SNR and HF Band Power #####\n",
    "                \n",
    "                #----- 2.1) Phase-Band SNR Check -----#\n",
    "                check_SNR = True\n",
    "                SNR_JSON_data = None\n",
    "                if check_SNR == True:\n",
    "                    bands_name = ['Delta', 'Theta', 'Wide Theta', 'High Theta', 'Beta']\n",
    "                    bands = [(1,4), (6,10), (4,12), (10,16), (13,30)]\n",
    "                    SNR_JSON_data = {}\n",
    "                    x = raw.get_data(picks=channel_name)[0]\n",
    "                    fs = float(raw.info['sfreq'])\n",
    "                    for i, b in enumerate(bands):\n",
    "                        snr_db, snr_info = phase_band_snr(x, fs, band=b, neighbor_bw=2.0, exclude_bw=0.5, remove_aperiodic=True)\n",
    "                        SNR_JSON_data[f\"{bands_name[i]}\"] = snr_info\n",
    "                        log_lines.append(f\"Phase-band SNR: {snr_db} dB at ~{snr_info.get('f0', np.nan)} Hz \"\n",
    "                                         f\"(aperiodic_removed={snr_info['aperiodic_removed']})\")\n",
    "\n",
    "                #----- 2.2) HF Band Power -----#\n",
    "                check_HF = True\n",
    "                HF_JSON_data = None\n",
    "                if check_HF == True:\n",
    "                    HF_JSON_data = {}\n",
    "                    # Set HF band to process\n",
    "                    hf_low, hf_high = 50, 120\n",
    "                    \n",
    "                    hf_metrics = hf_band_power_metrics(\n",
    "                        x, fs,\n",
    "                        hf_band=(hf_low, hf_high),\n",
    "                        ref_band=(20, 150),\n",
    "                        remove_aperiodic=True,\n",
    "                        fit_range=(2, 200)\n",
    "                    )\n",
    "                    HF_JSON_data[\"HF Power\"] = hf_metrics\n",
    "                    log_lines.append(\n",
    "                        \"HF Power: \"\n",
    "                        f\"band={hf_metrics.get('hf_band')}, ref={hf_metrics.get('ref_band')}, \"\n",
    "                        f\"rel={hf_metrics.get('hf_rel_power', np.nan):.4f}, \"\n",
    "                        f\"pct_in_ref={hf_metrics.get('hf_percentile_in_ref', np.nan):.1f}%\"\n",
    "                    )   \n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------#\n",
    "\n",
    "                # Skip entire PAC calculation if only want additional diagnostic info on data\n",
    "                if skip_PAC == False:\n",
    "                    ##### 3) Trim Sample, Epoch Data, Detrend #####\n",
    "    \n",
    "                    # Determine end of segment to process, in seconds\n",
    "                    start_sec = 0\n",
    "                    target_sec = data_length * 60.0 if data_length != -1 else raw.times[-1]\n",
    "                    target_min = int(target_sec / 60)\n",
    "                    tmax = min(target_sec, raw.times[-1])\n",
    "                    # Snap to an exact number of samples:\n",
    "                    target_n = int(np.floor(min(tmax, raw.times[-1]) * downsampled_sampling_rate))\n",
    "                    tmax_exact = (target_n - 1) / downsampled_sampling_rate if target_n > 0 else 0\n",
    "                    raw.crop(tmin=start_sec, tmax=tmax_exact, include_tmax=True, verbose=False)\n",
    "                    log_lines += [\n",
    "                        f\"Raw fs before resample: {sampling_rate} Hz\",\n",
    "                        f\"Raw fs after resample:  {downsampled_sampling_rate} Hz\",\n",
    "                        f\"Total samples: {raw.n_times}, total_sec: {raw.times[-1]}\",\n",
    "                    ]\n",
    "    \n",
    "                    # Skip file if shorter than allowed minimum\n",
    "                    if target_min < minimum_length:\n",
    "                        log_lines.append(f\"{file} < {minimum_length} min. Skipping.\")\n",
    "                        continue\n",
    "    \n",
    "                    reject_criteria = None if rejection_threshold == 0.0 else dict(eeg=rejection_threshold * 1e-3)  # scaled to mV\n",
    "    \n",
    "                    events = mne.make_fixed_length_events(raw, duration=epoch_len)\n",
    "                    epochs = mne.Epochs(raw, \n",
    "                                        events, \n",
    "                                        tmin=0, \n",
    "                                        tmax=epoch_len, \n",
    "                                        reject=reject_criteria,  \n",
    "                                        baseline=None, \n",
    "                                        preload=True)\n",
    "                    log_lines.append(f\"Data Epoched to a length of {epoch_len} seconds.\")\n",
    "    \n",
    "                    if len(epochs) == 0:\n",
    "                        log_lines.append(f\"All epochs rejected. Channel skipped as bad channel.\")\n",
    "                        continue \n",
    "                    \n",
    "                    # Detrend data\n",
    "                    if detrend_epochs:\n",
    "                        data = epochs.get_data()\n",
    "                        order = 1\n",
    "                        data_detrended = mne.filter.detrend(data, order=order)\n",
    "                        log_lines.append(f\"Epochs detrended with an order of {order}.\")\n",
    "                        cleaned_epochs = data_detrended.squeeze(axis=1)\n",
    "                    else:\n",
    "                        cleaned_epochs = epochs.get_data().squeeze(axis=1)\n",
    "    \n",
    "                    ##### 4) Extract phase and amplitdue components of epochs, compute PAC map, permute surrogate data #####\n",
    "                    \n",
    "                    phases, msg, e = capture_stdout(p.filter, downsampled_sampling_rate, cleaned_epochs,\n",
    "                                                    ftype='phase', n_jobs=parallel_processes)\n",
    "                    log_lines.append(msg)\n",
    "                    \n",
    "                    amplitudes, msg, e = capture_stdout(p.filter, downsampled_sampling_rate, cleaned_epochs,\n",
    "                                                        ftype='amplitude', n_jobs=parallel_processes)\n",
    "                    log_lines.append(msg)\n",
    "            \n",
    "                    pac_map, msg, e = capture_stdout(p.fit, phases, amplitudes, n_perm=surrogate_permutations,\n",
    "                                                     n_jobs=parallel_processes, random_state=seed, verbose=False)\n",
    "                    log_lines.append(msg)\n",
    "                    \n",
    "                    del phases, amplitudes\n",
    "\n",
    "                    ##### 5) Generate comodulogram, export plot and algorithm info to txt, and session info to JSON #####\n",
    "                    \n",
    "                    save_pac_outputs(Analysis_class, output_directory, rat, channel_name, day_num, full_day_desc, start_sec, target_sec, \n",
    "                                     timestamp, p, log_lines, pac_map, json_path, probe, day_session, SNR_JSON_data, HF_JSON_data, skip_PAC)\n",
    "                else:\n",
    "                    start_sec = None\n",
    "                    target_sec = None\n",
    "                    p = None\n",
    "                    pac_map = None\n",
    "                    save_pac_outputs(Analysis_class, output_directory, rat, channel_name, day_num, full_day_desc, start_sec, target_sec, \n",
    "                                     timestamp, p, log_lines, pac_map, json_path, probe, day_session, SNR_JSON_data, HF_JSON_data, skip_PAC)\n",
    "            \n",
    "    except Exception as e:\n",
    "            logging.error(f\"Error in compute_PAC_of_channels: {e}\", exc_info=True)\n",
    "            show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9443ed6-ac8a-4e9e-8b0e-fac80299783b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_PAC_of_rats(Analysis_class):\n",
    "    \"\"\"\n",
    "    This function processes PAC on channels individually. It is the original way of\n",
    "    performing the computation but by design cannot use the autofilter library which\n",
    "    necessitates processing all channels of a rat at once. It is kept to retain the\n",
    "    felxiblity that comes with choosing channels to process but is not able to remove\n",
    "    poor epochs from data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve PAC settings\n",
    "        (pha_freqs, amp_freqs, idpac, dcomplex, cycles, width, n_bins,\n",
    "         sampling_rate, epoch_len, MIN_CHUNK_MIN, CHUNK_MIN, CHUNK_LIMIT,\n",
    "         parallel_processes, surrogate_permutations, notch_freqs,\n",
    "         high_pass_filter, detrend_epochs, apply_autofilter\n",
    "        )= Analysis_class.get_all_PAC_parameters()\n",
    "\n",
    "        # Initialize TensorPAC object\n",
    "        p = Pac(idpac=idpac, f_pha=pha_freqs, f_amp=amp_freqs,\n",
    "                dcomplex=dcomplex, cycle=cycles, width=width, n_bins=n_bins)\n",
    "\n",
    "        # Prepare IO paths\n",
    "        output_dir = r\"C:\\Users\\holot\\Desktop\\Summer Research\\Research GUI V2\\PAC Comodulogram\"\n",
    "        folder_path = Analysis_class.get_folder_path\n",
    "        nwb_list = Analysis_class.get_nwb_list()\n",
    "\n",
    "\n",
    "        for file in nwb_list:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "\n",
    "            # Nested dict with file, rat, channel, and column info\n",
    "            selected_rats_info = Analysis_class.get_selected_rats()\n",
    "            rat_keys = selected_rats_info[file].keys()\n",
    "            for rat in rat_keys:\n",
    "                data_columns = selected_rats_info[file][rat][\"data_columns\"]\n",
    "                channels = selected_rats_info[file][rat][\"channels\"]\n",
    "                channel_names = Analysis_class.get_channel_names_from_rat(rat)\n",
    "\n",
    "                # Sort columns for HDF5 to be able to grab from matrix\n",
    "                # Match sorting with channels to retain channel/column/name pairing\n",
    "                # a) zip → (column, channel, name)\n",
    "                linked_col_ch_nm = list(zip(data_columns, channels, channel_names))\n",
    "                \n",
    "                # b) sort by column index (first element of each tuple)\n",
    "                linked_col_ch_nm.sort(key=lambda x: x[0])\n",
    "                \n",
    "                # c) unzip into synced, sorted lists\n",
    "                data_columns, channels, channel_names = map(list, zip(*linked_col_ch_nm))\n",
    "\n",
    "                # Grab raw data (n_ch, n_samples)\n",
    "                data = get_raw_matrix(file_path, data_columns)  \n",
    "                \n",
    "                # Build MNE object\n",
    "                \n",
    "                #channel_names = []\n",
    "                #for channel_num in sorted_channels:\n",
    "                #    channel_names.append('CSC' + str(channel_num))\n",
    "                \n",
    "                info = mne.create_info(ch_names=channel_names, sfreq=sampling_rate, ch_types=\"eeg\")\n",
    "                info.set_montage(rat_probe_montage(channel_names))  # coords for AutoReject\n",
    "                raw  = mne.io.RawArray(data, info)\n",
    "            \n",
    "                # Apply channel-wide filters\n",
    "                if notch_freqs:\n",
    "                    raw.notch_filter(freqs=notch_freqs, method=\"fir\")\n",
    "                if high_pass_filter:\n",
    "                    raw.filter(l_freq=high_pass_filter, h_freq=None)\n",
    "            \n",
    "                # Epoch data\n",
    "                events = mne.make_fixed_length_events(raw, duration=epoch_len)\n",
    "                epochs = mne.Epochs(raw, events, tmin=0, tmax=epoch_len,\n",
    "                                    baseline=None, preload=True)\n",
    "            \n",
    "                # Apply Autoreject filter\n",
    "                ar = AutoReject(n_interpolate=[1, 2], n_jobs=8)\n",
    "                epochs, reject_log = ar.fit_transform(epochs, return_log=True)\n",
    "                print(f\"Rejected {sum(reject_log.bad_epochs)} bad epochs\")\n",
    "                reject_log.plot('horizontal')\n",
    "\n",
    "                # ---------- PAC channel-by-channel ----------\n",
    "                ep_array = epochs.get_data()  # (n_epochs, n_ch, n_times)\n",
    "                for i, channel_name in enumerate(channel_names):\n",
    "                    cleaned = ep_array[:, i, :]               # shape (n_epochs, n_times)\n",
    "                    phases  = p.filter(sampling_rate, cleaned, ftype=\"phase\",     n_jobs=parallel_processes)\n",
    "                    amps    = p.filter(sampling_rate, cleaned, ftype=\"amplitude\", n_jobs=parallel_processes)\n",
    "                    pac_map = p.fit(phases, amps, n_perm=surrogate_permutations,\n",
    "                                    n_jobs=parallel_processes)\n",
    "                    \n",
    "                    del phases, amps\n",
    "            \n",
    "                    pvals = p.infer_pvalues(p=0.05)\n",
    "                    comod = pac_map.mean(-1)\n",
    "                \n",
    "                    # Plot Comodulogram\n",
    "                    fig, ax = plt.subplots(figsize=(6,5))\n",
    "                    p.comodulogram(comod,\n",
    "                                   title=f\"{file} | {channel_name}\",\n",
    "                                   fz_labels=10, cmap='viridis', colorbar=True)\n",
    "                \n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    #start_min = int(start_s / 60)\n",
    "                    #end_min   = int(end_s   / 60)\n",
    "                    #base = f\"file_{file}_ch_{channel_name}_{start_min}-{end_min}min_{timestamp}\"\n",
    "                    base = f\"file_{file}_ch_{channel_name}_{timestamp}\"\n",
    "                    fig.savefig(os.path.join(output_dir, base + \".png\"), dpi=300)\n",
    "                    plt.close(fig)\n",
    "                \n",
    "                    idx    = np.unravel_index(np.argmax(comod, axis=None), comod.shape)\n",
    "                    pha_f  = p.f_pha[idx[0]]\n",
    "                    amp_f  = p.f_amp[idx[1]]\n",
    "                    val    = comod[idx]\n",
    "                    peak_p = pvals[idx]\n",
    "                \n",
    "                    # write summary without formatting errors\n",
    "                    with open(os.path.join(output_dir, base + \".txt\"), \"w\") as fh:\n",
    "                        fh.write(f\"channel:         {channel_name}\\n\")\n",
    "                        fh.write(f\"phase_freq (Hz): {pha_f}\\n\")\n",
    "                        fh.write(f\"amp_freq  (Hz):  {amp_f}\\n\")\n",
    "                        fh.write(f\"max_PAC_value:   {val}\\n\")\n",
    "                        fh.write(f\"p_value_peak:    {peak_p}\\n\")\n",
    "                        \n",
    "    except Exception as e:\n",
    "            logging.error(f\"Error in compute_PAC_of_rats: {e}\", exc_info=True)\n",
    "            show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b1247fd-5a64-4788-b1b4-56343354738d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def rat_probe_montage(ch_names):\n",
    "    \"\"\"\n",
    "    Sets very rough spatial locations for autoreject to compare\n",
    "    neighboring channels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        coords = {}\n",
    "        for idx, name in enumerate(ch_names):\n",
    "            # simplistic left/right & anterior/posterior mapping\n",
    "            side = -0.01 if \"L\" in name else 0.01\n",
    "            ant  =  0.01 if \"PFC\" in name else -0.01 if \"HPC\" in name else 0.0\n",
    "            vent = -0.01 if \"AMG\" or \"D\" in name else 0.01 if \"V\" in name else 0.0\n",
    "            coords[name] = np.array([side, ant, vent])\n",
    "        return mne.channels.make_dig_montage(coords)\n",
    "    except Exception as e:\n",
    "            logging.error(f\"Error in rat_probe_montage: {e}\", exc_info=True)\n",
    "            show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d45a6570-93de-40c6-b43e-44d4e35a21c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_raw_matrix(file_path, col_indices, start_sec=0, end_sec=-1):\n",
    "    \"\"\"\n",
    "    Read multiple columns from an NWB ElectricalSeries in one pass.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with NWBHDF5IO(file_path, \"r\") as io:\n",
    "            nwb = io.read()\n",
    "            elec_series = nwb.acquisition[\"ElectricalSeries\"]\n",
    "            sampling_rate = elec_series.rate  # in Hz\n",
    "            conversion = elec_series.conversion  # NCS scaling factor (bits to volts)\n",
    "    \n",
    "            # slice rows once, all cols at once\n",
    "            start_idx = int(start_sec * sampling_rate)\n",
    "            stop_idx  = int(elec_series.data.shape[0] if end_sec == -1 else end_sec * sampling_rate)\n",
    "    \n",
    "            # fancy-index the second axis → shape (n_samples, n_sel)\n",
    "            arr = elec_series.data[start_idx:stop_idx, col_indices]\n",
    "            data = np.asarray(arr).T * conversion       # -> (n_sel, n_samples)\n",
    "    \n",
    "        return data\n",
    "    except Exception as e:\n",
    "            logging.error(f\"Error in get_raw_matrix: {e}\", exc_info=True)\n",
    "            show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "968d5dfc-090d-4e53-a18c-c0ed05e3e1db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_PSD(Analysis_class):\n",
    "    \"\"\"\n",
    "    Compute and save power spectral density (PSD) plots for raw and filtered data\n",
    "    on a single channel. Includes optional notch filter, high-pass filter, and\n",
    "    1/f correction via log-log linear detrending.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load PSD parameters\n",
    "        (notch_freqs, \n",
    "         high_pass_filter, \n",
    "         correct_1overf, \n",
    "         sampling_rate, \n",
    "         output_directory,\n",
    "         fmin,\n",
    "         fmax,\n",
    "         voltage_scale, \n",
    "         PSD_FFT_resolution, \n",
    "         PSD_plot_raw, \n",
    "         PSD_plot_filtered, \n",
    "         PSD_plot_grouping_method) = Analysis_class.get_all_PSD_parameters()\n",
    "\n",
    "        # Load output parameters\n",
    "        (output_folder_name, \n",
    "         export_PNG, \n",
    "         export_PDF, \n",
    "         export_SVG, \n",
    "         export_EPS, \n",
    "         image_height, \n",
    "         image_width, \n",
    "         image_DPI,\n",
    "         color_palette,\n",
    "         y_custom_axis,\n",
    "         yaxis_top,\n",
    "         yaxis_bottom,\n",
    "         alpha,\n",
    "         save_data) = Analysis_class.get_export_parameters()\n",
    "    \n",
    "        folder_path = Analysis_class.get_folder_path\n",
    "        nwb_list    = Analysis_class.get_nwb_list()\n",
    "        channels    = Analysis_class.get_selected_channels()\n",
    "        fig_dict = {}\n",
    "\n",
    "        parallel_processes = Analysis_class.get_parallel_processes()\n",
    "\n",
    "        # Band definitions \n",
    "        BANDS_HZ = {\n",
    "            \"Delta\"      : (1,   4),\n",
    "            \"Theta\"      : (4,  12),\n",
    "            \"Beta\"       : (12, 30),\n",
    "            \"Gamma\"      : (30, 80),\n",
    "            \"HighGamma\"  : (80, 200),\n",
    "        }\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        def summarise_bands(freqs_hz: np.ndarray, psd_db: np.ndarray):\n",
    "            \"\"\"\n",
    "            Return {band: {'peak_db': ..., 'avg_db': ...}, ...}\n",
    "            using inclusive lower-edge, exclusive upper-edge bins.\n",
    "            \"\"\"\n",
    "            band_summary = {}\n",
    "            for band, (low, high) in BANDS_HZ.items():\n",
    "                sel = (freqs_hz >= low) & (freqs_hz < high)\n",
    "                if not sel.any():          # band not covered by current f-range\n",
    "                    band_summary[band] = {\"peak_db\": None, \"avg_db\": None}\n",
    "                    continue\n",
    "        \n",
    "                band_vals = psd_db[sel]\n",
    "                band_summary[band] = {\n",
    "                    \"peak_db\": float(np.nanmax(band_vals)),\n",
    "                    \"avg_db\" : float(np.nanmean(band_vals)),\n",
    "                    # optional: the frequency at the peak\n",
    "                    # \"peak_Hz\": float(freqs_hz[sel][np.nanargmax(band_vals)]),\n",
    "                }\n",
    "            return band_summary\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        # Create probe color map\n",
    "        PROBES = ['RAMG', 'RPFC', 'RVHPC', 'RDHPC', 'LHPCSCREW', 'RHPCSCREW', 'LPFCSCREW', 'RPFCSCREW']\n",
    "        DAYS = ['Day0', 'Day1', 'Day3', 'Day7', 'Day14', 'Day21', 'Day0control', 'PreDay0']\n",
    "        PROBE_COLOR_MAP = {probe: color_palette[i] for i, probe in enumerate(PROBES)}\n",
    "        DAY_COLOR_MAP = {probe: color_palette[i] for i, probe in enumerate(DAYS)}\n",
    "        \n",
    "        # Create Output Folder\n",
    "        timestamp = datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "        if \"[Timestamp]\" in output_folder_name:\n",
    "            output_folder_name = output_folder_name.split(\"[Timestamp]\")[0]\n",
    "            output_folder_name = f\"{output_folder_name}{timestamp}\"\n",
    "    \n",
    "        output_folder_name = output_folder_name.replace(\"PAC\",\"PSD\")\n",
    "        output_directory = os.path.join(output_directory, output_folder_name)\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        json_path = os.path.join(output_directory, \"PSD Session Summary.json\")\n",
    "        json_data_path = os.path.join(output_directory, \"PSD Session Data.json\")\n",
    "\n",
    "        # Add Session-level information to JSON output file\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, \"r\") as f:\n",
    "                psd_metadata = json.load(f)\n",
    "        else:\n",
    "            psd_metadata = {}\n",
    "\n",
    "        metadata = \"Metadata\"\n",
    "        psd_metadata[metadata] = {\n",
    "            \"timestamp\" : timestamp,\n",
    "            \"notch_freqs\" : notch_freqs, \n",
    "            \"high_pass_filter\" : high_pass_filter, \n",
    "            \"correct_1overf\" : correct_1overf, \n",
    "            \"sampling_rate\" : sampling_rate, \n",
    "            \"output_directory\" : output_directory,\n",
    "            \"fmin\" : fmin,\n",
    "            \"fmax\" : fmax,\n",
    "            \"voltage_scale\" : voltage_scale, \n",
    "            \"PSD_FFT_resolution\" : PSD_FFT_resolution, \n",
    "            \"PSD_plot_raw\" : PSD_plot_raw, \n",
    "            \"PSD_plot_filtered\" : PSD_plot_filtered, \n",
    "            \"PSD_plot_grouping_method\" : PSD_plot_grouping_method,\n",
    "            \"output_folder_name\" : output_folder_name, \n",
    "            \"export_PNG\" : export_PNG, \n",
    "            \"export_PDF\" : export_PDF, \n",
    "            \"export_SVG\" : export_SVG, \n",
    "            \"export_EPS\" : export_EPS, \n",
    "            \"image_height\" : image_height, \n",
    "            \"image_width\" : image_width, \n",
    "            \"image_DPI\" : image_DPI,\n",
    "            \"color_palette\" : color_palette,\n",
    "            \"y_custom_axis\" : y_custom_axis,\n",
    "            \"yaxis_top\" : yaxis_top,\n",
    "            \"yaxis_bottom\" : yaxis_bottom,\n",
    "            \"alpha\" : alpha,\n",
    "            \"folder_path\" : folder_path,\n",
    "            \"nwb_list\" : nwb_list,\n",
    "            \"channels\" : channels,   \n",
    "        }\n",
    "\n",
    "        # Rat-specific data in metadata JSON file\n",
    "        psd_metadata[\"Data\"] = {}\n",
    "        \n",
    "        # Write back to JSON\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(psd_metadata, f, indent=4)\n",
    "\n",
    "        # Separate JSON file for large data files\n",
    "        if save_data:\n",
    "            # Add Session-level information to JSON output file\n",
    "            if os.path.exists(json_data_path):\n",
    "                with open(json_data_path, \"r\") as f:\n",
    "                    psd_data = json.load(f)\n",
    "            else:\n",
    "                psd_data = {}\n",
    "            \n",
    "            # Dict to store computed results\n",
    "            psd_data[\"Data\"] = {}\n",
    "            \n",
    "            # Write back to JSON\n",
    "            with open(json_data_path, \"w\") as f:\n",
    "                json.dump(psd_data, f, indent=4)\n",
    "\n",
    "        # Loop through every selected NWB file\n",
    "        for file in nwb_list:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            selected  = channels.get(file, {})\n",
    "            if not selected:\n",
    "                continue\n",
    "\n",
    "            # Loop through every channel selected in current NWB file\n",
    "            for channel_name, data_index in selected.items():\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "                ########## Setup ##########\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "                \n",
    "                # Obtain current rat and probe from channel\n",
    "                channel_num = int(channel_name.split('CSC')[1])\n",
    "                current_rat, current_probe = Analysis.get_rat_and_probe_from_channel(file, channel_name)\n",
    "\n",
    "                # Determine day from file name\n",
    "                file_day = \"Day N/A\"\n",
    "                split_name = file.split('_')\n",
    "                for name_piece in split_name:\n",
    "                    if \"Day\" in name_piece:\n",
    "                        file_day = name_piece\n",
    "                        file_day_num = ''.join(filter(str.isdigit, name_piece))\n",
    "\n",
    "                # Grab the session (i.e. Day0_0001.nwb -> 0001)\n",
    "                if split_name[-1]:\n",
    "                    file_day_session = split_name[-1].split('.nwb')[0]\n",
    "\n",
    "                # Create dictionary entiries of the user wishes to group channels on plots\n",
    "                if PSD_plot_grouping_method == \"Group by Rat\":\n",
    "\n",
    "                    # Check if this rat already has a dict\n",
    "                    if current_rat not in fig_dict:\n",
    "                        fig_dict[current_rat] = {}\n",
    "\n",
    "                    # Check if this rat already has this day\n",
    "                    if file_day not in fig_dict[current_rat]:\n",
    "                        fig_dict[current_rat][file_day] = {}\n",
    "\n",
    "                    # Check if this rat already has this session\n",
    "                    if file_day_session not in fig_dict[current_rat][file_day]:\n",
    "                        fig, ax = plt.subplots(figsize=(image_width, image_height))\n",
    "                        fig_dict[current_rat][file_day][file_day_session] = (fig, ax)\n",
    "                        \n",
    "                elif PSD_plot_grouping_method == \"Group by Channel\":\n",
    "                    # Check if this channel already has a figure\n",
    "                    if current_probe not in fig_dict:\n",
    "                        fig, ax = plt.subplots(figsize=(image_width, image_height))\n",
    "                        fig_dict[current_probe] = (fig, ax)\n",
    "                        \n",
    "                # ---------------------------------------------------------------------------------------\n",
    "                ########## Filtering ##########\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "                        \n",
    "                # Load raw data and timestamps\n",
    "                times, data = get_raw_data(file_path, data_index, start=0, end=-1)\n",
    "                data = data * voltage_scale\n",
    "                raw_data = data.copy()\n",
    "                fs = 1.0 / np.median(np.diff(times))\n",
    "                #del times\n",
    "\n",
    "                # Wrap in MNE object\n",
    "                info = mne.create_info([channel_name], fs, ch_types=\"eeg\")\n",
    "                raw = mne.io.RawArray(data[np.newaxis, :], info, verbose=False)\n",
    "\n",
    "                # Apply notch filters\n",
    "                if notch_freqs:\n",
    "                    raw.notch_filter(notch_freqs, picks=channel_name, method='fir', verbose=False)\n",
    "        \n",
    "                # Apply high-pass filter\n",
    "                if high_pass_filter and high_pass_filter > 0.0:\n",
    "                    raw.filter(l_freq=high_pass_filter, h_freq=200, picks=channel_name, verbose=False)\n",
    "\n",
    "                # Pull data out of MNE object\n",
    "                data_filt = raw.get_data(picks=channel_name)[0]\n",
    "            \n",
    "                # Determine number of FFT values to compute. \n",
    "                # Higher resolution = more plotting points, MAX nperseg = # data samples in data / sampling rate\n",
    "                nperseg = int(fs * PSD_FFT_resolution)\n",
    "\n",
    "                # Compute PSD using MNE welch method\n",
    "                psd_raw, freqs_raw = psd_array_welch(raw_data[np.newaxis, :], \n",
    "                                                     sfreq=fs, \n",
    "                                                     n_fft=nperseg, \n",
    "                                                     fmin=fmin, \n",
    "                                                     fmax=fmax, \n",
    "                                                     n_jobs=parallel_processes, \n",
    "                                                     verbose=False)\n",
    "                \n",
    "                psd_filt, freqs_filt = psd_array_welch(data_filt[np.newaxis, :], \n",
    "                                                       sfreq=fs, \n",
    "                                                       n_fft=nperseg, \n",
    "                                                       fmin=fmin, \n",
    "                                                       fmax=fmax, \n",
    "                                                       n_jobs=parallel_processes, \n",
    "                                                       verbose=False)\n",
    "            \n",
    "                # Linear PSD value (unused, dB conversion interfaces better with matplotlib)\n",
    "                #psd_raw_linear = psd_raw[0]\n",
    "                #psd_filt_linear = psd_filt[0]\n",
    "                \n",
    "                # Convert to dB\n",
    "                psd_raw_db = 10 * np.log10(np.where(psd_raw[0] > 0, psd_raw[0], np.nan))\n",
    "                psd_filt_db = 10 * np.log10(np.where(psd_filt[0] > 0, psd_filt[0], np.nan))\n",
    "\n",
    "                # Apply 1/f correction [Applies to FILTERED data]\n",
    "                if correct_1overf:\n",
    "                    log_freqs = np.log10(freqs_filt[1:])\n",
    "                    log_psd = np.log10(np.where(psd_filt[0][1:] > 0, psd_filt[0][1:], np.nan))\n",
    "                    # Remove NaNs before fitting\n",
    "                    valid_idx = np.isfinite(log_freqs) & np.isfinite(log_psd)\n",
    "                    slope, intercept, *_ = linregress(log_freqs, log_psd)\n",
    "                    trend = slope * log_freqs + intercept\n",
    "                    psd_filt_corrected = log_psd - trend\n",
    "                    freqs_final = freqs_filt[1:]\n",
    "\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "                ########## Plotting ##########\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "\n",
    "                # Access current plot based on grouping method\n",
    "                if PSD_plot_grouping_method == \"Group by Rat\":\n",
    "                    fig, ax = fig_dict[current_rat][file_day][file_day_session]\n",
    "                elif PSD_plot_grouping_method == \"Group by Channel\":\n",
    "                    fig, ax = fig_dict[current_probe]\n",
    "                elif PSD_plot_grouping_method == \"No Grouping\":\n",
    "                    fig, ax = plt.subplots(figsize=(image_width, image_height))\n",
    "\n",
    "                # Choose Label based on user selection\n",
    "                if PSD_plot_raw and PSD_plot_filtered:\n",
    "                    raw_label = f\" Raw\"\n",
    "                    filter_label = f\" Filt\"\n",
    "                    oneoverf_label = f\" (1/f)\"\n",
    "                else:\n",
    "                    raw_label = f\"\"\n",
    "                    filter_label = f\"\"\n",
    "                    oneoverf_label = f\"\"\n",
    "\n",
    "                # Set correct color and label based on grouping method\n",
    "                if PSD_plot_grouping_method == \"Group by Channel\":\n",
    "                    selection_criteria = f\"R{current_rat} \" + file_day.replace('Day', 'D')\n",
    "                    # Select color map based on day/probe, black as backup\n",
    "                    color = DAY_COLOR_MAP.get(file_day, 'black')\n",
    "                else:\n",
    "                    selection_criteria = current_probe\n",
    "                    # Select color map based on day/probe, black as backup\n",
    "                    color = PROBE_COLOR_MAP.get(current_probe, 'black')\n",
    "\n",
    "                # Plot data according to user selection\n",
    "                if PSD_plot_raw:\n",
    "                    ax.plot(freqs_raw, psd_raw_db, label=f\"{selection_criteria}{raw_label}\", color=color, alpha=alpha)\n",
    "\n",
    "                if PSD_plot_filtered:\n",
    "                    ax.plot(freqs_filt, psd_filt_db, label=f\"{selection_criteria}{filter_label}\", color=color, alpha=alpha)\n",
    "\n",
    "                if correct_1overf:\n",
    "                    ax.plot(freqs_final, psd_filt_corrected, label=f\"{selection_criteria}{oneoverf_label}\", color=color, alpha=alpha)\n",
    "\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "                ########## Statistic Calculating ##########\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "\n",
    "                band_stats_raw  = summarise_bands(freqs_raw,  psd_raw_db)\n",
    "                band_stats_filt = summarise_bands(freqs_filt, psd_filt_db)\n",
    "                \n",
    "                # 1. Broadband power (dB mean over 1–200 Hz)\n",
    "                bb_mask = (freqs_raw >= 1) & (freqs_raw < 200)\n",
    "                broadband_raw  = float(np.nanmean(psd_raw_db[bb_mask]))\n",
    "                broadband_filt = float(np.nanmean(psd_filt_db[bb_mask]))\n",
    "                \n",
    "                # 2. Theta-Delta Ratio\n",
    "                tdr_raw  = band_stats_raw['Theta']['avg_db'] - band_stats_raw['Delta']['avg_db']   # dB diff = log-ratio\n",
    "                tdr_filt = band_stats_filt['Theta']['avg_db'] - band_stats_filt['Delta']['avg_db']\n",
    "                \n",
    "                # 3. Aperiodic exponent & offset (fit on log10(power) vs log10(freq))\n",
    "                fit_mask = (freqs_raw > 0) & (freqs_raw >= 2) & (freqs_raw < 40)\n",
    "                \n",
    "                x = np.log10(freqs_raw[fit_mask])\n",
    "                y = psd_raw_db[fit_mask] / 10.0      # convert dB → log10(power)\n",
    "                \n",
    "                valid = np.isfinite(x) & np.isfinite(y)\n",
    "                slope, offset_log10 = np.polyfit(x[valid], y[valid], 1)\n",
    "                \n",
    "                aperiodic_exp = float(-slope)        # unitless k\n",
    "                aperiodic_off = float(10 * offset_log10)  # dB µV²/Hz\n",
    "\n",
    "                biomarkers = {\n",
    "                    \"Broadband_dB\": {\"Raw\": broadband_raw, \"Filtered\": broadband_filt},\n",
    "                    \"ThetaDelta_dB\": {\"Raw\": tdr_raw, \"Filtered\": tdr_filt},\n",
    "                    \"Aperiodic\": {\"Exponent\": aperiodic_exp, \"Offset\": aperiodic_off},\n",
    "                }\n",
    "\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "                ########## JSON Exporting ##########\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "\n",
    "                # Access JSON file to store info\n",
    "                if os.path.exists(json_path):\n",
    "                    with open(json_path, \"r\") as f:\n",
    "                        psd_metadata = json.load(f)\n",
    "                else:\n",
    "                    psd_metadata = {}\n",
    "\n",
    "                # Access JSON file to store data\n",
    "                if os.path.exists(json_data_path):\n",
    "                    with open(json_data_path, \"r\") as f:\n",
    "                        psd_data = json.load(f)\n",
    "                else:\n",
    "                    psd_data = {}\n",
    "\n",
    "                # Save plot info to JSON file\n",
    "                data_key = f\"{current_rat}_{file_day}_{file_day_session}_{current_probe}\"\n",
    "                if data_key not in psd_metadata[\"Data\"]:\n",
    "                    # Save rat metadata\n",
    "                    psd_metadata[\"Data\"][data_key] = {}\n",
    "                    psd_metadata[\"Data\"][data_key][\"Info\"] = {\n",
    "                        \"Rat\" : current_rat,\n",
    "                        \"Day\" : file_day,\n",
    "                        \"Session\" : file_day_session,\n",
    "                        \"Probe\" : current_probe\n",
    "                    }\n",
    "\n",
    "                    # Save band-specific info\n",
    "                    band_dict = {}\n",
    "                    if PSD_plot_raw:\n",
    "                        band_dict[\"Raw\"] = band_stats_raw\n",
    "                    if PSD_plot_filtered:\n",
    "                        band_dict[\"Filtered\"] = band_stats_filt\n",
    "                    psd_metadata[\"Data\"][data_key][\"BandSummary\"] = band_dict\n",
    "\n",
    "                    # Save general biomarkers\n",
    "                    psd_metadata[\"Data\"][data_key][\"BiomarkerSummary\"] = biomarkers\n",
    "\n",
    "                    # Write back to JSON\n",
    "                    with open(json_path, \"w\") as f:\n",
    "                        json.dump(psd_metadata, f, indent=4)\n",
    "\n",
    "                \n",
    "                # Save PSD data\n",
    "                if save_data == True and data_key not in psd_data[\"Data\"]:\n",
    "                    psd_data[\"Data\"][data_key] = {}\n",
    "                    metadata_dict = {\n",
    "                        \"Rat\" : current_rat,\n",
    "                        \"Day\" : file_day,\n",
    "                        \"Session\" : file_day_session,\n",
    "                        \"Probe\" : current_probe\n",
    "                    }\n",
    "\n",
    "                    psd_dict = {}\n",
    "                    if PSD_plot_raw:\n",
    "                        psd_dict |= {\"Raw_x\": freqs_raw.tolist(), \"Raw_y\": psd_raw_db.tolist()}\n",
    "                    if PSD_plot_filtered:\n",
    "                        psd_dict |= {\"Filt_x\": freqs_filt.tolist(), \"Filt_y\": psd_filt_db.tolist()}\n",
    "                    if correct_1overf:\n",
    "                        psd_dict |= {\"Norm_x\": freqs_final.tolist(), \"Norm_y\": psd_filt_corrected.tolist()}\n",
    "                    psd_data[\"Data\"][data_key] = {\"Info\" : metadata_dict, \"Data\" : psd_dict}\n",
    "\n",
    "                    # Write back to JSON\n",
    "                    with open(json_data_path, \"w\") as f:\n",
    "                        json.dump(psd_data, f, indent=4)\n",
    "\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "                ########## Figure Exporting ##########\n",
    "                # ---------------------------------------------------------------------------------------\n",
    "                                    \n",
    "                # Immediately configure and save figure if there is no grouping\n",
    "                if PSD_plot_grouping_method == \"No Grouping\":\n",
    "                    title = ax.set_title(f\"PSD | Rat{current_rat} | {file_day} | {file_day_session} | {current_probe}\")\n",
    "                    title.set_fontproperties(termes_font_bold)\n",
    "                    title.set_fontsize(24)\n",
    "                    xl = ax.set_xlabel(\"Frequency (Hz)\")\n",
    "                    xl.set_fontproperties(termes_font_bold)\n",
    "                    xl.set_fontsize(18)\n",
    "                    yl = ax.set_ylabel(\"Power Spectral Density (dB µ$V^2$/Hz)\")\n",
    "                    yl.set_fontproperties(termes_font_bold)\n",
    "                    yl.set_fontsize(18)\n",
    "                    if y_custom_axis:\n",
    "                        ax.set_ylim(top=yaxis_top, bottom=yaxis_bottom)\n",
    "                    legend = ax.legend()\n",
    "                    for text in legend.get_texts():\n",
    "                        text.set_fontproperties(termes_font)\n",
    "                        text.set_fontsize(14)\n",
    "                    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "                        label.set_fontproperties(termes_font)\n",
    "                        label.set_fontsize(14)\n",
    "                    ax.grid(True)\n",
    "                \n",
    "                    # Save figure\n",
    "                    timestamp = datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "                    base_name = f\"{channel_name}_PSD_{timestamp}\"\n",
    "    \n",
    "                    if export_PNG:\n",
    "                        fig_path = os.path.join(output_directory, base_name + \".png\")\n",
    "                        fig.tight_layout()\n",
    "                        fig.savefig(fig_path, dpi=image_DPI)\n",
    "                        \n",
    "                    if export_PDF:\n",
    "                        fig_path = os.path.join(output_directory, base_name + \".pdf\")\n",
    "                        fig.tight_layout()\n",
    "                        fig.savefig(fig_path)\n",
    "            \n",
    "                    if export_SVG:\n",
    "                        fig_path = os.path.join(output_directory, base_name + \".svg\")\n",
    "                        fig.tight_layout()\n",
    "                        fig.savefig(fig_path)\n",
    "            \n",
    "                    if export_EPS:\n",
    "                        fig_path = os.path.join(output_directory, base_name + \".eps\")\n",
    "                        fig.tight_layout()\n",
    "                        fig.savefig(fig_path)\n",
    "            \n",
    "                    plt.close(fig)\n",
    "                \n",
    "                #############################################################################################################\n",
    "\n",
    "        # Process dictonaries outside for-loop, makes applying sorting method easier\n",
    "        if PSD_plot_grouping_method == \"Group by Rat\":\n",
    "            for rat_id, day_dict in fig_dict.items():\n",
    "                for file_day, day_session_dict in day_dict.items():\n",
    "                    for file_day_session, (fig, ax) in day_session_dict.items():\n",
    "                        extension = \"\"\n",
    "                        if file_day_session == \"0000\":\n",
    "                            extension = \" | Control Case\"\n",
    "                        elif file_day_session == \"0001\":\n",
    "                            extension = \" | DFP Injection\"\n",
    "                        elif file_day_session == \"0002\":\n",
    "                            extension = \" | MDZ Response\"\n",
    "\n",
    "                        title = ax.set_title(f\"PSD | Rat{rat_id} | {file_day}{extension}\")\n",
    "                        title.set_fontproperties(termes_font_bold)\n",
    "                        title.set_fontsize(24)\n",
    "                        xl = ax.set_xlabel(\"Frequency (Hz)\")\n",
    "                        xl.set_fontproperties(termes_font_bold)\n",
    "                        xl.set_fontsize(18)\n",
    "                        yl = ax.set_ylabel(\"Power Spectral Density (dB µV²/Hz)\")\n",
    "                        yl.set_fontproperties(termes_font_bold)\n",
    "                        yl.set_fontsize(18)\n",
    "                        if y_custom_axis:\n",
    "                            ax.set_ylim(top=yaxis_top, bottom=yaxis_bottom)\n",
    "                        legend = ax.legend()\n",
    "                        for text in legend.get_texts():\n",
    "                            text.set_fontproperties(termes_font)\n",
    "                            text.set_fontsize(14)\n",
    "                        for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "                            label.set_fontproperties(termes_font)\n",
    "                            label.set_fontsize(14)\n",
    "                        ax.grid(True)\n",
    "        \n",
    "                        # Save figure\n",
    "                        timestamp = datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "                        base_name = f\"{rat_id}_{file_day}_{file_day_session}_PSD_{timestamp}\"\n",
    "        \n",
    "                        if export_PNG:\n",
    "                            fig_path = os.path.join(output_directory, base_name + \".png\")\n",
    "                            fig.tight_layout()\n",
    "                            fig.savefig(fig_path, dpi=image_DPI)\n",
    "                            \n",
    "                        if export_PDF:\n",
    "                            fig_path = os.path.join(output_directory, base_name + \".pdf\")\n",
    "                            fig.tight_layout()\n",
    "                            fig.savefig(fig_path)\n",
    "                \n",
    "                        if export_SVG:\n",
    "                            fig_path = os.path.join(output_directory, base_name + \".svg\")\n",
    "                            fig.tight_layout()\n",
    "                            fig.savefig(fig_path)\n",
    "                \n",
    "                        if export_EPS:\n",
    "                            fig_path = os.path.join(output_directory, base_name + \".eps\")\n",
    "                            fig.tight_layout()\n",
    "                            fig.savefig(fig_path)\n",
    "                \n",
    "                        plt.close(fig)\n",
    "\n",
    "        if PSD_plot_grouping_method == \"Group by Channel\":                        \n",
    "            for current_probe, (fig, ax) in fig_dict.items():   \n",
    "                title = ax.set_title(f\"PSD | {current_probe}\")\n",
    "                title.set_fontproperties(termes_font_bold)\n",
    "                title.set_fontsize(24)\n",
    "                xl = ax.set_xlabel(\"Frequency (Hz)\")\n",
    "                xl.set_fontproperties(termes_font_bold)\n",
    "                xl.set_fontsize(18)\n",
    "                yl = ax.set_ylabel(\"1/f Corrected Power Spectral Density (dB µV²)\").set_fontproperties(termes_font_bold)\n",
    "                yl.set_fontproperties(termes_font_bold)\n",
    "                yl.set_fontsize(18)\n",
    "                if y_custom_axis:\n",
    "                    ax.set_ylim(top=yaxis_top, bottom=yaxis_bottom)\n",
    "                legend = ax.legend() \n",
    "                for text in legend.get_texts():\n",
    "                    text.set_fontproperties(termes_font)\n",
    "                    text.set_fontsize(14)\n",
    "                for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "                    label.set_fontproperties(termes_font)\n",
    "                    label.set_fontsize(14)\n",
    "                ax.grid(True)\n",
    "\n",
    "                # Save figure\n",
    "                timestamp = datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "                base_name = f\"{current_probe}_PSD_{timestamp}\"\n",
    "\n",
    "                if export_PNG:\n",
    "                    fig_path = os.path.join(output_directory, base_name + \".png\")\n",
    "                    fig.tight_layout()\n",
    "                    fig.savefig(fig_path, dpi=image_DPI)\n",
    "                    \n",
    "                if export_PDF:\n",
    "                    fig_path = os.path.join(output_directory, base_name + \".pdf\")\n",
    "                    fig.tight_layout()\n",
    "                    fig.savefig(fig_path)\n",
    "        \n",
    "                if export_SVG:\n",
    "                    fig_path = os.path.join(output_directory, base_name + \".svg\")\n",
    "                    fig.tight_layout()\n",
    "                    fig.savefig(fig_path)\n",
    "        \n",
    "                if export_EPS:\n",
    "                    fig_path = os.path.join(output_directory, base_name + \".eps\")\n",
    "                    fig.tight_layout()\n",
    "                    fig.savefig(fig_path)\n",
    "        \n",
    "                plt.close(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in compute_PSD: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ca19fb1-3cd4-44ec-b636-789d8f8143c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_PSD_depreciated(Analysis_class):\n",
    "    \"\"\"\n",
    "    Compute and save power spectral density (PSD) plots for raw and filtered data\n",
    "    for each selected NWB file and channel. The raw PSD will be plotted alongside\n",
    "    a series of notch filters (60, 120, 180, 240 Hz) and a 2 Hz high-pass filtered PSD\n",
    "    to illustrate noise removal.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    from scipy.signal import welch, iirnotch, butter, filtfilt\n",
    "\n",
    "    try:\n",
    "        # Prepare I/O paths\n",
    "        output_dir = r\"C:\\Users\\holot\\Desktop\\Summer Research\\Research GUI V2\\PAC Comodulogram\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        folder_path = Analysis_class.get_folder_path\n",
    "        nwb_list    = Analysis_class.get_nwb_list()\n",
    "        channels    = Analysis_class.get_selected_channels()\n",
    "\n",
    "        notch_freqs = [60.0, 120.0, 180.0, 240.0]\n",
    "\n",
    "        for file in nwb_list:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            selected  = channels.get(file, {})\n",
    "            if not selected:\n",
    "                print(f\"No channels selected for {file}. Skipping PSD computation.\")\n",
    "                continue\n",
    "\n",
    "            for channel_name, data_index in selected.items():\n",
    "                print(f\"Computing PSD for {file} | channel {channel_name}\")\n",
    "\n",
    "                # Load raw data and timestamps\n",
    "                times, data = get_raw_data(file_path, data_index, start=0, end=-1)\n",
    "                fs = 1.0 / np.median(np.diff(times))\n",
    "                del times\n",
    "\n",
    "                # Compute raw PSD\n",
    "                nperseg = min(len(data), int(4 * fs))\n",
    "                f_raw, Pxx_raw = welch(data, fs=fs, nperseg=nperseg)\n",
    "\n",
    "                # Apply cascaded notch filters at specified harmonics\n",
    "                data_notch = data.copy()\n",
    "                for f0 in notch_freqs:\n",
    "                    b_notch, a_notch = iirnotch(f0, Q=30.0, fs=fs)\n",
    "                    data_notch = filtfilt(b_notch, a_notch, data_notch)\n",
    "\n",
    "                # Apply 2 Hz high-pass filter to remove slow drift\n",
    "                b_hp, a_hp = butter(4, 2.0 / (fs / 2.0), btype='high')\n",
    "                data_filt = filtfilt(b_hp, a_hp, data_notch)\n",
    "\n",
    "                # Compute filtered PSD\n",
    "                f_filt, Pxx_filt = welch(data_filt, fs=fs, nperseg=nperseg)\n",
    "\n",
    "                # Plot and save both PSDs\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.semilogy(f_raw,  Pxx_raw,  label='Raw')\n",
    "                plt.semilogy(f_filt, Pxx_filt, label='Notches+HPF')\n",
    "                plt.xlabel('Frequency (Hz)')\n",
    "                plt.ylabel('Power Spectral Density')\n",
    "                plt.title(f\"PSD: {file} | Channel {channel_name}\")\n",
    "                plt.legend()\n",
    "\n",
    "                # Build filename and save\n",
    "                timestamp = datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "                base      = f\"{file}_ch_{channel_name}_PSD_{timestamp}\"\n",
    "                png_path  = os.path.join(output_dir, base + \".png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(png_path, dpi=300)\n",
    "                plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        import logging\n",
    "        logging.error(f\"Error in compute_PSD: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e36a42b-1a42-44b7-9628-53ae5c368e77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_analysis_window(parent, Analysis_class):\n",
    "    \"\"\"\n",
    "    Create an analysis window with all options for file selection, preprocessing, and results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set a default output directory for the analysis\n",
    "        base_dir = get_default_output_directory()\n",
    "        output_dir = os.path.join(base_dir, \"PAC Output\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        Analysis_class.set_output_directory(output_dir)\n",
    "        \n",
    "        with dpg.child_window(tag=\"ana_buttons_window\",\n",
    "                              border=True, \n",
    "                              autosize_x=True,\n",
    "                              height=35,\n",
    "                              parent=\"ana_child_window\"):\n",
    "            with dpg.group(horizontal=True, parent=\"ana_buttons\", tag=\"ana_buttons_group\"):\n",
    "                dpg.add_text(\"PAC Options:\")\n",
    "                                \n",
    "        nwb_files = Analysis_class.get_file_list\n",
    "        dpg.add_button(label=\"Compute PAC\",\n",
    "                       parent=\"ana_buttons_group\",\n",
    "                       callback=PAC_callback,\n",
    "                       user_data=Analysis_class)\n",
    "        dpg.add_button(label=\"Compute Synthetic PAC\",\n",
    "                       parent=\"ana_buttons_group\",\n",
    "                       callback=lambda:compute_PAC_of_channels_SYNTHETIC(Analysis_class))\n",
    "        dpg.add_button(label=\"Crunch PAC JSON Data\",\n",
    "                       parent=\"ana_buttons_group\",\n",
    "                       callback=crunch_PAC_JSON_data_callback,\n",
    "                       user_data=Analysis_class)\n",
    "\n",
    "        dpg.add_text(\"  PSD Options:\", parent=\"ana_buttons_group\")\n",
    "        dpg.add_button(label=\"Compute PSD\",\n",
    "                       parent=\"ana_buttons_group\",\n",
    "                       callback=lambda:compute_PSD(Analysis_class))\n",
    "        dpg.add_button(label=\"Crunch PSD JSON Data\",\n",
    "                       parent=\"ana_buttons_group\",\n",
    "                       callback=crunch_PSD_JSON_data_callback,\n",
    "                       user_data=Analysis_class)\n",
    "\n",
    "        dpg.add_text(\"  Extra:\", parent=\"ana_buttons_group\")\n",
    "        dpg.add_button(label=\"Open Output Location\",\n",
    "                       parent=\"ana_buttons_group\",\n",
    "                       callback=open_output_location_callback,\n",
    "                       user_data=Analysis_class)\n",
    "        \n",
    "        with dpg.child_window(parent=parent,\n",
    "                              tag=f\"ana{Analysis_class.ID}_window\",\n",
    "                              autosize_y=True,\n",
    "                              autosize_x=True,\n",
    "                              border=True,\n",
    "                              horizontal_scrollbar=True):\n",
    "            # NWB File Subwindow\n",
    "            with dpg.group(tag=f\"ana{Analysis_class.ID}_window_group\",\n",
    "                           parent=f\"ana{Analysis_class.ID}_window\",\n",
    "                           horizontal=True):\n",
    "                # NWB File Selected Column\n",
    "                with dpg.child_window(tag=f\"ana{Analysis_class.ID}_NWB_window\", \n",
    "                                      border=True, \n",
    "                                      width=275, \n",
    "                                      autosize_y=True, \n",
    "                                      horizontal_scrollbar=True, \n",
    "                                      no_scrollbar=False, \n",
    "                                      menubar=True, \n",
    "                                      parent=f\"ana{Analysis_class.ID}_window_group\"):\n",
    "                    with dpg.menu_bar():\n",
    "                            dpg.add_menu(label=\"NWB Files\")\n",
    "\n",
    "                    # Build rat-to-files dictionary\n",
    "                    rat_to_files = {i: [] for i in range(1, 65)}  # Rats 1 through 64\n",
    "            \n",
    "                    for file in nwb_files:\n",
    "                        rats_in_file = NWBFolder.get_rats_in_file(file)\n",
    "                        for rat in rats_in_file:\n",
    "                            if 1 <= rat <= 64:\n",
    "                                rat_to_files[rat].append(file)\n",
    "            \n",
    "                    # Build UI elements grouped by rat\n",
    "                    for rat_num in range(1, 65):\n",
    "                        files = rat_to_files[rat_num]\n",
    "                        if not files:\n",
    "                            continue  # Skip rats with no files\n",
    "            \n",
    "                        with dpg.collapsing_header(label=f\"Rat {rat_num}\", \n",
    "                                                   tag=f\"ana{Analysis_class.ID}_{rat_num}_header\", \n",
    "                                                   default_open=False, \n",
    "                                                   parent=f\"ana{Analysis_class.ID}_NWB_window\"):\n",
    "                            for file in sorted(files):\n",
    "                                checkbox_tag = f\"{parent}_{rat_num}_{file}\"\n",
    "                                dpg.add_checkbox(\n",
    "                                    label=file,\n",
    "                                    tag=checkbox_tag,\n",
    "                                    callback=update_ana_nwb_list_callback,\n",
    "                                    user_data=(Analysis_class, file, f\"ana{Analysis_class.ID}_channel_window\")\n",
    "                                )\n",
    "\n",
    "                    \"\"\"\n",
    "                    for file in nwb_files:\n",
    "                        dpg.add_checkbox(label=file,\n",
    "                                      tag=f\"ana{Analysis_class.ID}_window_{file}\",\n",
    "                                      parent=f\"ana{Analysis_class.ID}_NWB_window\",\n",
    "                                      callback=update_ana_nwb_list_callback,\n",
    "                                      user_data=(Analysis_class, file, f\"ana{Analysis_class.ID}_channel_window\"))\n",
    "                    \"\"\"\n",
    "\n",
    "                # Channel Selection Column\n",
    "                with dpg.child_window(tag=f\"ana{Analysis_class.ID}_channel_window\", \n",
    "                                      border=True, \n",
    "                                      width=275, \n",
    "                                      autosize_y=True,\n",
    "                                      horizontal_scrollbar=True, \n",
    "                                      no_scrollbar=False, \n",
    "                                      menubar=True, \n",
    "                                      parent=f\"ana{Analysis_class.ID}_window_group\"):\n",
    "                    with dpg.menu_bar():\n",
    "                            dpg.add_menu(label=\"Channels\")              \n",
    "\n",
    "                # PAC Options Column\n",
    "                with dpg.child_window(tag=f\"ana{Analysis_class.ID}_parameter_window\", \n",
    "                                      border=True, \n",
    "                                      width=335, \n",
    "                                      autosize_y=True,\n",
    "                                      horizontal_scrollbar=True, \n",
    "                                      no_scrollbar=False, \n",
    "                                      menubar=True, \n",
    "                                      parent=f\"ana{Analysis_class.ID}_window_group\"):\n",
    "                    with dpg.menu_bar():\n",
    "                                      dpg.add_menu(label=\"Algorithm Parameters\")\n",
    "                    with dpg.tab_bar(tag=f\"ana{Analysis_class.ID}_parameter_tabs\", \n",
    "                                      parent=f\"ana{Analysis_class.ID}_parameter_window\"):\n",
    "                        with dpg.tab(label=\"PAC\", tag=\"analysis_PAC_tab\", \n",
    "                                      parent=f\"ana{Analysis_class.ID}_parameter_tabs\"):\n",
    "        \n",
    "                            dpg.add_text(\"Low Frequency Parameters\")\n",
    "        \n",
    "                            dpg.add_combo(label=\"Resolution Override\",\n",
    "                                              tag=\"PAC_options_low_override\",\n",
    "                                              items=['lres', 'mres',\n",
    "                                                     'hres', 'No Bins', 'Custom'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_lowfreq_override(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Lowpass\",\n",
    "                                              tag=\"PAC_options_low_lowpass\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_lowfreq_lowpass(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Highpass\",\n",
    "                                              tag=\"PAC_options_low_highpass\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_lowfreq_highpass(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Width\",\n",
    "                                              tag=\"PAC_options_low_width\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_lowfreq_width(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Step\",\n",
    "                                              tag=\"PAC_options_low_step\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_lowfreq_step(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_separator()\n",
    "                            dpg.add_text(\"High Frequency Parameters\")\n",
    "        \n",
    "                            dpg.add_combo(label=\"Resolution Override\",\n",
    "                                              tag=\"PAC_options_high_override\",\n",
    "                                              items=['lres', 'mres',\n",
    "                                                     'hres','No Bins', 'Custom'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_highfreq_override(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Lowpass\",\n",
    "                                              tag=\"PAC_options_high_lowpass\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_highfreq_lowpass(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Highpass\",\n",
    "                                              tag=\"PAC_options_high_highpass\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_highfreq_highpass(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Width\",\n",
    "                                              tag=\"PAC_options_high_width\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_highfreq_width(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Step\",\n",
    "                                              tag=\"PAC_options_high_step\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_highfreq_step(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_separator()\n",
    "                            dpg.add_text(\"Filtering\")\n",
    "        \n",
    "                            dpg.add_input_float(label=\"High-Pass Filter\",\n",
    "                                              tag=\"PAC_options_high_pass_filter\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_high_pass_filter(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_combo(label=\"Filter Notch Freq.\",\n",
    "                                              tag=\"PAC_options_filter_notch_frequencies\",\n",
    "                                              items=['None',\n",
    "                                                     '[60]',\n",
    "                                                     '[60, 120]',\n",
    "                                                     '[60, 120, 180]',\n",
    "                                                     '[60, 120, 180, 240]',\n",
    "                                                     '[60, 120, 180, 240, 300]'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_filter_notch_frequencies(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_input_int(label=\"Rejection Threshold\",\n",
    "                                              tag=\"PAC_options_rejection_threshold\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_rejection_threshold(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_rejection_threshold(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_text(\"^ 1 = 1mV, Recc. ~3\")\n",
    "        \n",
    "                            dpg.add_checkbox(label=\"Detrend Epochs\", \n",
    "                                              tag=\"PAC_options_detrend_epochs\",\n",
    "                                              default_value=Analysis_class.get_detrend_epochs(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_checkbox(label=\"Apply Autofilter\", \n",
    "                                              tag=\"PAC_options_apply_autofilter\",\n",
    "                                              default_value=Analysis_class.get_apply_autofilter(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "                            dpg.add_text(\"(^ Must Select a Rat!)\")\n",
    "\n",
    "                            dpg.add_separator()\n",
    "                            dpg.add_text(\"Phase-Amplitdue Coupling Parameters\")\n",
    "                                \n",
    "                            dpg.add_combo(label=\"PAC Methods\",\n",
    "                                              tag=\"PAC_options_PAC_methods\",\n",
    "                                              items=['Mean Vector Length', 'Modulation Index',\n",
    "                                                     'Heights Ratio', 'ndPAC',\n",
    "                                                     'Phase-Locking Value', 'Gaussian Copula PAC'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PAC_method(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "                            \n",
    "                            dpg.add_combo(label=\"Surrogate Method\",\n",
    "                                              tag=\"PAC_options_surrogate_method\",\n",
    "                                              items=['No Surrogates',\n",
    "                                                     'Swap Phase/Ampl. Across Trials',\n",
    "                                                     'Swap Amplitude Time Blocks','Time Lag'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_surrogate_method(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "                            \n",
    "                            dpg.add_combo(label=\"Normalization Method\",\n",
    "                                              tag=\"PAC_options_normalization_method\",\n",
    "                                              items=['No Normalization','Subtract Mean of Surrogtes',\n",
    "                                                     'Divide Mean of Surrogates',\n",
    "                                                     'Sub+Div Mean of Surrogates','Z-score'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_normalization_method(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_normalization_method(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_separator()\n",
    "                            dpg.add_text(\"Situational\")\n",
    "\n",
    "                            dpg.add_checkbox(label=\"Skip PAC\", \n",
    "                                              tag=\"PAC_options_skip_PAC\",\n",
    "                                              default_value=Analysis_class.get_skip_PAC(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_skip_PAC(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_checkbox(label=\"Inject PAC\", \n",
    "                                              tag=\"PAC_options_inject_PAC\",\n",
    "                                              default_value=Analysis_class.get_inject_PAC(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_inject_PAC(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_combo(label=\"Dcomplex Method\",\n",
    "                                              tag=\"PAC_options_dcomplex_method\",\n",
    "                                              items=['Wavelet', 'Hilbert'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_dcomplex_method(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "                            \n",
    "                            dpg.add_input_int(label=\"Phase Cycles\",\n",
    "                                              tag=\"PAC_options_phase_cycles\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_phase_cycles(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "            \n",
    "                            dpg.add_input_int(label=\"Amplitude Cycles\",\n",
    "                                              tag=\"PAC_options_amplitude_cycles\",\n",
    "                                              width=150,\n",
    "                                              default_value=6,\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Morlets Wavelet Width\",\n",
    "                                              tag=\"PAC_options_morlet’s_wavelet_width\",\n",
    "                                              width=150,\n",
    "                                              default_value=7,\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"KLD or HRPAC Bins\",\n",
    "                                              tag=\"PAC_options_KLD_or_HRPAC_bins\",\n",
    "                                              width=150,\n",
    "                                              default_value=18,\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_separator()\n",
    "                            dpg.add_text(\"Performance\")\n",
    "                            dpg.add_text(\"* -1 = Maximum Possible\")\n",
    "\n",
    "                            dpg.add_input_int(label=\"Set Seed (0=None)\",\n",
    "                                              tag=\"Performance_options_seed\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_seed(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_seed(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Sampling Rate\",\n",
    "                                              tag=\"Performance_options_sampling_rate\",\n",
    "                                              width=150,\n",
    "                                              default_value=2000,\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Downsample Rate\",\n",
    "                                              tag=\"Performance_options_downsample_rate\",\n",
    "                                              width=150,\n",
    "                                              default_value=500,\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Epoch Length (sec)\",\n",
    "                                              tag=\"Performance_options_epoch_length\",\n",
    "                                              width=150,\n",
    "                                              default_value=5,\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Data Length (min)\",\n",
    "                                              tag=\"Performance_options_data_length\",\n",
    "                                              width=150,\n",
    "                                              default_value=10,\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Minimum Length\",\n",
    "                                              tag=\"Performance_options_minimum_length\",\n",
    "                                              width=150,\n",
    "                                              default_value=5,\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Parallel Processes*\",\n",
    "                                              tag=\"Performance_options_parallel_processes\",\n",
    "                                              width=150,\n",
    "                                              default_value=8,\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "        \n",
    "                            dpg.add_input_int(label=\"Surrogate Count\",\n",
    "                                              tag=\"Performance_options_surrogate_count\",\n",
    "                                              width=150,\n",
    "                                              default_value=200,\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "    \n",
    "    \n",
    "                        with dpg.tab(label=\"PSD\",\n",
    "                                      tag=\"analysis_PSD_tab\", \n",
    "                                      parent=f\"ana{Analysis_class.ID}_parameter_tabs\"):\n",
    "                            \n",
    "                            dpg.add_text(\"Parameters\")\n",
    "\n",
    "                            dpg.add_input_float(label=\"Min Frequency\",\n",
    "                                              tag=\"PSD_options_fmin\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PSD_fmin(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_input_float(label=\"Max Frequency\",\n",
    "                                              tag=\"PSD_options_fmax\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PSD_fmax(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_input_float(label=\"High-Pass Filter\",\n",
    "                                              tag=\"PSD_options_high_pass_filter\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PSD_high_pass_filter(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_combo(label=\"Filter Notch Freq.\",\n",
    "                                              tag=\"PSD_options_notch_frequencies\",\n",
    "                                              items=['None',\n",
    "                                                     '[60]',\n",
    "                                                     '[60, 120]',\n",
    "                                                     '[60, 120, 180]',\n",
    "                                                     '[60, 120, 180, 240]',\n",
    "                                                     '[60, 120, 180, 240, 300]'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PSD_notch_frequencies(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_combo(label=\"Voltage Scale\",\n",
    "                                              tag=\"PSD_options_voltage_scale\",\n",
    "                                              items=['Volts',\n",
    "                                                     'Millivolts',\n",
    "                                                     'Microvolts'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PSD_voltage_scale(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            \n",
    "                            dpg.add_input_int(label=\"FFT Resolution\",\n",
    "                                              tag=\"PSD_options_FFT_resolution\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PSD_FFT_resolution(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_PSD_FFT_resolution(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_text(\"^ 256 Max/per 5 min of data\")\n",
    "                            dpg.add_separator()\n",
    "\n",
    "                            dpg.add_combo(label=\"Plot Grouping Method\",\n",
    "                                              tag=\"PSD_options_plot_grouping_method\",\n",
    "                                              items=['No Grouping',\n",
    "                                                     'Group by Rat',\n",
    "                                                     'Group by Channel'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PSD_plot_grouping_method(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_PSD_plot_grouping_method(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_checkbox(label=\"Plot Raw Data\", \n",
    "                                              tag=\"PSD_options_plot_raw\",\n",
    "                                              default_value=Analysis_class.get_PSD_plot_raw(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_PSD_plot_raw(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_checkbox(label=\"Plot Filtered Data\", \n",
    "                                              tag=\"PSD_options_plot_filtered\",\n",
    "                                              default_value=Analysis_class.get_PSD_plot_filtered(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_PSD_plot_filtered(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                            dpg.add_checkbox(label=\"Correct 1/f\", \n",
    "                                              tag=\"PSD_options_correct_1overf\",\n",
    "                                              default_value=Analysis_class.get_PSD_correct_1overf(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_PSD_correct_1overf(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "                            \n",
    "                # Performance & Export Options Column\n",
    "                with dpg.child_window(tag=f\"ana{Analysis_class.ID}_perform_window\", \n",
    "                                      border=True, \n",
    "                                      width=325, \n",
    "                                      autosize_y=True,\n",
    "                                      horizontal_scrollbar=True, \n",
    "                                      no_scrollbar=False, \n",
    "                                      menubar=True, \n",
    "                                      parent=f\"ana{Analysis_class.ID}_window_group\"):\n",
    "                    with dpg.menu_bar():\n",
    "                            dpg.add_menu(label=\"Export\")\n",
    "\n",
    "                    dpg.add_button(label=\"Set Output Location\",\n",
    "                                  callback=set_PAC_output_directory_callback,\n",
    "                                  user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_text(\"Output Folder Name (Editable)\")\n",
    "\n",
    "                    dpg.add_input_text(label=\"\",\n",
    "                                      tag=\"Export_options_output_folder_name\",\n",
    "                                      default_value=\"GUIDA PAC Session [Timestamp]\",\n",
    "                                      callback=PAC_options_callback,\n",
    "                                      user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_checkbox(label=\"Export Data to JSON\", \n",
    "                                      tag=\"Export_options_save_data\",\n",
    "                                      default_value=Analysis_class.get_save_data(),\n",
    "                                      callback=lambda sender, app_data, user_data: user_data.set_save_data(app_data),\n",
    "                                      user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_separator()\n",
    "                    dpg.add_text(\"Export Image As:\")\n",
    "\n",
    "                    dpg.add_checkbox(label=\"PNG\", \n",
    "                                      tag=\"Export_options_export_PNG\",\n",
    "                                      default_value=Analysis_class.get_export_PNG(),\n",
    "                                      callback=PAC_options_callback,\n",
    "                                      user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_checkbox(label=\"PDF\", \n",
    "                                      tag=\"Export_options_export_PDF\",\n",
    "                                      default_value=Analysis_class.get_export_PDF(),\n",
    "                                      callback=PAC_options_callback,\n",
    "                                      user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_checkbox(label=\"SVG\", \n",
    "                                      tag=\"Export_options_export_SVG\",\n",
    "                                      default_value=Analysis_class.get_export_SVG(),\n",
    "                                      callback=PAC_options_callback,\n",
    "                                      user_data=Analysis_class)\n",
    "                    \n",
    "                    dpg.add_checkbox(label=\"EPS\", \n",
    "                                      tag=\"Export_options_export_EPS\",\n",
    "                                      default_value=Analysis_class.get_export_EPS(),\n",
    "                                      callback=PAC_options_callback,\n",
    "                                      user_data=Analysis_class)\n",
    "                    \n",
    "                    dpg.add_separator()\n",
    "                    \n",
    "                    dpg.add_input_float(label=\"Image Width (In.)\",\n",
    "                                              tag=\"Export_options_image_width\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_image_width(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_input_float(label=\"Image Height (In.)\",\n",
    "                                              tag=\"Export_options_image_height\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_image_height(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "                    \n",
    "                    dpg.add_input_int(label=\"Image DPI\",\n",
    "                                              tag=\"Export_options_image_DPI\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_image_DPI(),\n",
    "                                              callback=PAC_options_callback,\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_text(\"Comodulogram Reccomendation\")\n",
    "                    dpg.add_text(\"Width = 6   Height = 5\")\n",
    "                    dpg.add_text(\"PSD Reccomendation\")\n",
    "                    dpg.add_text(\"Width = 10  Height = 5\")\n",
    "\n",
    "                    dpg.add_separator()\n",
    "                    dpg.add_text(\"PAC Options\")\n",
    "\n",
    "                    dpg.add_checkbox(label=\"Use Custom Colormap Scale\", \n",
    "                                              tag=\"PAC_export_options_custom_colormap\",\n",
    "                                              default_value=Analysis_class.get_PAC_custom_colormap(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_PAC_custom_colormap(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_input_float(label=\"Set Vmin\",\n",
    "                                              tag=\"PAC_export_options_vmin\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PAC_vmin(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_PAC_vmin(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_input_float(label=\"Set Vmax\",\n",
    "                                              tag=\"PAC_export_options_vmax\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PAC_vmax(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_PAC_vmax(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_input_float(label=\"Comod. Interp. *\",\n",
    "                                              tag=\"PAC_export_options_comod_interpolation\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_PAC_comod_interpolation(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_PAC_comod_interpolation(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_text(\"* Interpolates Only Image NOT Data\")\n",
    "                    dpg.add_text(\"0.1 = (Row x Columns) x 10\")\n",
    "                    dpg.add_text(\"0.5 = (Row x Columns) x 2\")\n",
    "\n",
    "                    dpg.add_separator()\n",
    "                    dpg.add_text(\"PSD Options\")\n",
    "                    \n",
    "                    dpg.add_checkbox(label=\"Use Custom Y-axis\", \n",
    "                                              tag=\"Export_options_y_custom_axis\",\n",
    "                                              default_value=Analysis_class.get_y_custom_axis(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_y_custom_axis(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_input_float(label=\"Y-axis Top (dB)\",\n",
    "                                              tag=\"Export_options_yaxis_top\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_yaxis_top(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_yaxis_top(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_input_float(label=\"Y-axis Bottom (dB)\",\n",
    "                                              tag=\"Export_options_yaxis_bottom\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_yaxis_bottom(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_yaxis_bottom(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_combo(label=\"Color Palette\",\n",
    "                                              tag=\"Export_options_color_palette\",\n",
    "                                              items=['Bright Colors',\n",
    "                                                     'Tableau Colors',\n",
    "                                                     'Colorblind Okabe Ito',\n",
    "                                                     'Colorblind Cud'],\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_color_palette(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_color_palette(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "                    dpg.add_input_float(label=\"Line Opacity (0-1)\",\n",
    "                                              tag=\"Export_options_alpha\",\n",
    "                                              width=150,\n",
    "                                              default_value=Analysis_class.get_alpha(),\n",
    "                                              callback=lambda sender, app_data, user_data: user_data.set_alpha(app_data),\n",
    "                                              user_data=Analysis_class)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in create_plot_window: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba654833-e722-44f6-96b1-8a6aa9eecacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Plotting Functions #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f15b168b-1030-4735-a939-fe57f860cebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(plot_instance):\n",
    "    \"\"\"\n",
    "    Function for plotting provided data in dearpygui's plot widget.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine data to access\n",
    "        electrode_mapping = plot_instance.get_electrode_mapping()\n",
    "        selected_channel = plot_instance.get_channel()\n",
    "        raw_data_column_index = electrode_mapping[selected_channel]\n",
    "        voltage_scaling_factor = plot_instance.get_voltage_scale()\n",
    "    \n",
    "        # Construct full file path from the selected NWB file\n",
    "        file_path = os.path.join(plot_instance.get_folder_path, plot_instance.nwb_file)\n",
    "        # Get the raw data\n",
    "        x, y = get_raw_data(file_path, raw_data_column_index, plot_instance.get_data_start(), plot_instance.get_data_end())\n",
    "\n",
    "        # Scale to be volts, millivolts, or microvolts\n",
    "        if voltage_scaling_factor != 1:\n",
    "            y = y * voltage_scaling_factor\n",
    "\n",
    "        # Apply bandpass filter if that plot type is selected\n",
    "        if plot_instance.get_plot_type() == \"Filter\":\n",
    "            with NWBHDF5IO(file_path, 'r') as io:\n",
    "                nwb = io.read()\n",
    "                elec_series = nwb.acquisition[\"ElectricalSeries\"]\n",
    "                sampling_rate = elec_series.rate  # in Hz\n",
    "\n",
    "            # Convert y to an acceptable data format for mne int16 -> float64\n",
    "            y = y.astype(np.float64)\n",
    "            y = filter_data(data=y, sfreq=sampling_rate, lowcut=plot_instance.get_lowcut(), highcut=plot_instance.get_highcut())\n",
    "\n",
    "        # Update the plot data\n",
    "        series_tag = f\"{plot_instance.ID}_plot_series\"\n",
    "        # For a line series in DearPyGui, we update its x and y data.\n",
    "        dpg.set_value(series_tag, [x.tolist(), y.tolist()])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in plot_data: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2fbdcb7-fc70-4ce0-9285-f74eca49961d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_channels_for_file(file_path, plot_instance):\n",
    "    \"\"\"\n",
    "    Given the full file path, return a list of available timeseries channels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with NWBHDF5IO(file_path, 'r') as io:\n",
    "            nwb = io.read()\n",
    "            \n",
    "            # Convert the electrodes table to a pandas DataFrame for easier inspection.\n",
    "            df_electrodes = nwb.electrodes.to_dataframe()\n",
    "            \n",
    "            # Extract the 'channel_name' column\n",
    "            channel_names = df_electrodes['channel_name'].tolist()\n",
    "        \n",
    "            # Sort using a key that extracts the numeric part:\n",
    "            sorted_channel_names = sorted(channel_names, key=lambda x: int(x.replace('CSC', '')))\n",
    "        \n",
    "            # Map displayed order (1,2,3,4,...64) to default order (10,11,12,...,1,11)\n",
    "            mapping = {row['channel_name']: idx for idx, row in df_electrodes.iterrows()}\n",
    "            plot_instance.set_electrode_mapping(mapping)\n",
    "        \n",
    "            return sorted_channel_names\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in get_channels_for_file: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05e9b69e-b308-4ef2-8f4f-624ed0cfa806",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_raw_data(file_path, data_index, start, end):\n",
    "    \"\"\"\n",
    "    Retrieve raw data for the given file and channel (as an index) for the duration of start and end.\n",
    "    Use end = -1 to automatically go to the end of the recording.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with NWBHDF5IO(file_path, 'r') as io:\n",
    "            nwb = io.read()\n",
    "            elec_series = nwb.acquisition[\"ElectricalSeries\"]\n",
    "            sampling_rate = elec_series.rate  # in Hz\n",
    "            conversion = elec_series.conversion  # NCS scaling factor (bits to volts)\n",
    "            # Get starting time; if None, assume 0.\n",
    "            t0 = elec_series.starting_time if elec_series.starting_time is not None else 0\n",
    "    \n",
    "            total_samples = elec_series.data.shape[0]\n",
    "            sample_start = int(start * sampling_rate)\n",
    "\n",
    "            if end == -1:\n",
    "                sample_end = total_samples\n",
    "            else:\n",
    "                sample_end = int(end * sampling_rate)\n",
    "\n",
    "            # Ensure bounds are valid\n",
    "            sample_start = max(0, min(sample_start, total_samples))\n",
    "            sample_end = max(sample_start + 1, min(sample_end, total_samples))\n",
    "            \n",
    "            # Generate time vector (x):\n",
    "            if elec_series.timestamps is not None:\n",
    "                # If explicit timestamps exist, slice them.\n",
    "                x = np.array(elec_series.timestamps[sample_start:sample_end])\n",
    "            else:\n",
    "                # Otherwise, compute time from starting_time and sampling_rate.\n",
    "                # np.arange(sample_start, sample_end) gives the sample numbers.\n",
    "                x = t0 + np.arange(sample_start, sample_end) / sampling_rate\n",
    "            \n",
    "            # Extract y data from the specified channel (data_index).\n",
    "            y = np.array(elec_series.data[sample_start:sample_end, data_index]) * conversion\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in get_raw_data: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "487a8f0b-6868-4d83-8c13-56ca6da55675",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_raw_data_column_length(file_path):\n",
    "    \"\"\"\n",
    "    Retrieve the total number of time samples in the ElectricalSeries\n",
    "    and return the duration in seconds.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with NWBHDF5IO(file_path, 'r') as io:\n",
    "            nwb = io.read()\n",
    "            elec_series = nwb.acquisition[\"ElectricalSeries\"]\n",
    "            sampling_rate = elec_series.rate  # Hz\n",
    "\n",
    "            # Get total number of rows (samples)\n",
    "            total_samples = elec_series.data.shape[0]\n",
    "\n",
    "            duration_sec = int(total_samples / sampling_rate)\n",
    "            return duration_sec\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in get_raw_data_column_length: {e}\", exc_info=True)\n",
    "        show_popup(error_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca6859a0-d329-4cf6-8c21-7ae4f661c2a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_plot_window(parent, plot_instance):\n",
    "    \"\"\"\n",
    "    Create a plot window containing a settings panel on the left and a plot on the right.\n",
    "    \n",
    "    Parameters:\n",
    "        parent: The parent tag or window to which this plot window will be attached.\n",
    "        plot_instance: An instance of the Plot class containing plot-related settings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with dpg.group(tag=f\"viz_list_plot_group_{plot_instance.ID}\", horizontal=True, parent=parent):\n",
    "            # Create the overall child window for the plot window.\n",
    "            with dpg.child_window(parent=f\"viz_list_plot_group_{plot_instance.ID}\", tag=f\"{plot_instance.ID}_window\", height=300, autosize_x=True, border=True):\n",
    "                \n",
    "                # Create a horizontal group to split the window into left and right sections.\n",
    "                with dpg.group(horizontal=True):\n",
    "                    \n",
    "                    # Left Section: Settings Panel\n",
    "                    with dpg.child_window(width=220, autosize_y=True, border=False, menubar=True, tag=f\"{plot_instance.ID}_settings_panel\"):\n",
    "                        with dpg.menu_bar():\n",
    "                            dpg.add_menu(label=f\"\", tag=f\"{plot_instance.ID}_plot_name\")\n",
    "\n",
    "                        with dpg.tab_bar(label=\"Plot Settings\"):\n",
    "                            with dpg.tab(label=\"Import\", tag=f\"{plot_instance.ID}_import_tab\"):\n",
    "                                dpg.add_combo(label=\"Plot Type\",\n",
    "                                              items=[\"Raw\", \"Filter\"],\n",
    "                                              default_value=plot_instance.plot_type if plot_instance.plot_type else \"Raw\",\n",
    "                                              width=138,\n",
    "                                              callback=change_plot_type_callback,\n",
    "                                              user_data=plot_instance)\n",
    "                \n",
    "                                dpg.add_separator()\n",
    "                \n",
    "                                NWB_file_list = plot_instance.get_file_list\n",
    "                                dpg.add_combo(label=\"NWB File\",\n",
    "                                              tag=f\"{plot_instance.ID}_nwb_file_combo\",\n",
    "                                              items=NWB_file_list,\n",
    "                                              width=138,\n",
    "                                              callback=select_NWB_file_callback,\n",
    "                                              user_data=plot_instance)\n",
    "                \n",
    "                                dpg.add_combo(label=\"Channel\",\n",
    "                                              tag=f\"{plot_instance.ID}_channel_combo\",\n",
    "                                              items=[],\n",
    "                                              width=138,\n",
    "                                              callback=select_NWB_channel_callback,\n",
    "                                              user_data=plot_instance)\n",
    "\n",
    "                            with dpg.tab(label=\"Plotting\", tag=f\"{plot_instance.ID}_plotting_tab\"):\n",
    "                                dpg.add_text(f\"Data Range: {plot_instance.get_data_min()}-{plot_instance.get_data_max()}\",\n",
    "                                             tag=f\"{plot_instance.ID}_range_text\")\n",
    "            \n",
    "                                dpg.add_input_int(label=\"Start (s)\",\n",
    "                                                  tag=f\"{plot_instance.ID}_start_int\",\n",
    "                                                  width=138,\n",
    "                                                  callback=set_start_callback,\n",
    "                                                  user_data=plot_instance,\n",
    "                                                  on_enter=True,\n",
    "                                                  min_value=plot_instance.get_data_min(),\n",
    "                                                  max_value=plot_instance.get_data_max(),\n",
    "                                                  min_clamped=True,\n",
    "                                                  max_clamped=True)\n",
    "            \n",
    "                                dpg.add_input_int(label=\"End (s)\",\n",
    "                                                  tag=f\"{plot_instance.ID}_end_int\",\n",
    "                                                  width=138,\n",
    "                                                  callback=set_end_callback,\n",
    "                                                  user_data=plot_instance,\n",
    "                                                  on_enter=True,\n",
    "                                                  min_value=plot_instance.get_data_min(),\n",
    "                                                  max_value=plot_instance.get_data_max(),\n",
    "                                                  min_clamped=True,\n",
    "                                                  max_clamped=True)\n",
    "        \n",
    "                                dpg.add_separator()\n",
    "        \n",
    "                                dpg.add_input_int(label=\"Shift (s)\",\n",
    "                                                  tag=f\"{plot_instance.ID}_shift_int\",\n",
    "                                                  width=138,\n",
    "                                                  on_enter=True)\n",
    "        \n",
    "                                with dpg.group(horizontal=True):\n",
    "        \n",
    "                                    dpg.add_text(f\"Shift Keys: \")\n",
    "        \n",
    "                                    dpg.add_button(label=\"Button\", \n",
    "                                                   tag=f\"{plot_instance.ID}_shift_left\",\n",
    "                                                   callback=shift_data_left_callback, \n",
    "                                                   user_data=plot_instance,\n",
    "                                                   arrow=True, \n",
    "                                                   direction=dpg.mvDir_Left)\n",
    "                                    \n",
    "                                    dpg.add_button(label=\"Button\", \n",
    "                                                   tag=f\"{plot_instance.ID}_shift_right\",\n",
    "                                                   callback=shift_data_right_callback, \n",
    "                                                   user_data=plot_instance,\n",
    "                                                   arrow=True, \n",
    "                                                   direction=dpg.mvDir_Right)\n",
    "\n",
    "                                dpg.add_checkbox(label=\"Sync Axis\", \n",
    "                                                 tag=f\"{plot_instance.ID}_sync_status\",\n",
    "                                                 callback=sync_axis_callback,\n",
    "                                                 user_data=plot_instance,\n",
    "                                                 default_value=False)\n",
    "\n",
    "                                dpg.add_combo(label=\"Y Scale\",\n",
    "                                              tag=f\"{plot_instance.ID}_voltage_combo\",\n",
    "                                              items=[\"Volts\",\"Millivolts\",\"Microvolts\"],\n",
    "                                              width=138,\n",
    "                                              default_value=\"Millivolts\",\n",
    "                                              callback=voltage_scale_callback,\n",
    "                                              user_data=plot_instance)\n",
    "\n",
    "                            with dpg.tab(label=\"Export\", tag=f\"{plot_instance.ID}_export_tab\"):\n",
    "                                dpg.add_input_text(label=\"Custom Name\",\n",
    "                                                   tag=f\"{plot_instance.ID}_custom_plot_name\",\n",
    "                                                   #hint=\"Leave blank for default\",\n",
    "                                                   callback=lambda sender, app_data,\n",
    "                                                   user_data : user_data.set_export_status(export_status),\n",
    "                                                   user_data=plot_instance)\n",
    "\n",
    "                                dpg.add_checkbox(label=\"Use Custom Output Path\", \n",
    "                                                 tag=f\"{plot_instance.ID}_export_status\",\n",
    "                                                 callback=lambda sender, app_data,\n",
    "                                                 user_data : user_data.set_custom_name(app_data),\n",
    "                                                 user_data=plot_instance,\n",
    "                                                 default_value=False)\n",
    "                                \n",
    "                                dpg.add_button(label=\"Figure Export\", \n",
    "                                                 tag=f\"{plot_instance.ID}_export_trigger\",\n",
    "                                                 callback=export_trigger_callback,\n",
    "                                                 user_data=plot_instance)\n",
    "                    \n",
    "                    # Right Section: Plot Display\n",
    "                    # This child window auto-sizes in both directions.\n",
    "                    with dpg.child_window(autosize_x=True, autosize_y=True, border=False, tag=f\"{plot_instance.ID}_plot_area_panel\"):\n",
    "                        # Create a plot widget that fills this section.\n",
    "                        # Note: Setting width and height to -1 lets the plot auto-size to its container.\n",
    "                        with dpg.plot(label=\"Plot\", tag=f\"{plot_instance.ID}_plot_widget\", width=-1, height=-1,no_title=True):        \n",
    "                            # Add the X Axis (using the DearPyGui constant for the X Axis).\n",
    "                            with dpg.plot_axis(dpg.mvXAxis, tag=f\"{plot_instance.ID}_x_axis\"):\n",
    "                                pass  # You can add series to the axis later.\n",
    "                            \n",
    "                            # Add the Y Axis.\n",
    "                            with dpg.plot_axis(dpg.mvYAxis, tag=f\"{plot_instance.ID}_y_axis\"):\n",
    "                                # Create an empty line series that will be updated with data.\n",
    "                                dpg.add_line_series([], [], label=\"Raw Data\", tag=f\"{plot_instance.ID}_plot_series\")\n",
    "\n",
    "        dpg.add_button(label=\"Add Plot\", \n",
    "                       tag=\"add_plot_button\",\n",
    "                       parent=parent, \n",
    "                       user_data=parent, \n",
    "                       callback=add_plot_callback,\n",
    "                       show=True)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in create_plot_window: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d46fecd-77fe-445b-b4ea-0ed70a293f80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sync_axis(plot_instance):\n",
    "    \"\"\"\n",
    "    Function to sync every other selected synced plot to the newest change in a plot.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Wait for rendered plot to update before fetching axis data\n",
    "        dpg.split_frame()\n",
    "        \n",
    "        x_min, x_max = dpg.get_axis_limits(f\"{plot_instance.ID}_x_axis\")\n",
    "        y_min, y_max = dpg.get_axis_limits(f\"{plot_instance.ID}_y_axis\")\n",
    "\n",
    "        logging.debug(f\"x_min and x_max are: {x_min}, {x_max}\")\n",
    "        logging.debug(f\"y_min and y_max are: {y_min}, {y_max}\")\n",
    "\n",
    "        for plot_instance_ID in Plot.get_sync_list():\n",
    "            logging.debug(f\"Processing plot ID {plot_instance_ID}\")\n",
    "            if plot_instance_ID != plot_instance.ID:  # Avoid syncing to itself\n",
    "                logging.debug(f\"Applying limit from plot {plot_instance.name} to ID {plot_instance_ID}\")\n",
    "                dpg.set_axis_limits_auto(f\"{plot_instance_ID}_x_axis\")\n",
    "                dpg.set_axis_limits_auto(f\"{plot_instance_ID}_y_axis\")\n",
    "                dpg.set_axis_limits(f\"{plot_instance_ID}_x_axis\", x_min, x_max)\n",
    "                dpg.set_axis_limits(f\"{plot_instance_ID}_y_axis\", y_min, y_max)\n",
    "  \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in sync_axis: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d98ec7f-5a4c-459f-ae66-6a6e0be7f5da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_data(data, sfreq, lowcut, highcut):\n",
    "    \"\"\"\n",
    "    Function to apply bandpass filter to raw data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filtered_data = mne.filter.filter_data(data, sfreq, l_freq=lowcut, h_freq=highcut, method='fir', fir_design='firwin', phase='zero')\n",
    "        return filtered_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in filter_data: {e}\", exc_info=True)\n",
    "        show_popup(error_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6910f004-507e-4004-9874-2e857f6dd50a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def export_plot(x_data, y_data, x_min, x_max, y_min, y_max, name, export_path):\n",
    "    \"\"\"\n",
    "    Generate and save a matplotlib plot from provided data and axis limits.\n",
    "\n",
    "    Parameters:\n",
    "        x_data (array-like): X-axis data\n",
    "        y_data (array-like): Y-axis data\n",
    "        x_min (float): Minimum x-axis limit\n",
    "        x_max (float): Maximum x-axis limit\n",
    "        y_min (float): Minimum y-axis limit\n",
    "        y_max (float): Maximum y-axis limit\n",
    "        name (str): Title of the plot and base of filename\n",
    "        export_path (str): Directory to save the output image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create figure and axis\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.plot(x_data, y_data, linewidth=1)\n",
    "\n",
    "        # Set axis limits\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "\n",
    "        # Set title and labels\n",
    "        ax.set_title(name)\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Signal\")\n",
    "\n",
    "        # Ensure export path exists\n",
    "        os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "        # Construct full filename\n",
    "        filename = f\"{name}.png\"\n",
    "        filepath = os.path.join(export_path, filename)\n",
    "\n",
    "        # Save figure\n",
    "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"Plot saved to: {filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to export plot: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5935a930-117e-473c-bac2-7158169cb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### I/O & Misc Functions #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8d31bce-4d82-4e37-a997-9c006aaca8a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_default_output_directory():\n",
    "    '''\n",
    "    # Function to get the current file path of the executable.\n",
    "    '''\n",
    "    try:\n",
    "        if getattr(sys, 'frozen', False):\n",
    "            # Running as compiled executable\n",
    "            base_dir = os.path.dirname(sys.executable)\n",
    "        elif '__file__' in globals():\n",
    "            # Running as a Python script\n",
    "            base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        else:\n",
    "            # Interactive environment (e.g., Jupyter, IPython)\n",
    "            base_dir = os.getcwd()\n",
    "\n",
    "        return base_dir\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in get_default_output_directory: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d12a891f-9512-4fa2-8601-feac3780c519",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_NWB_folder_filepath():\n",
    "    '''\n",
    "    # Function to get where NWB files are stored\n",
    "    '''\n",
    "    try:\n",
    "        # Open a folder selection dialog\n",
    "        root = Tk()\n",
    "        root.withdraw()  # Hide the root window\n",
    "        project_folder = filedialog.askdirectory(initialdir=\"Projects\", title=\"Select NWB Data Folder\")\n",
    "        if not project_folder:\n",
    "            return  # No folder selected\n",
    "        return project_folder\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in select_NWB_folder: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f26affef-135a-4ab7-914b-cc9110b8b653",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def display_metadata(file_path):\n",
    "    \"\"\"\n",
    "    Load metadata from an NWB file and display it in the UI.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with NWBHDF5IO(file_path, mode='r') as io:\n",
    "            nwbfile = io.read()\n",
    "            metadata = {\n",
    "                \"Identifier\": nwbfile.identifier,\n",
    "                \"Session Description\": nwbfile.session_description,\n",
    "                \"Experimenter\": nwbfile.experimenter,\n",
    "                \"Session ID\" : nwbfile.session_id,\n",
    "                \"Institution\": nwbfile.institution,\n",
    "                \"Experiment Description\" : nwbfile.experiment_description,\n",
    "                \n",
    "                \"Session Start Time\": nwbfile.session_start_time.isoformat(),\n",
    "                \"File Create Date\" : nwbfile.file_create_date,\n",
    "                \"Timestamps Reference Time\" : nwbfile.timestamps_reference_time.isoformat(),\n",
    "            \n",
    "                \"Notes\" : nwbfile.notes\n",
    "            }\n",
    "\n",
    "        # Update or create the metadata display window\n",
    "        if dpg.does_item_exist(\"import_window_grouping_stageB\"):\n",
    "            dpg.delete_item(\"import_window_grouping_stageB\")  # Clear previous content\n",
    "            \n",
    "        with dpg.group(tag=\"import_window_grouping_stageB\", parent=\"import_window_grouping_stageA\"):\n",
    "            with dpg.child_window(tag=\"project_info_window\", menubar=True, border=True, autosize_y=True, autosize_x=True, parent=\"import_window_grouping_stageB\"):\n",
    "                with dpg.menu_bar():\n",
    "                    dpg.add_menu(label=\"Project Info\")\n",
    "                for key, value in metadata.items():\n",
    "                    dpg.add_text(f\"{key}: {value}\", wrap=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in display_metadata: {e}\", exc_info=True)\n",
    "        metadata = {\"Error\": \"Unable to load metadata\"}\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ce13b129-e97a-44fa-aa81-1a593a482fff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_NWB_file_window(parent_tag, parent, nwb_files, folder_path, NWBFolder_Class):\n",
    "    \"\"\"\n",
    "    Creates the lefthand UI for NWB files under the specified parent.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #print(f\"NWB File List: {nwb_files}\")\n",
    "        # Check if the window already exists and close it if necessary\n",
    "        if dpg.does_item_exist(f\"{parent}_window_grouping_stageA\"):\n",
    "            dpg.delete_item(f\"{parent}_window_grouping_stageA\")\n",
    "        \n",
    "        with dpg.group(tag=f\"{parent}_window_grouping_stageA\", parent=parent_tag, horizontal=True):\n",
    "            with dpg.child_window(tag=f\"{parent}_NWB_window\", \n",
    "                                  border=True, width=300, \n",
    "                                  autosize_y=True, \n",
    "                                  horizontal_scrollbar=False, \n",
    "                                  no_scrollbar=False, \n",
    "                                  menubar=True, \n",
    "                                  parent=f\"{parent}_window_grouping_stageA\"):\n",
    "                with dpg.menu_bar():\n",
    "                    dpg.add_menu(label=\"NWB Files\")\n",
    "                \"\"\"\n",
    "                for file in nwb_files:\n",
    "                    dpg.add_checkbox(\n",
    "                        label=file,\n",
    "                        tag=f\"{parent}_{file}\",  # Ensure tags are unique across parents\n",
    "                        callback=checkbox_callback,\n",
    "                        user_data=(folder_path, f\"{parent}_NWB_window\", file),  # Pass all tags and folder path\n",
    "                        parent=f\"{parent}_NWB_window\"\n",
    "                    )\n",
    "                \"\"\"\n",
    "\n",
    "                # Build rat-to-files dictionary\n",
    "                rat_to_files = {i: [] for i in range(1, 65)}  # Rats 1 through 64\n",
    "        \n",
    "                for file in nwb_files:\n",
    "                    rats_in_file = NWBFolder.get_rats_in_file(file)\n",
    "                    for rat in rats_in_file:\n",
    "                        if 1 <= rat <= 64:\n",
    "                            rat_to_files[rat].append(file)\n",
    "        \n",
    "                # Build UI elements grouped by rat\n",
    "                for rat_num in range(1, 65):\n",
    "                    files = rat_to_files[rat_num]\n",
    "                    if not files:\n",
    "                        continue  # Skip rats with no files\n",
    "        \n",
    "                    with dpg.collapsing_header(label=f\"Rat {rat_num}\",tag=f\"{parent}_{rat_num}_header\", default_open=False, parent=f\"{parent}_NWB_window\"):\n",
    "                        for file in sorted(files):\n",
    "                            checkbox_tag = f\"{parent}_{rat_num}_{file}\"\n",
    "                            dpg.add_checkbox(\n",
    "                                label=file,\n",
    "                                tag=checkbox_tag,\n",
    "                                callback=checkbox_callback,\n",
    "                                user_data=(folder_path, f\"{parent}_{rat_num}_header\", file)\n",
    "                            )\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                # List of recognized days in proper order\n",
    "                DAYS = ['PreDay0', 'Day0Control', 'Day0', 'Day1', 'Day3', 'Day7', 'Day14', 'Day21']\n",
    "        \n",
    "                # Build rat-to-files dictionary\n",
    "                rat_to_files = {i: [] for i in range(1, 65)}  # Rats 1 through 64\n",
    "        \n",
    "                for file in nwb_files:\n",
    "                    rats_in_file = NWBFolder.get_rats_in_file(file)\n",
    "                    for rat in rats_in_file:\n",
    "                        if 1 <= rat <= 64:\n",
    "                            rat_to_files[rat].append(file)\n",
    "        \n",
    "                # Build UI elements grouped by rat, then by day\n",
    "                for rat_num in range(1, 65):\n",
    "                    files = rat_to_files[rat_num]\n",
    "                    if not files:\n",
    "                        continue\n",
    "        \n",
    "                    with dpg.collapsing_header(label=f\"Rat {rat_num}\", default_open=False, parent=f\"{parent}_NWB_window\"):\n",
    "                        \n",
    "                        # Build a dictionary to group files by day\n",
    "                        day_to_files = {}\n",
    "                        for file in files:\n",
    "                            day_number, day_label = NWBFolder.get_day_from_file(file)\n",
    "                            if day_label is None:\n",
    "                                continue\n",
    "                            if day_label not in day_to_files:\n",
    "                                day_to_files[day_label] = []\n",
    "                            day_to_files[day_label].append(file)\n",
    "        \n",
    "                        # Sort days using the custom DAY order\n",
    "                        for day_label in DAYS:\n",
    "                            if day_label not in day_to_files:\n",
    "                                continue\n",
    "                            with dpg.collapsing_header(label=f\"{day_label}\",tag=f\"\", default_open=False):\n",
    "                                for file in sorted(day_to_files[day_label]):\n",
    "                                    checkbox_tag = f\"{parent}_{file}\"\n",
    "                                    dpg.add_checkbox(\n",
    "                                        label=file,\n",
    "                                        tag=checkbox_tag,\n",
    "                                        callback=checkbox_callback,\n",
    "                                        user_data=(folder_path, f\"{parent}_NWB_window\", file)\n",
    "                                    )\n",
    "                \"\"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in create_NWB_file_window: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0159d672-ce76-4e50-bc49-a53b2ce383ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def display_NWB_files(NWBFolder_Class):\n",
    "    \"\"\"\n",
    "    Function to display NWB files found in input folder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        folder_path = NWBFolder_Class.get_folder_path()\n",
    "\n",
    "        # Get list of NWB files in folder\n",
    "        nwb_files = [f for f in os.listdir(folder_path) if f.endswith('.nwb')]\n",
    "\n",
    "        # Clear previous list and update class\n",
    "        NWBFolder.set_file_list(nwb_files)\n",
    "\n",
    "        # Clear existing NWB file UI if present\n",
    "        #if dpg.does_item_exist(\"import_child_window\"):\n",
    "        #    dpg.delete_item(\"import_child_window\", children_only=True)\n",
    "\n",
    "        # Recreate NWB file list\n",
    "        create_NWB_file_window(parent_tag=\"import_child_window\",\n",
    "                               parent=\"import\", \n",
    "                               nwb_files=nwb_files, \n",
    "                               folder_path=folder_path,\n",
    "                               NWBFolder_Class=NWBFolder_Class)\n",
    "\n",
    "        # Clear and rebuild visualization window\n",
    "        if dpg.does_item_exist(\"viz_child_window\"):\n",
    "            dpg.delete_item(\"viz_child_window\", children_only=True)\n",
    "\n",
    "        with dpg.group(tag=\"viz_buttons\", horizontal=True, parent=\"viz_child_window\"):\n",
    "            with dpg.child_window(tag=\"plot_buttons\", border=True, autosize_x=True, height=35, parent=\"viz_buttons\"):\n",
    "                with dpg.group(horizontal=True, parent=\"plot_buttons\"):\n",
    "                    dpg.add_text(\"Plot Options:\")\n",
    "\n",
    "        # Create new plot instance and add to plot window\n",
    "        plot_class_1 = Plot()\n",
    "\n",
    "        # Clear existing Plot UI if present\n",
    "        if dpg.does_item_exist(\"viz_child_window\"):\n",
    "            dpg.delete_item(\"viz_child_window\", children_only=True)\n",
    "            \n",
    "        create_plot_window(parent=\"viz_child_window\", plot_instance=plot_class_1)\n",
    "\n",
    "        # Instantiate Analysis class for Analysis window\n",
    "        Analysis_class = Analysis()\n",
    "\n",
    "        # Clear existing Plot UI if present\n",
    "        if dpg.does_item_exist(\"ana_child_window\"):\n",
    "            dpg.delete_item(\"ana_child_window\", children_only=True)\n",
    "\n",
    "        # Fill analysis tab with starting info\n",
    "        create_analysis_window(parent=\"ana_child_window\", Analysis_class=Analysis_class)\n",
    "\n",
    "        # Create new JSON instance and add to JSON window\n",
    "        JSON_class = JSON()\n",
    "        create_JSON_window(JSON_class)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in display_NWB_files: {e}\", exc_info=True)\n",
    "        show_popup(error_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f2aeca2-80c5-4e39-89a1-bfca26b762a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    \"\"\"\n",
    "    Function to load the configuration on startup.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(config_file):\n",
    "            try:\n",
    "                with open(config_file, \"r\") as f:\n",
    "                    config_data = json.load(f)\n",
    "                    logging.debug(f\"Config data: {config_data}\")\n",
    "                return config_data\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading config: {e}\", exc_info=True)\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in load_config: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "611a7652-9d26-4d95-9b9e-b8fd5be4b432",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def copy_log_to_clipboard():\n",
    "    '''\n",
    "    # Function to copy current error log to clipboard\n",
    "    '''\n",
    "    try:\n",
    "        with open(log_filename, 'r') as log_file:\n",
    "            log_content = log_file.read()\n",
    "            clipboard_content = f\"Log File: {os.path.basename(log_filename)}\\n\\n{log_content}\"\n",
    "            pyperclip.copy(clipboard_content)\n",
    "            logging.debug(\"Log file copied to clipboard with file name\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in copy_log_to_clipboard: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ad9985a-73e2-4aed-a7b7-1377f745b01e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def show_popup(message):\n",
    "    '''\n",
    "    # Function to create a pop-up window in Dear PyGui\n",
    "    '''\n",
    "    try:\n",
    "        if dpg.does_item_exist(\"popup_window\"):\n",
    "            dpg.delete_item(\"popup_window\")\n",
    "        with dpg.window(label=\"Notification\", modal=True, no_title_bar=True, autosize=True, tag=\"popup_window\", show=False):\n",
    "            dpg.add_text(message, wrap=600)\n",
    "            with dpg.group(tag=\"popup_window_buttons\", parent=\"popup_window\", horizontal=True):\n",
    "                dpg.add_button(label=\"Close\", callback=lambda: dpg.delete_item(\"popup_window\"))\n",
    "                dpg.add_button(label=\"Open Log\", callback=lambda: os.startfile(log_filename))\n",
    "                dpg.add_button(label=\"Copy Log to Clipboard\", callback=copy_log_to_clipboard)\n",
    "    \n",
    "        dpg.show_item(\"popup_window\")\n",
    "        \n",
    "        # Allow the window to calculate its size\n",
    "        dpg.split_frame()\n",
    "    \n",
    "        # Get the viewport size\n",
    "        viewport_width = dpg.get_viewport_client_width()\n",
    "        viewport_height = dpg.get_viewport_client_height()\n",
    "    \n",
    "        # Get the window size\n",
    "        window_width = dpg.get_item_width(\"popup_window\")\n",
    "        window_height = dpg.get_item_height(\"popup_window\")\n",
    "    \n",
    "        # Calculate the position\n",
    "        pos_x = (viewport_width - window_width) // 2\n",
    "        pos_y = (viewport_height - window_height) // 2\n",
    "    \n",
    "        # Set the window position\n",
    "        dpg.set_item_pos(\"popup_window\", [pos_x, pos_y])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in show_popup: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "16594c3e-2448-45ad-87e8-c7267dfe56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################################\n",
    "# Callbacks\n",
    "#######################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4a4f0cd-f5e9-48d6-b803-447245a890f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### JSON Plotting Callbacks #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e597311a-f769-4d77-8e76-e75f41678ffb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def crunch_PAC_JSON_data_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to plot data of PAC JSON data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Analysis_class = user_data\n",
    "        output_dir = Analysis_class.get_output_directory()\n",
    "        \n",
    "        # Open a folder selection dialog\n",
    "        root = Tk()\n",
    "        root.withdraw() # hide the little Tk window\n",
    "    \n",
    "        JSON_path = filedialog.askopenfilename(\n",
    "            initialdir=output_dir,\n",
    "            title=\"Select JSON file\",\n",
    "            filetypes=[(\"JSON files\", \"*.json\")],\n",
    "        )\n",
    "        root.destroy()\n",
    "        if not JSON_path:\n",
    "            return  # No folder selected\n",
    "\n",
    "        #----- LF SNR Plotting -----#\n",
    "\n",
    "        ### 1) Load JSON's into pandas dataframe\n",
    "        df = load_snr_df(JSON_path, condition_map=None,\n",
    "                         bands_keep=[\"Delta\",\"Theta\",\"Wide Theta\",\"High Theta\",\"Beta\"])\n",
    "\n",
    "        ### 2) Track channels with high SNR\n",
    "        # Ensure snr_db is numeric (paranoia)\n",
    "        df[\"snr_db\"] = pd.to_numeric(df[\"snr_db\"], errors=\"coerce\")\n",
    "        \n",
    "        # Keep rows with SNR >= 6 dB\n",
    "        hits = df.loc[df[\"snr_db\"].ge(6)].copy()\n",
    "        \n",
    "        # Keep only the useful columns\n",
    "        cols_order = [\"rat\",\"day\",\"session\",\"condition\",\"probe\",\"channel\",\"band\",\"snr_db\"]\n",
    "        hits = hits[[c for c in cols_order if c in hits.columns]]\n",
    "        \n",
    "        ### 3) Plot per-band distributions\n",
    "        output_dir = Path(output_dir)\n",
    "        timestamp = datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "        session_folder = output_dir / f\"PAC JSON Statistics {timestamp}\"\n",
    "        session_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        figs = plot_snr_distributions(df, save_dir=session_folder, kind=\"box\", thresholds=(3,6))\n",
    "        \n",
    "        ### 4) Get a table of medians/IQRs to report\n",
    "        table = summarize_snr(df)\n",
    "\n",
    "        ### 5) Save SNR results\n",
    "        # Table data\n",
    "        out = os.path.join(session_folder, f\"SNR_report.csv\")\n",
    "        table.to_csv(out, float_format=\"%.4f\")\n",
    "\n",
    "        # High SNR data\n",
    "        for band, g in hits.groupby(\"band\"):\n",
    "            out = os.path.join(session_folder, f\"High_SNR_channels_in_{band}.csv\")\n",
    "            g.to_csv(out, index=False, float_format=\"%.4f\")\n",
    "\n",
    "        #----- HF SNR Plotting -----#\n",
    "\n",
    "        # 1) Load df with HF info\n",
    "        hf_df = load_hf_df(JSON_path, condition_map=None)\n",
    "        \n",
    "        # 2) Compute HF hits and distributions\n",
    "        # Save high-HF rows if you like (e.g., rel_power ≥ 0.2)\n",
    "        hf_hits = hf_df[hf_df[\"hf_rel_power\"].ge(0.4)].copy()\n",
    "        out = os.path.join(session_folder, f\"hf_hits_relpower_ge0.2.csv\")\n",
    "\n",
    "        # 3) Keep only the useful columns\n",
    "        cols_order = [\"rat\",\"day\",\"session\",\"condition\",\"probe\",\"hf_band_low\", \"hf_band_high\", \"hf_power\", \"hf_power_db\", \"ref_power\", \"hf_rel_power\", \"hf_percentile_in_ref\"]\n",
    "        hf_hits = hf_hits[[c for c in cols_order if c in hf_hits.columns]]\n",
    "        hf_hits.to_csv(out, index=False, float_format=\"%.4f\")\n",
    "        \n",
    "        # 4) Plot distributions\n",
    "        plot_hf_distributions(hf_df, metric=\"hf_rel_power\", save_dir=session_folder, thresholds=(0.3, 0.5))\n",
    "        plot_hf_distributions(hf_df, metric=\"hf_percentile_in_ref\", save_dir=session_folder, thresholds=(60,80))\n",
    "\n",
    "\n",
    "        #----- PAC AVG Trajectory Plotting -----#\n",
    "        # Load data from JSON file\n",
    "        df_PAC_AVG = load_pac_json_to_long_df(JSON_path)\n",
    "        if df_PAC_AVG is not None:\n",
    "            \"\"\"\n",
    "            # Case A: Day 0 sessions only (D0:0, D0:1, D0:2)\n",
    "            plot_pac_trajectories(df_PAC_AVG,\n",
    "                                  metrics=[\"primary_summary_value\",\"peak_value\",\"z_abs_mean\",\n",
    "                                           \"z_topk_mean\",\"frac_sig_ge_1.96\",\"peak_phase_hz\",\"peak_amplitude_hz\"],\n",
    "                                  case=\"Full_House\",\n",
    "                                  save=True,\n",
    "                                  output_dir=session_folder,\n",
    "                                  termes_font=termes_font, termes_font_bold=termes_font_bold,\n",
    "                                  clamp_y0=False)\n",
    "\n",
    "            \n",
    "            # Case B: D0:1 → Day 1 → Day 3 → Day 7 → Day 14\n",
    "            plot_pac_trajectories(df_PAC_AVG,\n",
    "                                  metrics=[\"primary_summary_value\",\"peak_value\",\"z_abs_mean\",\n",
    "                                           \"z_topk_mean\",\"frac_sig_ge_1.96\",\"peak_phase_hz\",\"peak_amplitude_hz\"],\n",
    "                                  case=\"D0_1_to_14\",\n",
    "                                  save=True,\n",
    "                                  output_dir=session_folder,\n",
    "                                  termes_font=termes_font, termes_font_bold=termes_font_bold,\n",
    "                                  clamp_y0=False)\n",
    "            \"\"\"\n",
    "            # Case study of all time points of interest\n",
    "            plot_pac_trajectories(df_PAC_AVG,\n",
    "                                  metrics=[\"primary_summary_value\",\"peak_value\",\"z_abs_mean\",\n",
    "                                           \"z_topk_mean\",\"frac_sig_ge_1.96\",\"peak_phase_hz\",\"peak_amplitude_hz\"],\n",
    "                                  case=\"Full_House\",\n",
    "                                  save=True,\n",
    "                                  output_dir=session_folder,\n",
    "                                  termes_font=termes_font, termes_font_bold=termes_font_bold,\n",
    "                                  clamp_y0=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in crunch_PAC_JSON_data_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c7cee829-1bd6-439c-b3e0-7240198c9f9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def crunch_PSD_JSON_data_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to plot data of JSON checkbox.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Analysis_class = user_data\n",
    "        output_dir = Analysis_class.get_output_directory()\n",
    "        \n",
    "        # Open a folder selection dialog\n",
    "        root = Tk()\n",
    "        root.withdraw() # hide the little Tk window\n",
    "    \n",
    "        JSON_path = filedialog.askopenfilename(\n",
    "            initialdir=output_dir,\n",
    "            title=\"Select JSON file\",\n",
    "            filetypes=[(\"JSON files\", \"*.json\")],\n",
    "        )\n",
    "        root.destroy()\n",
    "        if not JSON_path:\n",
    "            return  # No folder selected\n",
    "\n",
    "        plot_trajectories(JSON_path,\n",
    "                          metrics=[\"Broadband_dB_Raw\",\n",
    "                                   \"ThetaDelta_dB_Raw\",\n",
    "                                   \"Aperiodic_Exponent\"],\n",
    "                          probes=None, # or None to include all\n",
    "                          save=True,\n",
    "                          output_dir = output_dir)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in crunch_PSD_JSON_data_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "205c32bc-81fe-4f80-b09c-97117c1c1236",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def JSON_plot_data_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to plot data of JSON checkbox.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rat_id, day_id, channel_name, probe, data, session_id = user_data\n",
    "        if app_data == True:\n",
    "            dpg.add_scatter_series([int(day_id)],\n",
    "                                   [data],\n",
    "                                   label=f\"R{rat_id}_D{day_id}_{channel_name}_{probe}\",\n",
    "                                   parent=\"JSON_y_axis\",\n",
    "                                   tag=f\"F{session_id}_R{rat_id}_D{day_id}_{channel_name}_{probe}\")\n",
    "        else:\n",
    "            if dpg.does_item_exist(f\"F{session_id}_R{rat_id}_D{day_id}_{channel_name}_{probe}\"):\n",
    "                dpg.delete_item(f\"F{session_id}_R{rat_id}_D{day_id}_{channel_name}_{probe}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in JSON_plot_data_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f02d11d4-0494-4db9-b51d-cb56d70ae68d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def JSON_plot_data_series_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to plot data of JSON checkbox.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rat_id, day_series, channel_name, probe, average_pac_series, session_id = user_data\n",
    "        if app_data == True:\n",
    "            dpg.add_scatter_series(day_series,\n",
    "                                   average_pac_series,\n",
    "                                   label=f\"R{rat_id}_{channel_name}_{probe}\",\n",
    "                                   parent=\"JSON_y_axis\",\n",
    "                                   tag=f\"F{session_id}_R{rat_id}_{channel_name}_{probe}\")\n",
    "        else:\n",
    "            if dpg.does_item_exist(f\"F{session_id}_R{rat_id}_{channel_name}_{probe}\"):\n",
    "                dpg.delete_item(f\"F{session_id}_R{rat_id}_{channel_name}_{probe}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in JSON_plot_data_series_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "acc5befd-a821-46da-bdb3-fe7fed7479ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def import_JSON_file_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to import user-selected JSON file to program.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        JSON_class, meta_parent, data_parent, plot_parent = user_data\n",
    "        # Open a folder selection dialog\n",
    "        root = Tk()\n",
    "        root.withdraw()  # Hide the root window\n",
    "        JSON_file = filedialog.askopenfilename(initialdir=\"PAC Output\",\n",
    "                                            title=\"Select JSON File\",\n",
    "                                            filetypes=[(\"JSON files\", \"json\")])\n",
    "        if not JSON_file:\n",
    "            return  # No folder selected\n",
    "        metadata, pac_scores = load_pac_summary_json(JSON_file)\n",
    "\n",
    "        if \"Session Name\" in metadata:\n",
    "            session_name = metadata[\"Session Name\"]\n",
    "        else:\n",
    "            session_name = \"Session (Name DNE)\"\n",
    "\n",
    "        session_id = JSON_class.get_session_ID_counter()\n",
    "        JSON_class.set_session_ID_counter(session_id + 1)\n",
    "        dpg.add_tree_node(label=session_name,\n",
    "                          tag=f\"JSON_{session_id}_meta_tree\",\n",
    "                          parent=meta_parent)\n",
    "\n",
    "        for key in metadata:\n",
    "            if key != \"Session Name\":\n",
    "                dpg.add_text(f\"{key}: {metadata[key]}\",\n",
    "                             parent=f\"JSON_{session_id}_meta_tree\")\n",
    "\n",
    "        # Loop through rat → day → channel\n",
    "        dpg.add_tree_node(label=session_name,\n",
    "                          tag=f\"JSON_{session_id}_data_tree\",\n",
    "                          parent=data_parent)\n",
    "\n",
    "        # Create individual rat-day-channel checkboxes\n",
    "        for rat_id, rat_data in pac_scores.items():\n",
    "            dpg.add_tree_node(label=f\"Rat {rat_id}\",\n",
    "                              tag=f\"JSON_{session_id}_{rat_id}_data_tree\",\n",
    "                              parent=f\"JSON_{session_id}_data_tree\")\n",
    "\n",
    "            dpg.add_tree_node(label=\"Channels\",\n",
    "                          tag=f\"JSON_{session_id}_{rat_id}_channel_tree\",\n",
    "                          parent=f\"JSON_{session_id}_{rat_id}_data_tree\")\n",
    "            \n",
    "            for day_id, day_data in rat_data.items():\n",
    "                dpg.add_tree_node(label=f\"Day {day_id}\",\n",
    "                              tag=f\"JSON_{session_id}_{rat_id}_{day_id}_data_tree\",\n",
    "                              parent=f\"JSON_{session_id}_{rat_id}_data_tree\")\n",
    "                \n",
    "                for channel_name, channel_data in day_data.items():\n",
    "                    # Reduce channel number to 1-16\n",
    "                    ch_split = channel_name.split('CSC')\n",
    "                    ch_num = int(ch_split[1])\n",
    "                    ch_num = (ch_num - 1) % 16 + 1\n",
    "                    channel_name = ch_split[0] + str(ch_num)\n",
    "\n",
    "                    # Add checkbox for individual rat-channel-day\n",
    "                    average_pac = channel_data['average_pac']\n",
    "                    probe = channel_data['Probe Type']\n",
    "                    dpg.add_checkbox(label=f\"Channel {channel_name}\",\n",
    "                                     tag=f\"JSON_{session_id}_{rat_id}_{day_id}_{channel_name}_data_tree\",\n",
    "                                     parent=f\"JSON_{session_id}_{rat_id}_{day_id}_data_tree\",\n",
    "                                     default_value=False,\n",
    "                                     callback=JSON_plot_data_callback,\n",
    "                                     user_data=(rat_id,\n",
    "                                                day_id,\n",
    "                                                channel_name,\n",
    "                                                probe,\n",
    "                                                average_pac,\n",
    "                                                session_id))\n",
    "\n",
    "                    # Add checkbox for channel across all days\n",
    "                    if dpg.does_item_exist(f\"JSON_{session_id}_{rat_id}_{channel_name}_checkbox\"):\n",
    "                        # Get current user_data\n",
    "                        (rat_id, day_series, channel_name, probe, average_pac_series,\n",
    "                         session_id) = dpg.get_item_user_data(f\"JSON_{session_id}_{rat_id}_{channel_name}_checkbox\")\n",
    "                        # Append new average_pac to series and day\n",
    "                        average_pac_series.append(average_pac)\n",
    "                        day_series.append(int(day_id))\n",
    "                        # Update user_data\n",
    "                        dpg.set_item_user_data(f\"JSON_{session_id}_{rat_id}_{channel_name}_checkbox\",\n",
    "                                               (rat_id, day_series, channel_name, probe,\n",
    "                                               average_pac_series, session_id))\n",
    "                    else:\n",
    "                        # Make checkbox if this is the first time enountering channel\n",
    "                        dpg.add_checkbox(label=f\"Channel {channel_name}\",\n",
    "                                         tag=f\"JSON_{session_id}_{rat_id}_{channel_name}_checkbox\",\n",
    "                                         parent=f\"JSON_{session_id}_{rat_id}_channel_tree\",\n",
    "                                         default_value=False,\n",
    "                                         callback=JSON_plot_data_series_callback,\n",
    "                                         user_data=(rat_id,\n",
    "                                                    [int(day_id)],\n",
    "                                                    channel_name,\n",
    "                                                    probe,\n",
    "                                                    [average_pac],\n",
    "                                                    session_id))\n",
    "                    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in import_JSON_file_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b290ad25-147a-4d61-a92d-61aa2bd0b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Analysis Callbacks #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "27b56bf5-2ed2-40e4-a385-1d11c75a26f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def PAC_options_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to update selected PAC Options.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        setting = sender\n",
    "        updated_value = app_data\n",
    "        Analysis_class = user_data\n",
    "\n",
    "        if setting == \"PAC_options_low_override\":\n",
    "            Analysis_class.set_lowfreq_override(updated_value)\n",
    "        elif setting == \"PAC_options_low_lowpass\":\n",
    "            Analysis_class.set_lowfreq_lowpass(updated_value)\n",
    "        elif setting == \"PAC_options_low_highpass\":\n",
    "            Analysis_class.set_lowfreq_highpass(updated_value)\n",
    "        elif setting == \"PAC_options_low_width\":\n",
    "            Analysis_class.set_lowfreq_width(updated_value)\n",
    "        elif setting == \"PAC_options_low_step\":\n",
    "            Analysis_class.set_lowfreq_step(updated_value)\n",
    "        \n",
    "        elif setting == \"PAC_options_high_override\":\n",
    "            Analysis_class.set_highfreq_override(updated_value)\n",
    "        elif setting == \"PAC_options_high_lowpass\":\n",
    "            Analysis_class.set_highfreq_lowpass(updated_value)\n",
    "        elif setting == \"PAC_options_high_highpass\":\n",
    "            Analysis_class.set_highfreq_highpass(updated_value)\n",
    "        elif setting == \"PAC_options_high_width\":\n",
    "            Analysis_class.set_highfreq_width(updated_value)\n",
    "        elif setting == \"PAC_options_high_step\":\n",
    "            Analysis_class.set_highfreq_step(updated_value)\n",
    "\n",
    "        elif setting == \"PAC_options_high_pass_filter\":\n",
    "            Analysis_class.set_high_pass_filter(updated_value)\n",
    "        elif setting == \"PAC_options_filter_notch_frequencies\":\n",
    "            Analysis_class.set_filter_notch_frequencies(updated_value)\n",
    "        elif setting == \"PAC_options_detrend_epochs\":\n",
    "            Analysis_class.set_detrend_epochs(updated_value)\n",
    "        elif setting == \"PAC_options_apply_autofilter\":\n",
    "            Analysis_class.set_apply_autofilter(updated_value)\n",
    "\n",
    "        elif setting == \"PAC_options_PAC_methods\":\n",
    "            Analysis_class.set_PAC_method(updated_value)\n",
    "        elif setting == \"PAC_options_surrogate_method\":\n",
    "            Analysis_class.set_surrogate_method(updated_value)\n",
    "        elif setting == \"PAC_options_normalization_method\":\n",
    "            Analysis_class.set_normalization_method(updated_value)\n",
    "        elif setting == \"PAC_options_dcomplex_method\":\n",
    "            Analysis_class.set_dcomplex_method(updated_value)\n",
    "        elif setting == \"PAC_options_phase_cycles\":\n",
    "            Analysis_class.set_phase_cycles(updated_value)\n",
    "        elif setting == \"PAC_options_amplitude_cycles\":\n",
    "            Analysis_class.set_amplitude_cycles(updated_value)\n",
    "        elif setting == \"PAC_options_morlet’s_wavelet_width\":\n",
    "            Analysis_class.set_morlet_width(updated_value)\n",
    "        elif setting == \"PAC_options_KLD_or_HRPAC_bins\":\n",
    "            Analysis_class.set_KLD_or_HRPAC_bins(updated_value)\n",
    "\n",
    "        elif setting == \"Performance_options_sampling_rate\":\n",
    "            Analysis_class.set_sampling_rate(updated_value)\n",
    "        elif setting == \"Performance_options_downsample_rate\":\n",
    "            Analysis_class.set_downsample_rate(updated_value)\n",
    "        elif setting == \"Performance_options_epoch_length\":\n",
    "            Analysis_class.set_epoch_length(updated_value)\n",
    "        elif setting == \"Performance_options_data_length\":\n",
    "            Analysis_class.set_data_length(updated_value)  \n",
    "        elif setting == \"Performance_options_minimum_length\":\n",
    "            Analysis_class.set_minimum_length(updated_value)\n",
    "        elif setting == \"Performance_options_parallel_processes\":\n",
    "            Analysis_class.set_parallel_processes(updated_value)\n",
    "        elif setting == \"Performance_options_surrogate_count\":\n",
    "            Analysis_class.set_surrogate_count(updated_value)\n",
    "\n",
    "        elif setting == \"PSD_options_high_pass_filter\":\n",
    "            Analysis_class.set_PSD_high_pass_filter(updated_value)\n",
    "        elif setting == \"PSD_options_notch_frequencies\":\n",
    "            Analysis_class.set_PSD_notch_frequencies(updated_value)\n",
    "        elif setting == \"PSD_options_correct_1overf\":\n",
    "            Analysis_class.set_PSD_correct_1overf(updated_value)\n",
    "        elif setting == \"PSD_options_fmin\":\n",
    "            Analysis_class.set_PSD_fmin(updated_value)\n",
    "        elif setting == \"PSD_options_fmax\":\n",
    "            Analysis_class.set_PSD_fmax(updated_value)\n",
    "        elif setting == \"PSD_options_voltage_scale\":\n",
    "            Analysis_class.set_PSD_voltage_scale(updated_value)\n",
    "\n",
    "        elif setting == \"Export_options_output_folder_name\":\n",
    "            Analysis_class.set_output_folder_name(updated_value)\n",
    "        elif setting == \"Export_options_export_PNG\":\n",
    "            Analysis_class.set_export_PNG(updated_value)\n",
    "        elif setting == \"Export_options_export_PDF\":\n",
    "            Analysis_class.set_export_PDF(updated_value)\n",
    "        elif setting == \"Export_options_export_SVG\":\n",
    "            Analysis_class.set_export_SVG(updated_value)\n",
    "        elif setting == \"Export_options_export_EPS\":\n",
    "            Analysis_class.set_export_EPS(updated_value)\n",
    "        elif setting == \"Export_options_image_height\":\n",
    "            Analysis_class.set_image_height(updated_value)\n",
    "        elif setting == \"Export_options_image_width\":\n",
    "            Analysis_class.set_image_width(updated_value)\n",
    "        elif setting == \"Export_options_image_DPI\":\n",
    "            Analysis_class.set_image_DPI(updated_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in PAC_options_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a461aa99-3009-449f-860a-0ba3e2226552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def PAC_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to start PAC analysis of selected files.\n",
    "    Determines how to process PAC as individual channels\n",
    "    or all associated channels of a rat, based on the \n",
    "    user's decision to use autofilter.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Analysis_class = user_data\n",
    "        autofilter_status = Analysis_class.get_apply_autofilter()\n",
    "        if autofilter_status:\n",
    "            compute_PAC_of_rats(Analysis_class)\n",
    "        else:\n",
    "            compute_PAC_of_channels(Analysis_class)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in PAC_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "866f5fa4-e592-4e14-80a9-737ef1a2cee4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_ana_nwb_list_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to update list of selected NWB files in analysis class.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Analysis_class, file, channel_column_location = user_data\n",
    "        checked = app_data\n",
    "        if checked == True:\n",
    "            Analysis_class.add_to_nwb_list(file)\n",
    "\n",
    "            dpg.add_tree_node(label=file, tag=f\"{file}_tree\", parent=channel_column_location)\n",
    "            folder_path = Analysis_class.get_folder_path\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            with NWBHDF5IO(file_path, 'r') as io:\n",
    "                nwb = io.read()\n",
    "                # Convert electrodes table to a pandas DataFrame\n",
    "                df_electrodes = nwb.electrodes.to_dataframe()\n",
    "\n",
    "                # Create channel_name -> data index mapping\n",
    "                channel_name_to_index = {row['channel_name']: idx for idx, row in df_electrodes.iterrows()}\n",
    "            \n",
    "                # Extract and sort channel names by number (e.g., CSC10 -> 10)\n",
    "                sorted_channel_names = sorted(\n",
    "                    channel_name_to_index.keys(),\n",
    "                    key=lambda name: int(name.replace('CSC', ''))\n",
    "                )\n",
    "\n",
    "                # Make QOL Buttons to select all channels for each rat\n",
    "                rats_in_file = Analysis_class.get_rats_in_file(file)\n",
    "                for rat in rats_in_file:\n",
    "                    rat_channels = Analysis_class.get_channels_from_rat(file, rat)\n",
    "\n",
    "                    # Get the acutal column # of data in NWB file corresponding to this channel\n",
    "                    data_columns = []\n",
    "                    for channel in rat_channels:\n",
    "                        channel_name = 'CSC' + str(channel)\n",
    "                        if channel_name in channel_name_to_index:\n",
    "                            data_columns.append(channel_name_to_index[channel_name])\n",
    "                    \n",
    "                    dpg.add_checkbox(\n",
    "                        label=f\"Rat {rat}\",\n",
    "                        tag=f\"{file}_{rat}\",\n",
    "                        parent=f\"{file}_tree\",\n",
    "                        callback=update_selected_rat_callback,\n",
    "                        user_data=(Analysis_class, file, rat, rat_channels, data_columns)\n",
    "                    )\n",
    "\n",
    "                dpg.add_separator(parent=f\"{file}_tree\")\n",
    "\n",
    "                # Add buttons for each channel\n",
    "                for channel_name in sorted_channel_names:\n",
    "                    if channel_name not in channel_name_to_index:\n",
    "                        continue  # skip weird cases\n",
    "                    \n",
    "                    data_index = channel_name_to_index[channel_name]\n",
    "                    dpg.add_checkbox(\n",
    "                        label=channel_name,\n",
    "                        tag=f\"{file}_{channel_name}\",\n",
    "                        parent=f\"{file}_tree\",\n",
    "                        callback=update_selected_PAC_channels_callback,\n",
    "                        user_data=(Analysis_class, file, channel_name, data_index)\n",
    "                    )\n",
    "        else:\n",
    "            # Remove from list of selected nwb files\n",
    "            Analysis_class.remove_from_nwb_list(file)\n",
    "\n",
    "            # Remove NWB file entries in selected channels dict\n",
    "            if file in Analysis_class.selected_channels:\n",
    "                del Analysis_class.selected_channels[file]\n",
    "\n",
    "            # Delete the dearpygui tree node and children\n",
    "            if dpg.does_item_exist(f\"{file}_tree\"):\n",
    "                dpg.delete_item(f\"{file}_tree\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in update_ana_nwb_list_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0af90087-57ad-4395-b8c9-73090a1ef342",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_selected_rat_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to select all channels associated to a rat.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Analysis_class, file, rat, rat_channels, data_columns = user_data\n",
    "        is_checked = app_data\n",
    "\n",
    "        # Update rat selection\n",
    "        if is_checked:\n",
    "            Analysis_class.add_selected_rat(file, rat, rat_channels, data_columns)\n",
    "        else:\n",
    "            Analysis_class.remove_selected_rat(file, rat)\n",
    "\n",
    "        # Update corresponding channel selections\n",
    "        for channel_name in rat_channels:\n",
    "            channel_name = 'CSC' + str(channel_name)\n",
    "            checkbox_tag = f\"{file}_{channel_name}\"\n",
    "            if dpg.does_item_exist(checkbox_tag):\n",
    "                dpg.set_value(checkbox_tag, is_checked)\n",
    "                \n",
    "                config = dpg.get_item_configuration(checkbox_tag)\n",
    "                callback = config.get(\"callback\")\n",
    "                user_data = config.get(\"user_data\")\n",
    "    \n",
    "                if callback:\n",
    "                    callback(checkbox_tag, is_checked, user_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in update_selected_rat_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1068624e-7c67-4322-bc3a-f7b6b013914f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_selected_PAC_channels_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to update selected channels for each NWB file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Analysis_class, file, channel_name, data_index = user_data\n",
    "        checked = app_data  # True if checked, False if unchecked\n",
    "\n",
    "        if checked:\n",
    "            Analysis_class.add_selected_channel(file, channel_name, data_index)\n",
    "        else:\n",
    "            Analysis_class.remove_selected_channel(file, channel_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in update_selected_PAC_channels_callback: {e}\", exc_info=True)\n",
    "        show_popup(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8a3cd7a0-b434-4ffe-8c75-2ef6ef4a55ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def open_analysis_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to instantiate a window in the analysis tab for data analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Instantiate first instance of analysis class\n",
    "        Analysis_class = Analysis()\n",
    "\n",
    "        # Fill analysis tab with starting info\n",
    "        create_analysis_window(parent=\"ana_child_window\", Analysis_class=Analysis_class)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in open_analysis_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "809de1c5-79f9-4d2c-b57b-5a1b785133ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def set_PAC_output_directory_callback(sender, app_data, user_data):\n",
    "    '''\n",
    "    # Function to get where NWB files are stored\n",
    "    '''\n",
    "    try:\n",
    "        Analysis_class = user_data\n",
    "        # Open a folder selection dialog\n",
    "        root = Tk()\n",
    "        root.withdraw()  # Hide the root window\n",
    "        project_folder = filedialog.askdirectory(initialdir=\"PAC Output\", title=\"Set Output Folder Location\")\n",
    "        if not project_folder:\n",
    "            return  # No folder selected\n",
    "        Analysis_class.set_output_directory(project_folder)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in set_PAC_output_directory: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "21063549-5c54-470f-88e3-00408fdf59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Plotting Callbacks #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2b5c05bf-8519-4bab-a74c-bd834a7c432a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def select_NWB_file_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback when an NWB file is selected.\n",
    "    Updates the Plot instance's nwb_file attribute and updates the channel combo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data  # Passed in as user_data from the callback\n",
    "        selected_file = app_data  # The NWB file name selected from the combo\n",
    "        logging.debug(f\"Plot '{plot_instance.ID}': Selected NWB file: {selected_file}\")\n",
    "        plot_instance.nwb_file = selected_file # Update selected file in class\n",
    "    \n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(plot_instance.get_folder_path, selected_file)\n",
    "        # Get channels available in this file\n",
    "        channels = get_channels_for_file(file_path,plot_instance)\n",
    "        # Update the channel combo widget using its unique tag.\n",
    "        channel_combo_tag = f\"{plot_instance.ID}_channel_combo\"\n",
    "        dpg.configure_item(channel_combo_tag, items=channels)\n",
    "\n",
    "        # Clear channel selection and plot if anything has already been selected\n",
    "        dpg.set_value(channel_combo_tag, \"\")\n",
    "        dpg.set_value(f\"{plot_instance.ID}_plot_series\", [[],[]])\n",
    "        dpg.configure_item(f\"{plot_instance.ID}_plot_name\", label=f\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in selected_NWB_file_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7d23f5e8-2035-4e0d-8eb3-4a2d635efb2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def select_NWB_channel_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback when a channel is selected.\n",
    "    Updates the Plot instance's channel attribute, gets the raw data,\n",
    "    and updates the plot with the new data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data\n",
    "        selected_channel = app_data\n",
    "        # Update selected channel\n",
    "        plot_instance.set_channel(selected_channel)\n",
    "        # Construct full file path from the selected NWB file\n",
    "        file_path = os.path.join(plot_instance.get_folder_path, plot_instance.nwb_file)\n",
    "        \n",
    "        plot_data(plot_instance)\n",
    "\n",
    "        # Extract column of interest for updating GUI info\n",
    "        electrode_mapping = plot_instance.get_electrode_mapping()\n",
    "        selected_channel = plot_instance.get_channel()\n",
    "        raw_data_column_index = electrode_mapping[selected_channel]\n",
    "\n",
    "        # Update maximum allowed range for selected channel\n",
    "        max_length = get_raw_data_column_length(file_path)\n",
    "        plot_instance.set_data_max(max_length)\n",
    "        updated_range = f\"Data Range: {plot_instance.get_data_min()}-{max_length}\"\n",
    "        dpg.set_value(f\"{plot_instance.ID}_range_text\", updated_range)\n",
    "        dpg.configure_item(f\"{plot_instance.ID}_end_int\", max_value=max_length)\n",
    "        dpg.configure_item(f\"{plot_instance.ID}_start_int\", max_value=max_length)\n",
    "\n",
    "        # Update name of plot\n",
    "        rat_num, probe = plot_instance.get_rat_and_probe_from_channel\n",
    "        plot_instance.set_plot_name(rat_num)\n",
    "        dpg.configure_item(f\"{plot_instance.ID}_plot_name\",\n",
    "                           label=f\"Rat {rat_num} - {selected_channel} - ({probe})\")\n",
    "\n",
    "        # Update start and end widgets\n",
    "        dpg.set_value(f\"{plot_instance.ID}_start_int\", plot_instance.get_data_start())\n",
    "        dpg.set_value(f\"{plot_instance.ID}_end_int\", plot_instance.get_data_end())\n",
    "\n",
    "        # Fit both axes to the new data\n",
    "        dpg.fit_axis_data(f\"{plot_instance.ID}_x_axis\")\n",
    "        dpg.fit_axis_data(f\"{plot_instance.ID}_y_axis\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in select_NWB_channel_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7b756a9c-49d0-453c-b7c9-a3193e4609f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def voltage_scale_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback when a voltage scale is changed, updates corresponding plot.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data\n",
    "        new_voltage_scale = app_data\n",
    "\n",
    "        # Convert string representation to int\n",
    "        if new_voltage_scale == \"Millivolts\":\n",
    "            new_voltage_scale = 1000\n",
    "        elif new_voltage_scale == \"Microvolts\":\n",
    "            new_voltage_scale = 1000000\n",
    "        else:\n",
    "            new_voltage_scale = 1\n",
    "\n",
    "        # Get current scale and set new scale\n",
    "        current_voltage_scale = plot_instance.get_voltage_scale()\n",
    "        plot_instance.set_voltage_scale(new_voltage_scale)\n",
    "\n",
    "        # Compute ratio and get plot data to adjust y axis\n",
    "        scaling_ratio = new_voltage_scale / current_voltage_scale\n",
    "        x, y = dpg.get_value(f\"{plot_instance.ID}_plot_series\")\n",
    "        \n",
    "        # Convert y to a NumPy array if it isn’t already, then scale\n",
    "        y_scaled = (np.array(y) * scaling_ratio).tolist()\n",
    "        \n",
    "        # Update the line series\n",
    "        dpg.set_value(f\"{plot_instance.ID}_plot_series\", [x, y_scaled])\n",
    "\n",
    "        # Fit both axes to the new data\n",
    "        dpg.fit_axis_data(f\"{plot_instance.ID}_x_axis\")\n",
    "        dpg.fit_axis_data(f\"{plot_instance.ID}_y_axis\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in voltage_scale_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a47de09-03d4-442f-9e3e-217947381790",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def set_start_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Set starting x-axis values in seconds for selected plot\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Update left-most point to observe\n",
    "        plot_instance = user_data\n",
    "        new_data_start = app_data\n",
    "        plot_instance.set_data_start(new_data_start)\n",
    "\n",
    "        # Update right-most point if necessary\n",
    "        data_end = plot_instance.get_data_end()\n",
    "        if data_end < new_data_start:\n",
    "            plot_instance.set_data_end(new_data_start)\n",
    "            dpg.set_value(f\"{plot_instance.ID}_end_int\", new_data_start)\n",
    "\n",
    "        plot_data(plot_instance)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in set_start_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4469a029-f2ec-4934-a8b1-0b5af1add8df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def set_end_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Set ending x-axis values in seconds for selected plot\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Update right-most point to observe\n",
    "        plot_instance = user_data\n",
    "        new_data_end = app_data\n",
    "        plot_instance.set_data_end(new_data_end)\n",
    "\n",
    "        # Update left-most point if necessary\n",
    "        data_start = plot_instance.get_data_start()\n",
    "        if data_start > new_data_end:\n",
    "            plot_instance.set_data_start(new_data_end)\n",
    "            dpg.set_value(f\"{plot_instance.ID}_start_int\", new_data_end)\n",
    "\n",
    "        plot_data(plot_instance)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in set_end_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5db224e8-eccf-4a15-939d-2b7520c25734",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def shift_data_right_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Shifts observed data to the right in seconds by shift length specified in shift widget\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate new window of data to observe\n",
    "        plot_instance = user_data\n",
    "        shift_length = dpg.get_value(f\"{plot_instance.ID}_shift_int\")\n",
    "        data_start = plot_instance.get_data_start() + shift_length\n",
    "        data_end = plot_instance.get_data_end() + shift_length\n",
    "        \n",
    "        # Check if shift has exceeded max allowable value\n",
    "        max_value = plot_instance.get_data_max()\n",
    "        if data_start > max_value:\n",
    "            data_start = max_value\n",
    "        if data_end > max_value:\n",
    "            data_end = max_value\n",
    "        \n",
    "        # Update start and end widgets\n",
    "        plot_instance.set_data_start(data_start)\n",
    "        plot_instance.set_data_end(data_end)\n",
    "        dpg.set_value(f\"{plot_instance.ID}_start_int\", data_start)\n",
    "        dpg.set_value(f\"{plot_instance.ID}_end_int\", data_end)\n",
    "\n",
    "        plot_data(plot_instance)\n",
    "\n",
    "        # Fit both axes to the new data\n",
    "        dpg.fit_axis_data(f\"{plot_instance.ID}_x_axis\")\n",
    "        dpg.fit_axis_data(f\"{plot_instance.ID}_y_axis\")\n",
    "\n",
    "        sync_axis(plot_instance)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in shift_data_right_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "da39848b-d29e-4f56-b054-d13b732a31a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def shift_data_left_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Shifts observed data to the left in seconds by shift length specified in shift widget\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate new window of data to observe\n",
    "        plot_instance = user_data\n",
    "        shift_length = dpg.get_value(f\"{plot_instance.ID}_shift_int\")\n",
    "        data_start = plot_instance.get_data_start() - shift_length\n",
    "        data_end = plot_instance.get_data_end() - shift_length\n",
    "        \n",
    "        # Check if shift has exceeded min allowable value\n",
    "        min_value = plot_instance.get_data_min()\n",
    "        if data_start < min_value:\n",
    "            data_start = min_value\n",
    "        if data_end < min_value:\n",
    "            data_end = min_value\n",
    "\n",
    "        # Update start and end widgets\n",
    "        plot_instance.set_data_start(data_start)\n",
    "        plot_instance.set_data_end(data_end)\n",
    "        dpg.set_value(f\"{plot_instance.ID}_start_int\", data_start)\n",
    "        dpg.set_value(f\"{plot_instance.ID}_end_int\", data_end)\n",
    "\n",
    "        plot_data(plot_instance)\n",
    "        \n",
    "        # Fit both axes to the new data\n",
    "        dpg.fit_axis_data(f\"{plot_instance.ID}_x_axis\")\n",
    "        dpg.fit_axis_data(f\"{plot_instance.ID}_y_axis\")\n",
    "\n",
    "        sync_axis(plot_instance)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in shift_data_left_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4f56d0e4-f800-45b4-b8a5-fae160e1c1dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def change_plot_type_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback that updates chosen plot type.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data\n",
    "        plot_type = app_data\n",
    "        plot_instance.set_plot_type(plot_type)\n",
    "\n",
    "        if plot_type == \"Filter\":\n",
    "            dpg.add_separator(parent=f\"{plot_instance.ID}_import_tab\",\n",
    "                              tag=f\"{plot_instance.ID}_filter_separator\")\n",
    "            dpg.add_text(default_value=f\"Filter Settings\",\n",
    "                         parent=f\"{plot_instance.ID}_import_tab\",\n",
    "                         tag=f\"{plot_instance.ID}_filter_label\")\n",
    "            dpg.add_input_int(label=\"Lowcut (f)\",\n",
    "                              tag=f\"{plot_instance.ID}_lowcut\",\n",
    "                              parent=f\"{plot_instance.ID}_import_tab\",\n",
    "                              width=200,\n",
    "                              callback=set_lowcut_callback,\n",
    "                              user_data=plot_instance,\n",
    "                              on_enter=True,\n",
    "                              min_value=0,\n",
    "                              min_clamped=True,\n",
    "                              default_value=1)\n",
    "            dpg.add_input_int(label=\"Highcut (f)\",\n",
    "                              tag=f\"{plot_instance.ID}_highcut\",\n",
    "                              parent=f\"{plot_instance.ID}_import_tab\",\n",
    "                              width=200,\n",
    "                              callback=set_highcut_callback,\n",
    "                              user_data=plot_instance,\n",
    "                              on_enter=True,\n",
    "                              min_value=0,\n",
    "                              min_clamped=True,\n",
    "                              default_value=200)\n",
    "        if plot_type == \"Raw\":\n",
    "            if dpg.does_item_exist(f\"{plot_instance.ID}_filter_separator\"):\n",
    "                dpg.delete_item(f\"{plot_instance.ID}_filter_separator\")\n",
    "            if dpg.does_item_exist(f\"{plot_instance.ID}_filter_label\"):\n",
    "                dpg.delete_item(f\"{plot_instance.ID}_filter_label\")\n",
    "            if dpg.does_item_exist(f\"{plot_instance.ID}_lowcut\"):\n",
    "                dpg.delete_item(f\"{plot_instance.ID}_lowcut\")\n",
    "            if dpg.does_item_exist(f\"{plot_instance.ID}_highcut\"):\n",
    "                dpg.delete_item(f\"{plot_instance.ID}_highcut\")\n",
    "            plot_data(plot_instance)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in change_plot_type_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "376a5848-49cf-46a7-b4a7-77517922a4ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def set_lowcut_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback that updates lowcut value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data\n",
    "        lowcut = app_data\n",
    "        plot_instance.set_lowcut(lowcut)\n",
    "        plot_data(plot_instance)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in set_lowcut_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5670b083-78da-41b0-aa27-a843ad75a0b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def set_highcut_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback that updates highcut value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data\n",
    "        highcut = app_data\n",
    "        plot_instance.set_highcut(highcut)\n",
    "        plot_data(plot_instance)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in set_highcut_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7bb63f63-9d22-4d71-a223-1e6206eccfb6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sync_axis_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback that syncs every plot to match the new axis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data\n",
    "        checkbox = app_data\n",
    "\n",
    "        # Crucially remove lock on plot anytime sync is called\n",
    "        dpg.set_axis_limits_auto(f\"{plot_instance.ID}_x_axis\")\n",
    "        dpg.set_axis_limits_auto(f\"{plot_instance.ID}_y_axis\")\n",
    "\n",
    "        if checkbox == True:\n",
    "            plot_instance.add_to_sync_list(plot_instance.ID)\n",
    "        else:\n",
    "            plot_instance.remove_from_sync_list(plot_instance.ID)\n",
    "\n",
    "        sync_axis(plot_instance)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in sync_axis_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1b293c4-785e-4521-9b8d-9a2cd7460f12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def export_trigger_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback that sets paremeters for generating figure of chosen plots.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data\n",
    "        rat_num, probe = plot_instance.get_rat_and_probe_from_channel\n",
    "        selected_channel = plot_instance.get_channel()\n",
    "\n",
    "        # Use either default output path or custom\n",
    "        custom_name = plot_instance.get_custom_name()\n",
    "        output_choice = plot_instance.get_export_status()\n",
    "        if output_choice == True:\n",
    "            name = custom_name\n",
    "        else:\n",
    "            name = f\"Rat {rat_num} - {selected_channel} - {probe}\"\n",
    "\n",
    "        # Grab data\n",
    "        x_data, y_data = dpg.get_value(f\"{plot_instance.ID}_plot_series\")\n",
    "\n",
    "        # Grab plot boundaries\n",
    "        x_min, x_max = dpg.get_axis_limits(f\"{plot_instance.ID}_x_axis\")\n",
    "        y_min, y_max = dpg.get_axis_limits(f\"{plot_instance.ID}_y_axis\")\n",
    "\n",
    "        # Grab export path, use program folder location as default\n",
    "        export_path = plot_instance.get_plot_output_path()\n",
    "\n",
    "        if not export_path:  # If user hasn't set a path (default value)\n",
    "            export_path = os.path.join(os.getcwd(), \"Plot Output\")\n",
    "            os.makedirs(export_path, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "        export_plot(x_data, y_data, x_min, x_max, y_min, y_max, name, export_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in export_trigger_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5f2e1251-e4e3-4794-bac9-e6519d851c70",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def custom_plot_name_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback that overrides default naming scheme\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data\n",
    "        custom_name = app_data\n",
    "\n",
    "        plot_instance.set_custom_name(app_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in export_trigger_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2dc2f297-9781-4909-bc2e-d41830464b9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_export_status_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback that updates if plot should use a custom filepath.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data\n",
    "        export_status = app_data\n",
    "        plot_instance.set_export_status(export_status)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in update_export_status_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "479ac9ee-6f0a-44fd-bf99-0ac41c57e36d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def select_plot_output_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback that sets a custom output filepath for plots.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot_instance = user_data\n",
    "        \n",
    "        # Open a folder selection dialog\n",
    "        root = Tk()\n",
    "        root.withdraw()  # Hide the root window\n",
    "        plot_output_folder = filedialog.askdirectory(initialdir=\"Projects\", title=\"Select Output Folder\")\n",
    "        root.destroy()  # Destroy the Tk instance\n",
    "        if not plot_output_folder:\n",
    "            return  # No folder selected\n",
    "\n",
    "        plot_instance.set_plot_output_path(plot_output_folder)\n",
    "        logging.debug(f\"User-selected plot output path: {plot_output_folder}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in select_plot_output_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a6e886e-468d-4681-a68c-c96a99f2e955",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_plot_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Button to add an additional plot\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parent = user_data\n",
    "        plot_class_2 = Plot()\n",
    "        if dpg.does_item_exist(sender):\n",
    "            dpg.delete_item(sender)\n",
    "        create_plot_window(parent, plot_class_2)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in add_plot_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bd28ed77-28b6-42a9-8f45-76a557d5fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### I/O & Misc Callbacks #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d3144cd9-81b8-490f-b45e-44983eff25e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def open_output_location_callback(sender, app_data, user_data):\n",
    "    '''\n",
    "    # Function to open current analysis output location.\n",
    "    '''\n",
    "    try:\n",
    "        Analysis_class = user_data\n",
    "        output_dir = Analysis_class.get_output_directory()\n",
    "\n",
    "        if not os.path.isdir(output_dir):\n",
    "            logging.warning(f\"Output directory does not exist: {output_dir}\")\n",
    "            show_popup(f\"Output directory does not exist:\\n{output_dir}\")\n",
    "            return\n",
    "\n",
    "        os.startfile(output_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in open_output_location_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5cf28654-06a1-4411-accf-247a6f428d4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def select_NWB_folder_callback(sender, app_data):\n",
    "    '''\n",
    "    # Callback to find then display NWB files\n",
    "    '''\n",
    "    try:\n",
    "        # Get filepath to NWB folder from user, Display NWB files found\n",
    "        new_folder = get_NWB_folder_filepath()\n",
    "        if new_folder is None:\n",
    "            # The user may have canceled the folder selection.\n",
    "            logging.debug(f\"Cancelled folder selection in select_NWB_folder_callback.\")\n",
    "            return\n",
    "        NWBFolder.set_folder_path(new_folder)\n",
    "        display_NWB_files(NWBFolder_Class)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in select_NWB_folder_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a6c425a8-751f-43a7-a44a-e2f002ef41dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_NWB_folder_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback to save the NWB folder and checkbox state to JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        folder_path = NWBFolder.get_folder_path()\n",
    "        is_checked = dpg.get_value(\"save_NWB_folder\")\n",
    "        config_data = {\n",
    "            \"nwb_folder\": folder_path,\n",
    "            \"save_nwb_folder\": is_checked\n",
    "        }\n",
    "        with open(config_file, \"w\") as f:\n",
    "            json.dump(config_data, f)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in save_NWB_folder_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "37477015-4f7f-4ee6-9166-bdd5b4b3627a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def checkbox_callback(sender, app_data, user_data):\n",
    "    \"\"\"\n",
    "    Callback for checkbox interaction.\n",
    "    Clears other checkboxes within the same parent context and displays metadata of the selected file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtain information on sender and relevant tags\n",
    "        folder_path, parent, file = user_data\n",
    "        children = dpg.get_item_children(parent)\n",
    "        children_ids = children[1]\n",
    "        tag_list = [dpg.get_item_alias(child_id) for child_id in children_ids]\n",
    "        logging.debug(f\"Found children '{children}' and children ids '{children_ids}' and tag_list '{tag_list}' for parent '{parent}'.\")\n",
    "\n",
    "        # Iterate through the child IDs\n",
    "        for child_id in children_ids:\n",
    "            # Ensure the item exists and has a valid tag\n",
    "            if not dpg.does_item_exist(child_id):\n",
    "                continue\n",
    "\n",
    "            # Get the tag of the child\n",
    "            tag = dpg.get_item_alias(child_id)\n",
    "\n",
    "            # Skip invalid or empty tags\n",
    "            if not tag:\n",
    "                continue\n",
    "\n",
    "            # Check if the child is a checkbox\n",
    "            if dpg.get_item_type(child_id) == \"mvAppItemType::mvCheckbox\" and tag != sender:\n",
    "                # Uncheck the checkbox\n",
    "                dpg.set_value(tag, False)\n",
    "\n",
    "        # Display update metadata under Import tab\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        display_metadata(file_path)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in checkbox_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fcdd6352-b2fa-4239-a4fb-2777e58d75e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def resize_callback(sender, app_data):\n",
    "    '''\n",
    "    # Callback to adjust the size of GUIDA to fit size of entire window\n",
    "    '''\n",
    "    try:\n",
    "        height = dpg.get_viewport_client_height()\n",
    "        width = dpg.get_viewport_client_width()\n",
    "        #if dpg.does_item_exist(\"plot\"):\n",
    "        #    dpg.configure_item(\"plot\", width=width-16, height=height-110)\n",
    "        if dpg.does_item_exist(\"GUI_Tools_win\"):\n",
    "            dpg.configure_item(\"GUI_Tools_win\", width=width, height=height)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in resize_callback: {e}\", exc_info=True)\n",
    "        show_popup(error_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ba2050a5-fdc7-4b14-a146-f17f964a8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################################\n",
    "# Main\n",
    "#######################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542cb2b-5486-416d-b2fe-4ab9ecad6642",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\holot\\AppData\\Local\\Temp\\ipykernel_125504\\1837003639.py:78: UserWarning: The figure layout has changed to tight\n",
      "  fig.tight_layout()\n",
      "C:\\Users\\holot\\AppData\\Local\\Temp\\ipykernel_125504\\1837003639.py:78: UserWarning: The figure layout has changed to tight\n",
      "  fig.tight_layout()\n",
      "C:\\Users\\holot\\AppData\\Local\\Temp\\ipykernel_125504\\1837003639.py:78: UserWarning: The figure layout has changed to tight\n",
      "  fig.tight_layout()\n",
      "C:\\Users\\holot\\AppData\\Local\\Temp\\ipykernel_125504\\1837003639.py:78: UserWarning: The figure layout has changed to tight\n",
      "  fig.tight_layout()\n",
      "C:\\Users\\holot\\AppData\\Local\\Temp\\ipykernel_125504\\1837003639.py:78: UserWarning: The figure layout has changed to tight\n",
      "  fig.tight_layout()\n",
      "C:\\Users\\holot\\AppData\\Local\\Temp\\ipykernel_125504\\85368651.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby([\"band\", \"condition\"])[\"snr_db\"]\n",
      "C:\\Users\\holot\\AppData\\Local\\Temp\\ipykernel_125504\\2379239800.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_long.groupby([\"TimeKey\", \"Metric\"])[\"Value\"]\n"
     ]
    }
   ],
   "source": [
    "def run_GUI():\n",
    "    '''\n",
    "    # Function to start the Dear PyGui context in a thread\n",
    "    '''\n",
    "    try:\n",
    "        dpg.create_context()\n",
    "\n",
    "        with dpg.window(label=\"GUI Tools\", \n",
    "                        width=1150, \n",
    "                        height=550, \n",
    "                        no_resize=True, \n",
    "                        no_title_bar=True,\n",
    "                        no_move=True, \n",
    "                        no_collapse=True, \n",
    "                        no_close=True,\n",
    "                        tag=\"GUI_Tools_win\"):\n",
    "            \n",
    "            with dpg.tab_bar(label=\"tabs\"):\n",
    "                with dpg.tab(label=\"Import\", tag=\"import_tab\"):\n",
    "                    with dpg.child_window(tag=\"import_child_window\", \n",
    "                                          border=True, \n",
    "                                          autosize_x=True,\n",
    "                                          autosize_y=True, \n",
    "                                          parent=\"import_tab\"):\n",
    "                        with dpg.child_window(tag=\"import_buttons_child_window\", \n",
    "                            border=True, \n",
    "                            autosize_x=True,\n",
    "                            height=35,\n",
    "                            parent=\"import_child_window\"):\n",
    "                            with dpg.group(tag=\"import_buttons\", horizontal=True):\n",
    "                                dpg.add_text(\"Project Options:\")\n",
    "                                dpg.add_button(label=\"Select NWB Folder\", tag=\"select_NWB_folder\", callback=select_NWB_folder_callback)\n",
    "                                dpg.add_checkbox(label=\"Save NWB Location\", tag=\"save_NWB_folder\", callback=save_NWB_folder_callback)\n",
    "                                dpg.add_text(\"  Display Options:\")\n",
    "                                dpg.add_button(label=\"FullScreen\", callback= lambda: dpg.toggle_viewport_fullscreen())\n",
    "                                \n",
    "                with dpg.tab(label=\"Visualization\", tag=\"viz_tab\"):\n",
    "                    with dpg.child_window(tag=\"viz_child_window\", \n",
    "                                          border=True, \n",
    "                                          autosize_x=True,\n",
    "                                          autosize_y=True, \n",
    "                                          parent=\"viz_tab\"):\n",
    "                        with dpg.group(tag=\"viz_buttons\", horizontal=True, parent=\"viz_child_window\"):\n",
    "                            with dpg.child_window(tag=\"plot_buttons\",\n",
    "                                                  border=True, \n",
    "                                                  autosize_x=True,\n",
    "                                                  height=35, \n",
    "                                                  parent=\"viz_buttons\"):\n",
    "                                with dpg.group(horizontal=True, parent=\"plot_buttons\"):\n",
    "                                    dpg.add_text(\"Plot Options:\")\n",
    "                                    \n",
    "                with dpg.tab(label=\"Analysis\", tag=\"ana_tab\"):\n",
    "                    with dpg.child_window(tag=\"ana_child_window\", \n",
    "                                          border=True, \n",
    "                                          autosize_x=True,\n",
    "                                          autosize_y=True, \n",
    "                                          parent=\"ana_tab\"):\n",
    "                        with dpg.child_window(tag=\"ana_buttons_window\",\n",
    "                                              border=True, \n",
    "                                              autosize_x=True,\n",
    "                                              height=35):\n",
    "                            with dpg.group(horizontal=True, parent=\"ana_buttons\", tag=\"ana_buttons_group\"):\n",
    "                                dpg.add_text(\"Analysis Options:\")\n",
    "                                \n",
    "                with dpg.tab(label=\"JSON\", tag=\"JSON_tab\"):\n",
    "                    with dpg.child_window(tag=\"JSON_child_window\", \n",
    "                                          border=True, \n",
    "                                          autosize_x=True,\n",
    "                                          autosize_y=True, \n",
    "                                          parent=\"JSON_tab\"):\n",
    "                        with dpg.child_window(tag=\"JSON_buttons_window\",\n",
    "                                              border=True, \n",
    "                                              autosize_x=True,\n",
    "                                              height=35):\n",
    "                            with dpg.group(horizontal=True, parent=\"JSON_buttons\", tag=\"JSON_buttons_group\"):\n",
    "                                dpg.add_text(\"Analysis Options:\")\n",
    "                                    \n",
    "\n",
    "        # At startup, load the config and update NWBFolder and the checkbox accordingly.\n",
    "        config = load_config()\n",
    "        if config.get(\"save_nwb_folder\", False):\n",
    "            saved_folder = config.get(\"nwb_folder\", \"\")\n",
    "            if saved_folder:\n",
    "                # Instantiate Class, Get filepath to NWB folder from user, Display NWB files found\n",
    "                NWBFolder_Class = NWBFolder()\n",
    "                NWBFolder.set_folder_path(saved_folder)\n",
    "                display_NWB_files(NWBFolder_Class)\n",
    "            try:\n",
    "                dpg.set_value(\"save_NWB_folder\", True)\n",
    "            except Exception:\n",
    "                pass\n",
    "                \n",
    "        dpg.create_viewport(title='GUIDA', \n",
    "                            width=1400, \n",
    "                            height=700)\n",
    "        \n",
    "        dpg.setup_dearpygui()\n",
    "        dpg.show_viewport()\n",
    "        dpg.set_viewport_resize_callback(resize_callback)\n",
    "        # Set the exit callback\n",
    "        dpg.set_exit_callback(close_gui)\n",
    "        dpg.start_dearpygui()\n",
    "        dpg.destroy_context()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in run_GUI: {e}\", exc_info=True)\n",
    "        show_popup(error_string)\n",
    "\n",
    "# Function to properly close the GUI and clean up\n",
    "def close_gui():\n",
    "    try:\n",
    "        global gui_running\n",
    "        gui_running = False\n",
    "        time.sleep(1)  # Give some time for the loop to exit\n",
    "        logging.info(\"GUI has been closed and cleaned up.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in close_gui: {e}\", exc_info=True)\n",
    "        show_popup(error_string)\n",
    "\n",
    "run_GUI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b81eab-32c6-4535-9d0b-ee2ad5106660",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GUI Kernel V2",
   "language": "python",
   "name": "guikernelv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
